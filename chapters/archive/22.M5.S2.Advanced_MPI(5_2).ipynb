{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Dynamic Process Management in MPI\n",
        "In MPI, dynamic process management allows an MPI program to spawn new processes during execution, which is critical in scenarios where the workload changes dynamically. The key function for this purpose is MPI_Comm_spawn, which is used to create new processes while the program is running.\n",
        "\n",
        "In this example, the parent process (rank 0) will spawn two child processes using the MPI_Comm_spawn function. These child processes perform some work, and the parent process sends a message to the first child via an intercommunicator, which allows communication between the parent and child processes.\n",
        "\n",
        "##Key Functions\n",
        "- MPI_Comm_spawn: Spawns new processes dynamically during the execution of the MPI program.\n",
        "- MPI_Send: Sends a message from the parent process to the child processes.\n",
        "- MPI_Comm_rank: Determines the rank (process ID) within the current communicator.\n",
        "- MPI_Comm_size: Determines the total number of processes in the communicator.\n",
        "\n",
        "###In this example, we will:\n",
        "\n",
        "- Spawn two child processes from the parent process.\n",
        "- Send a message from the parent to one of the child processes.\n",
        "- Observe how the child processes receive the message and perform their work."
      ],
      "metadata": {
        "id": "2TySXwNiYtD7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rt5WKgYrYOz6",
        "outputId": "389e6ff1-01df-4e26-a29b-2bba267e4c2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parent process spawned child processes.\n",
            "Child 1 doing work.\n",
            "Child 0 received message from parent: 42\n"
          ]
        }
      ],
      "source": [
        "# Write the parent and child programs to files\n",
        "parent_code = \"\"\"\n",
        "#include <mpi.h>\n",
        "#include <stdio.h>\n",
        "\n",
        "int main(int argc, char *argv[]) {\n",
        "    MPI_Init(&argc, &argv);\n",
        "\n",
        "    int rank, size;\n",
        "    MPI_Comm intercomm;\n",
        "\n",
        "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
        "    MPI_Comm_size(MPI_COMM_WORLD, &size);\n",
        "\n",
        "    if (rank == 0) {\n",
        "        // Parent process spawns 2 child processes\n",
        "        MPI_Comm_spawn(\"child_program\", MPI_ARGV_NULL, 2, MPI_INFO_NULL, 0, MPI_COMM_SELF, &intercomm, MPI_ERRCODES_IGNORE);\n",
        "        printf(\"Parent process spawned child processes.\\\\n\");\n",
        "\n",
        "        // Parent sends message to children via intercommunicator\n",
        "        int msg = 42;\n",
        "        MPI_Send(&msg, 1, MPI_INT, 0, 0, intercomm); // Send to child 0\n",
        "    } else {\n",
        "        // Worker processes perform normal work\n",
        "        printf(\"Worker process %d doing work.\\\\n\", rank);\n",
        "    }\n",
        "\n",
        "    MPI_Finalize();\n",
        "    return 0;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "child_code = \"\"\"\n",
        "#include <mpi.h>\n",
        "#include <stdio.h>\n",
        "\n",
        "int main(int argc, char *argv[]) {\n",
        "    MPI_Init(&argc, &argv);\n",
        "\n",
        "    int rank;\n",
        "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
        "\n",
        "    if (rank == 0) {\n",
        "        // Child 0 receives message from the parent\n",
        "        MPI_Comm parent_comm;\n",
        "        MPI_Comm_get_parent(&parent_comm);\n",
        "        int msg;\n",
        "        MPI_Recv(&msg, 1, MPI_INT, 0, 0, parent_comm, MPI_STATUS_IGNORE);\n",
        "        printf(\"Child 0 received message from parent: %d\\\\n\", msg);\n",
        "    } else {\n",
        "        printf(\"Child %d doing work.\\\\n\", rank);\n",
        "    }\n",
        "\n",
        "    MPI_Finalize();\n",
        "    return 0;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Save the parent and child programs to files\n",
        "with open(\"parent_program.c\", \"w\") as parent_file:\n",
        "    parent_file.write(parent_code)\n",
        "\n",
        "with open(\"child_program.c\", \"w\") as child_file:\n",
        "    child_file.write(child_code)\n",
        "\n",
        "# Compile the C programs\n",
        "!mpicc -o parent_program parent_program.c\n",
        "!mpicc -o child_program child_program.c\n",
        "\n",
        "# Set environment variables to allow running as root\n",
        "import os\n",
        "os.environ['OMPI_ALLOW_RUN_AS_ROOT'] = '1'\n",
        "os.environ['OMPI_ALLOW_RUN_AS_ROOT_CONFIRM'] = '1'\n",
        "\n",
        "# Run the MPI program with 1 parent process, allowing oversubscription\n",
        "!mpirun --allow-run-as-root --oversubscribe -np 1 ./parent_program\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code Walkthrough\n",
        "### Parent Program:\n",
        "- MPI_Comm_spawn: The parent process (rank 0) uses MPI_Comm_spawn to dynamically create two child processes. This creates an intercommunicator that allows communication between the parent and child processes.\n",
        "\n",
        "- MPI_Send: The parent process sends a message (msg = 42) to the first child (child 0) using the intercommunicator.\n",
        "\n",
        "- MPI_Finalize: After sending the message and spawning the children, the parent process finalizes the MPI environment.\n",
        "\n",
        "### Child Program:\n",
        "- MPI_Comm_get_parent: Each child process uses this function to get the intercommunicator that connects it to the parent process.\n",
        "\n",
        " -MPI_Recv: Child 0 receives the message from the parent. The other child (child 1) simply prints that it is performing some work.\n",
        "\n",
        "- MPI_Finalize: Each child process finalizes its MPI environment when the work is done.\n",
        "\n",
        "### Output:\n",
        "The output of the program will show the parent process spawning two child processes and sending a message to child 0. Child 0 will print the received message, and child 1 will simply indicate that it is doing work."
      ],
      "metadata": {
        "id": "Z5TibjHQZEjo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dynamic Process Management: Load Balancing\n",
        "\n",
        "In **MPI** (Message Passing Interface), dynamic process management and load balancing are crucial when dealing with varying workloads during runtime. As computational demands change, more processes can be dynamically added to handle the load. This technique is especially useful in simulations or parallel applications where different regions or tasks may require more processing power at different times.\n",
        "\n",
        "### Load Balancing with Dynamic Processes\n",
        "\n",
        "Load balancing ensures that tasks are distributed efficiently across processes to maximize resource usage. Some key points about load balancing and dynamic process management in MPI include:\n",
        "\n",
        "- **Dynamic task allocation**: Tasks can be dynamically allocated to new processes as needed.\n",
        "- **Workload adjustment**: The process resources are adjusted based on the current computational load.\n",
        "- **Task redistribution**: Tasks can be redistributed among existing and new processes during execution.\n",
        "- **Heterogeneous environments**: This method is ideal for environments where the computational power varies across different hardware (e.g., CPUs, GPUs).\n",
        "- **Scaling**: You can dynamically scale the number of processes up or down to handle increasing or decreasing workload efficiently.\n",
        "\n",
        "In the following example, the parent process spawns two child processes to handle additional tasks dynamically. The parent assigns different tasks to each of these child processes and sends tasks to the workers through **intercommunication**.\n",
        "\n",
        "### Key Functions:\n",
        "- **MPI_Comm_spawn**: Used to dynamically spawn child processes during runtime.\n",
        "- **MPI_Send**: Sends data from the parent to the child processes.\n",
        "- **MPI_Recv**: Receives data in the child processes from the parent.\n"
      ],
      "metadata": {
        "id": "5CjooTIUZ-M_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Writing the parent program to a file\n",
        "parent_code = \"\"\"\n",
        "#include <mpi.h>\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "int main(int argc, char *argv[]) {\n",
        "    MPI_Init(&argc, &argv);\n",
        "\n",
        "    int rank, size;\n",
        "    MPI_Comm intercomm;\n",
        "\n",
        "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
        "    MPI_Comm_size(MPI_COMM_WORLD, &size);\n",
        "\n",
        "    if (rank == 0) {\n",
        "        // Parent process dynamically spawns 2 more workers\n",
        "        int extra_workers = 2;\n",
        "        MPI_Comm_spawn(\"worker_program\", MPI_ARGV_NULL, extra_workers, MPI_INFO_NULL, 0, MPI_COMM_SELF, &intercomm, MPI_ERRCODES_IGNORE);\n",
        "        printf(\"Parent process spawning extra workers for load balancing.\\\\n\");\n",
        "\n",
        "        // Example of task workloads\n",
        "        int tasks[2] = {10, 20};  // Two different tasks\n",
        "\n",
        "        // Parent sends different tasks to each child process\n",
        "        MPI_Send(&tasks[0], 1, MPI_INT, 0, 0, intercomm);  // Task for child 0\n",
        "        MPI_Send(&tasks[1], 1, MPI_INT, 1, 0, intercomm);  // Task for child 1\n",
        "    }\n",
        "\n",
        "    MPI_Finalize();\n",
        "    return 0;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Writing the worker (child) program to a file\n",
        "worker_code = \"\"\"\n",
        "#include <mpi.h>\n",
        "#include <stdio.h>\n",
        "\n",
        "int main(int argc, char *argv[]) {\n",
        "    MPI_Init(&argc, &argv);\n",
        "\n",
        "    int rank;\n",
        "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
        "    MPI_Comm parent_comm;\n",
        "    int task;\n",
        "\n",
        "    // Get the parent communicator\n",
        "    MPI_Comm_get_parent(&parent_comm);\n",
        "\n",
        "    // Each worker process receives a task from the parent\n",
        "    MPI_Recv(&task, 1, MPI_INT, 0, 0, parent_comm, MPI_STATUS_IGNORE);\n",
        "    printf(\"Worker process %d doing task %d\\\\n\", rank, task);\n",
        "\n",
        "    MPI_Finalize();\n",
        "    return 0;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Save the parent and worker programs to files\n",
        "with open(\"parent_program.c\", \"w\") as parent_file:\n",
        "    parent_file.write(parent_code)\n",
        "\n",
        "with open(\"worker_program.c\", \"w\") as worker_file:\n",
        "    worker_file.write(worker_code)\n",
        "\n",
        "# Compile the parent and worker programs\n",
        "!mpicc -o parent_program parent_program.c\n",
        "!mpicc -o worker_program worker_program.c\n",
        "\n",
        "# Set environment variables to allow running as root\n",
        "import os\n",
        "os.environ['OMPI_ALLOW_RUN_AS_ROOT'] = '1'\n",
        "os.environ['OMPI_ALLOW_RUN_AS_ROOT_CONFIRM'] = '1'\n",
        "\n",
        "# Run the MPI program with 1 parent process, allowing dynamic spawning of 2 children\n",
        "!mpirun --allow-run-as-root --oversubscribe -np 1 ./parent_program\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4Ef5yQtZ-jg",
        "outputId": "2dfe7027-bab9-42cf-c377-1c0a459b5acf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parent process spawning extra workers for load balancing.\n",
            "Worker process 0 doing task 10\n",
            "Worker process 1 doing task 20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explanation of the Code\n",
        "\n",
        "In this example, we demonstrate **dynamic process management** and **load balancing** using MPI.\n",
        "\n",
        "1. **Parent Process**:\n",
        "   - The parent process begins by initializing the MPI environment and spawning two child processes using `MPI_Comm_spawn`. The function creates an intercommunicator, which allows communication between the parent and the child processes.\n",
        "   - The parent assigns two different tasks (represented by the values `10` and `20`) to each child process by sending these tasks through `MPI_Send` via the intercommunicator. Each child receives its respective task and processes it.\n",
        "\n",
        "2. **Worker (Child) Processes**:\n",
        "   - Each worker process (child) receives a task from the parent using `MPI_Recv`. The `MPI_Comm_get_parent` function is used by the child processes to get the intercommunicator that connects them to the parent.\n",
        "   - The worker processes print out their assigned task and proceed with their work.\n",
        "\n",
        "### Output:\n",
        "The output will show the parent process spawning two child processes and assigning each one a different task:\n"
      ],
      "metadata": {
        "id": "EuXAHltiZ-3_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# One-Sided Communication in MPI\n",
        "\n",
        "In **MPI**, one-sided communication allows one process to directly access the memory of another process without requiring explicit cooperation from the target process. This is done using **Remote Memory Access (RMA)** operations, which include functions like `MPI_Put`, `MPI_Get`, and `MPI_Accumulate`.\n",
        "\n",
        "### One-Sided Communication Overview\n",
        "\n",
        "One-sided communication is useful in applications where processes need to frequently update or access shared data. Unlike traditional two-sided communication (e.g., `MPI_Send` and `MPI_Recv`), where both the sender and receiver need to actively participate in the communication, one-sided communication allows a process to write to or read from another process's memory independently.\n",
        "\n",
        "### Key Functions\n",
        "- **MPI_Win_create**: Defines a window of memory that can be accessed by other processes.\n",
        "- **MPI_Put**: A process writes data into the memory of another process.\n",
        "- **MPI_Win_fence**: Synchronizes RMA operations to ensure data consistency.\n",
        "- **MPI_Win_free**: Frees the memory window when the communication is complete.\n",
        "\n",
        "In this example, Process 0 writes a value to the memory of Process 1 using **MPI_Put**. Process 1 exposes its memory using a window, allowing Process 0 to write directly to it.\n",
        "\n"
      ],
      "metadata": {
        "id": "_x6oLK7Mcfhg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Writing the updated MPI one-sided communication example to a file\n",
        "mpi_code_updated = \"\"\"\n",
        "#include <mpi.h>\n",
        "#include <stdio.h>\n",
        "\n",
        "int main(int argc, char *argv[]) {\n",
        "    MPI_Init(&argc, &argv);\n",
        "\n",
        "    int rank;\n",
        "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
        "\n",
        "    // Initialize window and buffer for one-sided communication\n",
        "    int data;\n",
        "    MPI_Win win;\n",
        "\n",
        "    if (rank == 0) {\n",
        "        // Process 0 writes data to Process 1's memory\n",
        "        int value_to_put = 42;\n",
        "\n",
        "        // No memory to expose in Process 0, just creating a window for synchronization\n",
        "        MPI_Win_create(MPI_BOTTOM, 0, 1, MPI_INFO_NULL, MPI_COMM_WORLD, &win);\n",
        "\n",
        "        // Synchronize before starting the RMA operation\n",
        "        MPI_Win_fence(0, win);\n",
        "\n",
        "        // Write value to Process 1's memory at offset 0\n",
        "        MPI_Put(&value_to_put, 1, MPI_INT, 1, 0, 1, MPI_INT, win);\n",
        "\n",
        "        // Synchronize after the RMA operation\n",
        "        MPI_Win_fence(0, win);\n",
        "    } else if (rank == 1) {\n",
        "        // Process 1 exposes its memory for Process 0 to write into\n",
        "        int target_data = 0;\n",
        "        MPI_Win_create(&target_data, sizeof(int), sizeof(int), MPI_INFO_NULL, MPI_COMM_WORLD, &win);\n",
        "\n",
        "        // Synchronize before the RMA operation\n",
        "        MPI_Win_fence(0, win);\n",
        "\n",
        "        // Wait for Process 0 to complete the put operation\n",
        "        MPI_Win_fence(0, win);\n",
        "\n",
        "        // Process 1 retrieves the data written by Process 0\n",
        "        printf(\"Process 1 received data: %d\\\\n\", target_data);\n",
        "    }\n",
        "\n",
        "    MPI_Win_free(&win);\n",
        "    MPI_Finalize();\n",
        "    return 0;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Save the updated C program to a file\n",
        "with open(\"mpi_one_sided_sync.c\", \"w\") as mpi_file:\n",
        "    mpi_file.write(mpi_code_updated)\n",
        "\n",
        "# Compile the C program\n",
        "!mpicc -o mpi_one_sided_sync mpi_one_sided_sync.c\n",
        "\n",
        "# Set environment variables to allow running as root\n",
        "import os\n",
        "os.environ['OMPI_ALLOW_RUN_AS_ROOT'] = '1'\n",
        "os.environ['OMPI_ALLOW_RUN_AS_ROOT_CONFIRM'] = '1'\n",
        "\n",
        "# Run the MPI program with 2 processes, allowing one-sided communication\n",
        "!mpirun --allow-run-as-root --oversubscribe -np 2 ./mpi_one_sided_sync\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVc7xtHWcfNg",
        "outputId": "22c2159a-2159-4114-ab84-a32a0bbe4947"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Process 1 received data: 42\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explanation of the Code\n",
        "\n",
        "This example demonstrates **one-sided communication** between two processes using MPI.\n",
        "\n",
        "1. **Process 0**:\n",
        "   - Process 0 writes a value (`42`) into the memory of Process 1 using **MPI_Put**.\n",
        "   - It creates a window using `MPI_Win_create` with `MPI_BOTTOM` as the memory location, meaning that Process 0 does not expose any memory of its own, but rather writes into Process 1's memory.\n",
        "   - The function **MPI_Put** allows Process 0 to place the value into the memory of Process 1.\n",
        "\n",
        "2. **Process 1**:\n",
        "   - Process 1 exposes its memory for writing by creating a window (`MPI_Win_create`). The `target_data` variable holds the memory that will receive the value from Process 0.\n",
        "   - After the `MPI_Win_fence` call, Process 1 checks the value that has been written into its memory.\n",
        "\n",
        "3. **MPI_Win_fence**:\n",
        "   - The `MPI_Win_fence` call synchronizes the memory operations. Both Process 0 and Process 1 must call this function to ensure that the memory update by Process 0 is completed before Process 1 attempts to access it.\n",
        "\n",
        "### Output:\n",
        "\n",
        "The expected output will show Process 1 receiving the value written by Process 0:\n"
      ],
      "metadata": {
        "id": "wJ92yHo7c1ig"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Synchronization in One-Sided Communication in MPI\n",
        "\n",
        "In **MPI**, one-sided communication allows processes to directly read from or write to the memory of another process. Synchronization is crucial when performing **Remote Memory Access (RMA)** operations to ensure data consistency and prevent race conditions.\n",
        "\n",
        "### Synchronization Methods in One-Sided Communication\n",
        "\n",
        "- **MPI_Win_fence**: This is a simple synchronization method that acts as a barrier for RMA operations. It ensures that all communication completes before the next operation begins. Each process participating in RMA calls `MPI_Win_fence` to mark the start and end of the communication epoch.\n",
        "  \n",
        "- **MPI_Win_lock and MPI_Win_unlock**: These functions are used to lock memory for exclusive or shared access, preventing race conditions during RMA operations. However, this example will focus on `MPI_Win_fence`.\n",
        "\n",
        "In the following example, Process 0 will write a value to Process 1's memory using **MPI_Put**, and Process 1 will read the value after synchronization.\n",
        "\n",
        "### Key Functions:\n",
        "- **MPI_Win_create**: Defines a window of memory that other processes can access.\n",
        "- **MPI_Put**: Allows one process to write data directly to the memory of another process.\n",
        "- **MPI_Win_fence**: Synchronizes RMA operations to ensure that the data is available before any process attempts to access it.\n",
        "\n"
      ],
      "metadata": {
        "id": "DoaBgiefc86s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Write the corrected MPI C program\n",
        "mpi_code_corrected = \"\"\"\n",
        "#include <mpi.h>\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "int main(int argc, char *argv[]) {\n",
        "    int rc;\n",
        "    rc = MPI_Init(&argc, &argv);\n",
        "    if (rc != MPI_SUCCESS) {\n",
        "        printf(\"Error initializing MPI.\\\\n\");\n",
        "        MPI_Abort(MPI_COMM_WORLD, rc);\n",
        "    }\n",
        "\n",
        "    int rank, size;\n",
        "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
        "    MPI_Comm_size(MPI_COMM_WORLD, &size);\n",
        "\n",
        "    if (size < 2) {\n",
        "        if (rank == 0) {\n",
        "            printf(\"This program requires at least two MPI processes.\\\\n\");\n",
        "        }\n",
        "        MPI_Finalize();\n",
        "        return 0;\n",
        "    }\n",
        "\n",
        "    int *window_data;\n",
        "    MPI_Win win;\n",
        "\n",
        "    // Allocate memory for the window and create it\n",
        "    rc = MPI_Win_allocate(sizeof(int), sizeof(int), MPI_INFO_NULL, MPI_COMM_WORLD, &window_data, &win);\n",
        "    if (rc != MPI_SUCCESS) {\n",
        "        printf(\"Error allocating MPI Window.\\\\n\");\n",
        "        MPI_Abort(MPI_COMM_WORLD, rc);\n",
        "    }\n",
        "\n",
        "    // Initialize window data\n",
        "    *window_data = 0;\n",
        "\n",
        "    printf(\"Process %d: MPI Window allocated and initialized to %d.\\\\n\", rank, *window_data);\n",
        "\n",
        "    // Start of RMA epoch\n",
        "    rc = MPI_Win_fence(0, win);\n",
        "    if (rc != MPI_SUCCESS) {\n",
        "        printf(\"Error in MPI_Win_fence (start).\\\\n\");\n",
        "        MPI_Abort(MPI_COMM_WORLD, rc);\n",
        "    }\n",
        "\n",
        "    if (rank == 0) {\n",
        "        int value = 42;\n",
        "        printf(\"Process %d: Putting value %d to process %d's window.\\\\n\", rank, value, 1);\n",
        "        rc = MPI_Put(&value, 1, MPI_INT, 1, 0, 1, MPI_INT, win);\n",
        "        if (rc != MPI_SUCCESS) {\n",
        "            printf(\"Error in MPI_Put.\\\\n\");\n",
        "            MPI_Abort(MPI_COMM_WORLD, rc);\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // End of RMA epoch\n",
        "    rc = MPI_Win_fence(0, win);\n",
        "    if (rc != MPI_SUCCESS) {\n",
        "        printf(\"Error in MPI_Win_fence (end).\\\\n\");\n",
        "        MPI_Abort(MPI_COMM_WORLD, rc);\n",
        "    }\n",
        "\n",
        "    if (rank == 1) {\n",
        "        printf(\"Process %d received data: %d\\\\n\", rank, *window_data);\n",
        "    }\n",
        "\n",
        "    // Cleanup\n",
        "    MPI_Win_free(&win);\n",
        "    MPI_Finalize();\n",
        "    return 0;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Save the MPI C program to a file\n",
        "with open(\"mpi_one_sided_corrected.c\", \"w\") as mpi_file:\n",
        "    mpi_file.write(mpi_code_corrected)\n",
        "\n",
        "print(\"MPI C program written to 'mpi_one_sided_corrected.c'.\")\n",
        "\n",
        "# Step 2: Compile the MPI program\n",
        "!mpicc -o mpi_one_sided_corrected mpi_one_sided_corrected.c\n",
        "\n",
        "# Check if the executable was created\n",
        "import os\n",
        "if os.path.exists(\"mpi_one_sided_corrected\"):\n",
        "    print(\"Compilation successful. Executable 'mpi_one_sided_corrected' created.\")\n",
        "else:\n",
        "    print(\"Compilation failed. Please check the C code for errors.\")\n",
        "\n",
        "# Step 3: Set environment variables to allow running as root\n",
        "os.environ['OMPI_ALLOW_RUN_AS_ROOT'] = '1'\n",
        "os.environ['OMPI_ALLOW_RUN_AS_ROOT_CONFIRM'] = '1'\n",
        "\n",
        "print(\"Environment variables set to allow MPI to run as root.\")\n",
        "\n",
        "# Step 4: Execute the MPI program\n",
        "print(\"Executing the MPI program...\\n\")\n",
        "\n",
        "!mpirun --allow-run-as-root --oversubscribe -np 2 ./mpi_one_sided_corrected\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9V2CSInYeQFA",
        "outputId": "07e2af57-7c1f-462c-cfd2-37947d30843a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MPI C program written to 'mpi_one_sided_corrected.c'.\n",
            "Compilation successful. Executable 'mpi_one_sided_corrected' created.\n",
            "Environment variables set to allow MPI to run as root.\n",
            "Executing the MPI program...\n",
            "\n",
            "Process 0: MPI Window allocated and initialized to 0.\n",
            "Process 1: MPI Window allocated and initialized to 0.\n",
            "Process 0: Putting value 42 to process 1's window.\n",
            "Process 1 received data: 42\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explanation of the Code\n",
        "\n",
        "This example demonstrates **one-sided communication** between two processes using **MPI_Win_fence** for synchronization.\n",
        "\n",
        "1. **Process 0 (Writer)**:\n",
        "   - Process 0 creates an RMA window, but it does not use the memory in the window itself; it writes data directly into Process 1's memory.\n",
        "   - **MPI_Put** is used to write the value `42` into the memory of Process 1.\n",
        "   - **MPI_Win_fence** is called twice:\n",
        "     - First to start the RMA epoch before writing data.\n",
        "     - Second to end the RMA epoch after writing data to ensure synchronization.\n",
        "\n",
        "2. **Process 1 (Reader)**:\n",
        "   - Process 1 creates a window to expose its memory to Process 0.\n",
        "   - After calling **MPI_Win_fence** (which acts as a synchronization barrier), Process 1 checks the value written by Process 0.\n",
        "   - The second **MPI_Win_fence** call ensures that Process 1 only reads the value after the data has been fully written by Process 0.\n",
        "\n"
      ],
      "metadata": {
        "id": "S6VAyBWwc90f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Persistent Communication in MPI\n",
        "\n",
        "In **MPI**, persistent communication requests are useful when the communication pattern repeats many times, such as in iterative algorithms (e.g., solving systems of equations with the Jacobi method). Instead of repeatedly setting up and tearing down communication requests, persistent communication allows you to initialize the requests once and reuse them throughout multiple iterations.\n",
        "\n",
        "### Workflow for Persistent Communication\n",
        "\n",
        "1. **Initialize**: Create persistent communication requests using `MPI_Send_init` and `MPI_Recv_init`.\n",
        "2. **Start**: Begin communication in each iteration using `MPI_Start` or `MPI_Startall` (for multiple requests).\n",
        "3. **Wait**: Wait for the communication to complete using `MPI_Wait` or `MPI_Waitall` (for multiple requests).\n",
        "4. **Free**: After all iterations, release the resources by calling `MPI_Request_free`.\n",
        "\n",
        "This method is especially useful in scenarios where the communication pattern is the same across many iterations. It minimizes the overhead of repeatedly setting up and tearing down requests.\n",
        "\n",
        "### Example Use Case:\n",
        "A typical use case for persistent communication is in iterative solvers, like the **Jacobi method**, where each process exchanges boundary data with its neighbors in each iteration.\n",
        "\n"
      ],
      "metadata": {
        "id": "jq2hwsZdesQB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Writing the MPI persistent communication example to a file\n",
        "mpi_code_persistent = \"\"\"\n",
        "#include <mpi.h>\n",
        "#include <stdio.h>\n",
        "\n",
        "void compute_step() {\n",
        "    // Simulated computation step (could be anything like a Jacobi iteration)\n",
        "    printf(\"Performing computation step...\\\\n\");\n",
        "}\n",
        "\n",
        "int main(int argc, char *argv[]) {\n",
        "    MPI_Init(&argc, &argv);\n",
        "\n",
        "    int rank, size;\n",
        "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
        "    MPI_Comm_size(MPI_COMM_WORLD, &size);\n",
        "\n",
        "    int max_iters = 5;\n",
        "    int neighbor = (rank + 1) % 2;  // Simple neighbor setup for 2 processes\n",
        "    int send_data = rank + 1;       // Data to send (each process sends its rank+1)\n",
        "    int recv_data = 0;              // Buffer to receive data\n",
        "    MPI_Request send_req, recv_req;\n",
        "\n",
        "    // Initialize persistent communication requests\n",
        "    MPI_Send_init(&send_data, 1, MPI_INT, neighbor, 0, MPI_COMM_WORLD, &send_req);\n",
        "    MPI_Recv_init(&recv_data, 1, MPI_INT, neighbor, 0, MPI_COMM_WORLD, &recv_req);\n",
        "\n",
        "    // Enter the iterative computation loop\n",
        "    for (int iter = 0; iter < max_iters; iter++) {\n",
        "        // Start communication requests\n",
        "        MPI_Startall(2, (MPI_Request[]){send_req, recv_req});\n",
        "\n",
        "        // Wait for communication to complete\n",
        "        MPI_Waitall(2, (MPI_Request[]){send_req, recv_req}, MPI_STATUS_IGNORE);\n",
        "\n",
        "        // Perform computation (can be any function)\n",
        "        compute_step();\n",
        "\n",
        "        // Display received data\n",
        "        printf(\"Rank %d received data: %d in iteration %d\\\\n\", rank, recv_data, iter);\n",
        "    }\n",
        "\n",
        "    // Free persistent communication requests\n",
        "    MPI_Request_free(&send_req);\n",
        "    MPI_Request_free(&recv_req);\n",
        "\n",
        "    MPI_Finalize();\n",
        "    return 0;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Save the C program to a file\n",
        "with open(\"mpi_persistent.c\", \"w\") as mpi_file:\n",
        "    mpi_file.write(mpi_code_persistent)\n",
        "\n",
        "# Compile the C program\n",
        "!mpicc -o mpi_persistent mpi_persistent.c\n",
        "\n",
        "# Set environment variables to allow running as root\n",
        "import os\n",
        "os.environ['OMPI_ALLOW_RUN_AS_ROOT'] = '1'\n",
        "os.environ['OMPI_ALLOW_RUN_AS_ROOT_CONFIRM'] = '1'\n",
        "\n",
        "# Run the MPI program with 2 processes, allowing persistent communication\n",
        "!mpirun --allow-run-as-root --oversubscribe -np 2 ./mpi_persistent\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LPhXmdb4esqg",
        "outputId": "7f3c31ae-eebd-43d6-9fa1-4b2ae7dda43c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performing computation step...\n",
            "Performing computation step...\n",
            "Rank 1 received data: 1 in iteration 0\n",
            "Rank 0 received data: 2 in iteration 0\n",
            "Performing computation step...\n",
            "Rank 1 received data: 1 in iteration 1\n",
            "Performing computation step...\n",
            "Rank 0 received data: 2 in iteration 1\n",
            "Performing computation step...\n",
            "Performing computation step...\n",
            "Rank 1 received data: 1 in iteration 2\n",
            "Rank 0 received data: 2 in iteration 2\n",
            "Performing computation step...\n",
            "Rank 1 received data: 1 in iteration 3\n",
            "Performing computation step...\n",
            "Rank 0 received data: 2 in iteration 3\n",
            "Performing computation step...\n",
            "Rank 1 received data: 1 in iteration 4\n",
            "Performing computation step...\n",
            "Rank 0 received data: 2 in iteration 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explanation of the Code\n",
        "\n",
        "This example demonstrates **persistent communication** between two processes. The processes send and receive data between themselves in multiple iterations, using **MPI_Send_init** and **MPI_Recv_init** to set up persistent communication requests.\n",
        "\n",
        "1. **Initialization**:\n",
        "   - The program initializes persistent send and receive requests with `MPI_Send_init` and `MPI_Recv_init`.\n",
        "   - Each process will send its `rank + 1` to its neighbor (i.e., Process 0 sends 1, Process 1 sends 2).\n",
        "\n",
        "2. **Communication in Iterations**:\n",
        "   - In each iteration, `MPI_Startall` is used to start both the send and receive operations.\n",
        "   - `MPI_Waitall` ensures that the communication is complete before the next computation step.\n",
        "   - The `compute_step()` function is a placeholder for any actual computation that would be done after each communication round.\n",
        "\n",
        "3. **Freeing Requests**:\n",
        "   - After completing all iterations, the program frees the persistent communication requests using `MPI_Request_free`.\n"
      ],
      "metadata": {
        "id": "8yNcKNh1e1EQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Non-Blocking Point-to-Point Communication in MPI\n",
        "\n",
        "In **MPI**, non-blocking communication allows processes to continue computation while the communication is being performed in the background. This is in contrast to blocking communication, where a process waits until the communication is complete before proceeding.\n",
        "\n",
        "### Functions:\n",
        "- **MPI_Isend**: Initiates a non-blocking send operation.\n",
        "- **MPI_Irecv**: Initiates a non-blocking receive operation.\n",
        "- **MPI_Wait**: Ensures that the non-blocking operation completes.\n",
        "\n",
        "### Advantages of Non-Blocking Communication:\n",
        "- Non-blocking communication allows processes to overlap computation with communication, improving performance by reducing idle time.\n",
        "- It is particularly useful in situations where communication may take a significant amount of time, such as in large distributed systems.\n",
        "\n",
        "### Example Overview:\n",
        "- **Process 0** sends its rank to **Process 1** using `MPI_Isend`.\n",
        "- Both processes perform some simulated computation while the communication is happening.\n",
        "- After completing the computation, they use `MPI_Wait` to ensure the communication is complete before proceeding.\n",
        "\n",
        "In this example, **Process 1** will receive data from **Process 0**, while both perform computation in parallel to the data transfer.\n"
      ],
      "metadata": {
        "id": "dtvrxpldfSAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Writing the updated MPI non-blocking point-to-point communication example to a file\n",
        "mpi_code_nonblocking = \"\"\"\n",
        "#include <mpi.h>\n",
        "#include <stdio.h>\n",
        "\n",
        "int main(int argc, char *argv[]) {\n",
        "    MPI_Init(&argc, &argv);\n",
        "    int rank;\n",
        "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
        "\n",
        "    int buffer_send = rank + 1;  // Modify the sent value to be rank + 1\n",
        "    int buffer_recv = 0;\n",
        "    MPI_Request req_send, req_recv;\n",
        "\n",
        "    if (rank == 0) {\n",
        "        // Non-blocking send from Process 0 to Process 1\n",
        "        MPI_Isend(&buffer_send, 1, MPI_INT, 1, 0, MPI_COMM_WORLD, &req_send);\n",
        "        // Simulated computation after initiating the send\n",
        "        for (int i = 0; i < 100000; i++) {}  // Simulated computation\n",
        "        MPI_Wait(&req_send, MPI_STATUS_IGNORE);  // Wait for send to complete\n",
        "        printf(\"Process 0 finished sending.\\\\n\");\n",
        "    } else if (rank == 1) {\n",
        "        // Non-blocking receive by Process 1 from Process 0\n",
        "        MPI_Irecv(&buffer_recv, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, &req_recv);\n",
        "        // Simulated computation after initiating the receive\n",
        "        for (int i = 0; i < 100000; i++) {}  // Simulated computation\n",
        "        MPI_Wait(&req_recv, MPI_STATUS_IGNORE);  // Wait for receive to complete\n",
        "        printf(\"Received: %d from Process 0\\\\n\", buffer_recv);  // Print the received value\n",
        "    }\n",
        "    MPI_Finalize();\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Save the C program to a file\n",
        "with open(\"mpi_nonblocking.c\", \"w\") as mpi_file:\n",
        "    mpi_file.write(mpi_code_nonblocking)\n",
        "\n",
        "# Compile the C program\n",
        "!mpicc -o mpi_nonblocking mpi_nonblocking.c\n",
        "\n",
        "# Set environment variables to allow running as root\n",
        "import os\n",
        "os.environ['OMPI_ALLOW_RUN_AS_ROOT'] = '1'\n",
        "os.environ['OMPI_ALLOW_RUN_AS_ROOT_CONFIRM'] = '1'\n",
        "\n",
        "# Run the MPI program with 2 processes, demonstrating non-blocking communication\n",
        "!mpirun --allow-run-as-root --oversubscribe -np 2 ./mpi_nonblocking\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jsyZR6bRhF5W",
        "outputId": "adbb61a2-21d0-42a7-f4e1-a30cb42e3691"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Received: 1 from Process 0\n",
            "Process 0 finished sending.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explanation of the Code\n",
        "\n",
        "This example demonstrates **non-blocking point-to-point communication** between two processes using `MPI_Isend` and `MPI_Irecv`.\n",
        "\n",
        "1. **Process 0 (Sender)**:\n",
        "   - **Non-blocking Send**: `MPI_Isend` is used to initiate a non-blocking send of its rank to **Process 1**.\n",
        "   - **Simulated Computation**: While the send is happening, **Process 0** performs some computation (simulated by a loop).\n",
        "   - **MPI_Wait**: After the computation, **Process 0** waits for the send operation to complete using `MPI_Wait`.\n",
        "\n",
        "2. **Process 1 (Receiver)**:\n",
        "   - **Non-blocking Receive**: `MPI_Irecv` is used to initiate a non-blocking receive of the data from **Process 0**.\n",
        "   - **Simulated Computation**: While the receive is happening, **Process 1** also performs some computation.\n",
        "   - **MPI_Wait**: After the computation, **Process 1** waits for the receive operation to complete using `MPI_Wait`.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5PJrp3_-fSqh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Non-Blocking Collective Operations in MPI\n",
        "\n",
        "In **MPI**, non-blocking collective operations allow processes to start a collective operation, such as a broadcast or reduction, and continue performing computation while waiting for the operation to complete. This helps avoid the bottlenecks that can arise when processes wait for each other to complete collective operations.\n",
        "\n",
        "### Benefits of Non-Blocking Collectives:\n",
        "- Processes do not have to wait for the collective operation to complete before proceeding with computation.\n",
        "- It is particularly useful in applications where collective operations involve large datasets or where synchronization between processes can cause delays.\n",
        "\n",
        "### Functions:\n",
        "- **MPI_Ibcast**: Non-blocking version of `MPI_Bcast`, used to broadcast data from one process to all others.\n",
        "- **MPI_Ireduce**: Non-blocking version of `MPI_Reduce`, used to reduce data from all processes to a single result (e.g., sum, max).\n",
        "- **MPI_Wait**: Used to ensure the collective operation has completed before using the results.\n",
        "\n",
        "### Example Overview:\n",
        "In this example, we will use **MPI_Ireduce** to perform a non-blocking reduction operation. While the reduction is happening, the processes will perform some simulated computation. After the computation is done, the program will use `MPI_Wait` to ensure the reduction is complete before accessing the result.\n",
        "\n",
        "The reduction operation sums the values from all processes, and the result will be stored in the root process (Process 0).\n"
      ],
      "metadata": {
        "id": "LnNBnwX4gecd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Writing the MPI non-blocking collective operation example to a file\n",
        "mpi_code_nonblocking_collective = \"\"\"\n",
        "#include <mpi.h>\n",
        "#include <stdio.h>\n",
        "\n",
        "int main(int argc, char *argv[]) {\n",
        "    MPI_Init(&argc, &argv);\n",
        "\n",
        "    int rank, size;\n",
        "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
        "    MPI_Comm_size(MPI_COMM_WORLD, &size);\n",
        "\n",
        "    int sendbuf = rank + 1;  // Each process sends its rank + 1\n",
        "    int recvbuf = 0;         // The result will be stored in the root (rank 0)\n",
        "    MPI_Request req;\n",
        "\n",
        "    // Non-blocking reduction (sum) operation\n",
        "    MPI_Ireduce(&sendbuf, &recvbuf, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD, &req);\n",
        "\n",
        "    // Simulated computation while the reduction is happening\n",
        "    for (int i = 0; i < 100000; i++) {\n",
        "        // Perform some dummy computation\n",
        "        if (i % 20000 == 0 && rank == 0) {\n",
        "            printf(\"Process 0 doing computation during reduction...\\\\n\");\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // Wait for the non-blocking reduction to complete\n",
        "    MPI_Wait(&req, MPI_STATUS_IGNORE);\n",
        "\n",
        "    // Root process prints the result\n",
        "    if (rank == 0) {\n",
        "        printf(\"The sum of ranks is: %d\\\\n\", recvbuf);\n",
        "    }\n",
        "\n",
        "    MPI_Finalize();\n",
        "    return 0;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Save the C program to a file\n",
        "with open(\"mpi_nonblocking_collective.c\", \"w\") as mpi_file:\n",
        "    mpi_file.write(mpi_code_nonblocking_collective)\n",
        "\n",
        "# Compile the C program\n",
        "!mpicc -o mpi_nonblocking_collective mpi_nonblocking_collective.c\n",
        "\n",
        "# Set environment variables to allow running as root\n",
        "import os\n",
        "os.environ['OMPI_ALLOW_RUN_AS_ROOT'] = '1'\n",
        "os.environ['OMPI_ALLOW_RUN_AS_ROOT_CONFIRM'] = '1'\n",
        "\n",
        "# Run the MPI program with 4 processes, demonstrating non-blocking reduction\n",
        "!mpirun --allow-run-as-root --oversubscribe -np 4 ./mpi_nonblocking_collective\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZwCbBVHge2k",
        "outputId": "c5c85603-a060-44a8-9ff3-95cea0157094"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Process 0 doing computation during reduction...\n",
            "Process 0 doing computation during reduction...\n",
            "Process 0 doing computation during reduction...\n",
            "Process 0 doing computation during reduction...\n",
            "Process 0 doing computation during reduction...\n",
            "The sum of ranks is: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explanation of the Code\n",
        "\n",
        "This example demonstrates how to use **non-blocking collective operations** in MPI, specifically `MPI_Ireduce`, to perform a reduction operation while continuing computation.\n",
        "\n",
        "1. **Reduction Operation**:\n",
        "   - Each process sends its rank + 1 to the reduction operation.\n",
        "   - `MPI_Ireduce` is used to sum the values across all processes, and the result is stored in **Process 0**.\n",
        "   - Since this is a non-blocking operation, the processes do not wait for the reduction to complete immediately.\n",
        "\n",
        "2. **Simulated Computation**:\n",
        "   - While the reduction is happening in the background, the processes perform some dummy computation. In this case, the computation is simulated with a simple loop.\n",
        "   - **Process 0** prints messages during the computation to show that it is performing work while the reduction is ongoing.\n",
        "\n",
        "3. **Waiting for Completion**:\n",
        "   - After the computation is done, `MPI_Wait` is called to ensure that the non-blocking reduction has completed.\n",
        "   - Once the reduction is complete, **Process 0** prints the result of the reduction.\n",
        "\n",
        "### Output:\n",
        "The expected output will show that **Process 0** performs computation while the reduction is happening:\n"
      ],
      "metadata": {
        "id": "wAhHTMq4gfGM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Message Aggregation in MPI\n",
        "\n",
        "**Message aggregation** is a technique used to reduce communication overhead by combining multiple small messages into a larger message. This minimizes the number of times the startup latency of communication is incurred, especially in high-latency networks. Instead of sending multiple small messages, we combine them into a single structure and send it as one message.\n",
        "\n",
        "### Benefits of Message Aggregation:\n",
        "- Reduces the number of communication calls.\n",
        "- Reduces the startup latency associated with each message.\n",
        "- Optimizes the use of bandwidth by sending larger, aggregated messages.\n",
        "\n",
        "### Example:\n",
        "In this example, we will define a `data_packet` structure that contains information about temperature, pressure, and humidity. Instead of sending these values individually, we will aggregate them into a single packet and send them as a **binary message** (`MPI_BYTE`) from **Process 0** to **Process 1**.\n",
        "\n",
        "The structure we will use is:\n",
        "```c\n",
        "struct data_packet {\n",
        "    double temperature;\n",
        "    double pressure;\n",
        "    double humidity;\n",
        "};\n"
      ],
      "metadata": {
        "id": "6b-__kEEfssq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Writing the MPI message aggregation example to a file\n",
        "mpi_code_message_aggregation = \"\"\"\n",
        "#include <mpi.h>\n",
        "#include <stdio.h>\n",
        "\n",
        "// Define the data_packet structure\n",
        "struct data_packet {\n",
        "    double temperature;\n",
        "    double pressure;\n",
        "    double humidity;\n",
        "};\n",
        "\n",
        "int main(int argc, char *argv[]) {\n",
        "    MPI_Init(&argc, &argv);\n",
        "\n",
        "    int rank;\n",
        "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
        "\n",
        "    struct data_packet packet;\n",
        "\n",
        "    if (rank == 0) {\n",
        "        // Process 0 prepares the data to send\n",
        "        packet.temperature = 23.4;\n",
        "        packet.pressure = 1013.5;\n",
        "        packet.humidity = 45.6;\n",
        "\n",
        "        // Send the aggregated message as a single binary message (MPI_BYTE)\n",
        "        MPI_Send(&packet, sizeof(packet), MPI_BYTE, 1, 0, MPI_COMM_WORLD);\n",
        "        printf(\"Process 0 sent data: Temp=%.1f, Pressure=%.1f, Humidity=%.1f\\\\n\",\n",
        "               packet.temperature, packet.pressure, packet.humidity);\n",
        "    } else if (rank == 1) {\n",
        "        // Process 1 receives the data_packet structure\n",
        "        MPI_Recv(&packet, sizeof(packet), MPI_BYTE, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n",
        "        printf(\"Process 1 received data: Temp=%.1f, Pressure=%.1f, Humidity=%.1f\\\\n\",\n",
        "               packet.temperature, packet.pressure, packet.humidity);\n",
        "    }\n",
        "\n",
        "    MPI_Finalize();\n",
        "    return 0;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Save the C program to a file\n",
        "with open(\"mpi_message_aggregation.c\", \"w\") as mpi_file:\n",
        "    mpi_file.write(mpi_code_message_aggregation)\n",
        "\n",
        "# Compile the C program\n",
        "!mpicc -o mpi_message_aggregation mpi_message_aggregation.c\n",
        "\n",
        "# Set environment variables to allow running as root\n",
        "import os\n",
        "os.environ['OMPI_ALLOW_RUN_AS_ROOT'] = '1'\n",
        "os.environ['OMPI_ALLOW_RUN_AS_ROOT_CONFIRM'] = '1'\n",
        "\n",
        "# Run the MPI program with 2 processes, demonstrating message aggregation\n",
        "!mpirun --allow-run-as-root --oversubscribe -np 2 ./mpi_message_aggregation\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yMcW064Fft3I",
        "outputId": "b178c829-31b1-414e-838b-98b65353c0e5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Process 0 sent data: Temp=23.4, Pressure=1013.5, Humidity=45.6\n",
            "Process 1 received data: Temp=23.4, Pressure=1013.5, Humidity=45.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explanation of the Code\n",
        "\n",
        "This example demonstrates **message aggregation** by sending a single structure containing multiple data fields from **Process 0** to **Process 1**. The structure contains three fields: temperature, pressure, and humidity.\n",
        "\n",
        "1. **Structure Definition**:\n",
        "   - We define a structure `data_packet` that contains three `double` values: `temperature`, `pressure`, and `humidity`.\n",
        "\n",
        "2. **Process 0 (Sender)**:\n",
        "   - **Process 0** initializes the structure with specific values for temperature, pressure, and humidity.\n",
        "   - The structure is sent to **Process 1** using `MPI_Send`. Instead of sending each field individually, the entire structure is sent as a single message using the `MPI_BYTE` data type, which treats the structure as a raw block of memory.\n",
        "\n",
        "3. **Process 1 (Receiver)**:\n",
        "   - **Process 1** receives the entire structure in one go using `MPI_Recv`. The received structure is unpacked directly into a `data_packet` variable.\n",
        "   - **Process 1** prints the received temperature, pressure, and humidity values.\n",
        "\n"
      ],
      "metadata": {
        "id": "UahrP2w4gCtK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Topology-Aware Communication in MPI\n",
        "\n",
        "**Topology-aware communication** helps improve the performance of parallel applications by mapping MPI processes to match the underlying hardware topology. This reduces the number of communication \"hops\" between processes, which is particularly important in large-scale systems where network latency can affect performance.\n",
        "\n",
        "### Benefits of Topology-Aware Communication:\n",
        "- Aligns the logical MPI process grid with the physical hardware layout.\n",
        "- Minimizes communication distance (number of hops) between processes.\n",
        "- Reduces network congestion and improves overall application performance.\n",
        "\n",
        "### Function:\n",
        "- **MPI_Cart_create**: Creates a Cartesian grid topology that organizes processes into a structured grid. This allows for efficient neighbor communication, such as in simulations that involve grids or meshes (e.g., computational fluid dynamics).\n",
        "\n",
        "### Example Overview:\n",
        "In this example, we will create a 2D Cartesian grid of processes using `MPI_Cart_create`. The grid will help map processes logically and reduce the number of hops between communicating neighbors.\n",
        "\n",
        "- **dims**: Specifies the dimensions of the Cartesian grid (e.g., 2D grid with `x_size` and `y_size`).\n",
        "- **periods**: Specifies whether the grid should have periodic boundaries (e.g., for toroidal grids).\n",
        "- **cart_comm**: The communicator that will represent the new Cartesian grid.\n",
        "\n",
        "We will then print the coordinates of each process in the Cartesian grid.\n"
      ],
      "metadata": {
        "id": "sfZrilsUgK9C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Writing the MPI topology-aware communication example to a file\n",
        "mpi_code_topology_aware = \"\"\"\n",
        "#include <mpi.h>\n",
        "#include <stdio.h>\n",
        "\n",
        "int main(int argc, char *argv[]) {\n",
        "    MPI_Init(&argc, &argv);\n",
        "\n",
        "    int rank, size;\n",
        "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
        "    MPI_Comm_size(MPI_COMM_WORLD, &size);\n",
        "\n",
        "    // Define dimensions of the Cartesian grid (2D grid for simplicity)\n",
        "    int dims[2] = {0, 0};\n",
        "    MPI_Dims_create(size, 2, dims);  // Automatically compute grid dimensions\n",
        "\n",
        "    // No periodic boundaries\n",
        "    int periods[2] = {0, 0};\n",
        "    MPI_Comm cart_comm;\n",
        "\n",
        "    // Create the Cartesian grid topology\n",
        "    MPI_Cart_create(MPI_COMM_WORLD, 2, dims, periods, 1, &cart_comm);\n",
        "\n",
        "    // Get the coordinates of each process in the Cartesian grid\n",
        "    int coords[2];\n",
        "    MPI_Cart_coords(cart_comm, rank, 2, coords);\n",
        "\n",
        "    // Print the rank and the Cartesian coordinates of each process\n",
        "    printf(\"Process %d is at coordinates (%d, %d) in the Cartesian grid.\\\\n\",\n",
        "           rank, coords[0], coords[1]);\n",
        "\n",
        "    MPI_Finalize();\n",
        "    return 0;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Save the C program to a file\n",
        "with open(\"mpi_topology_aware.c\", \"w\") as mpi_file:\n",
        "    mpi_file.write(mpi_code_topology_aware)\n",
        "\n",
        "# Compile the C program\n",
        "!mpicc -o mpi_topology_aware mpi_topology_aware.c\n",
        "\n",
        "# Set environment variables to allow running as root\n",
        "import os\n",
        "os.environ['OMPI_ALLOW_RUN_AS_ROOT'] = '1'\n",
        "os.environ['OMPI_ALLOW_RUN_AS_ROOT_CONFIRM'] = '1'\n",
        "\n",
        "# Run the MPI program with 4 processes, demonstrating Cartesian grid topology\n",
        "!mpirun --allow-run-as-root --oversubscribe -np 4 ./mpi_topology_aware\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spmkCG95gLR6",
        "outputId": "7edea465-9354-42f5-f96a-4b23d317c085"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Process 0 is at coordinates (0, 0) in the Cartesian grid.\n",
            "Process 1 is at coordinates (0, 1) in the Cartesian grid.\n",
            "Process 3 is at coordinates (1, 1) in the Cartesian grid.\n",
            "Process 2 is at coordinates (1, 0) in the Cartesian grid.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explanation of the Code\n",
        "\n",
        "This example demonstrates how to use **MPI_Cart_create** to organize MPI processes into a Cartesian grid topology. Each process is placed in a 2D grid, and we print the coordinates of each process within the grid.\n",
        "\n",
        "1. **Defining the Grid Dimensions**:\n",
        "   - `MPI_Dims_create` automatically computes the dimensions of the Cartesian grid based on the total number of processes (`size`). In this case, we are creating a 2D grid.\n",
        "\n",
        "2. **Creating the Cartesian Communicator**:\n",
        "   - `MPI_Cart_create` creates the Cartesian grid topology. The `periods` array specifies whether the grid has periodic boundaries (for example, wrapping around the edges like a toroidal grid). In this case, the grid does not have periodic boundaries.\n",
        "\n",
        "3. **Retrieving Process Coordinates**:\n",
        "   - `MPI_Cart_coords` retrieves the coordinates of each process in the Cartesian grid. This allows us to determine where each process is located within the grid.\n",
        "\n",
        "4. **Printing Coordinates**:\n",
        "   - Each process prints its rank and the corresponding coordinates in the grid.\n",
        "\n",
        "### Output:\n",
        "The expected output will show the coordinates of each process within the Cartesian grid:\n"
      ],
      "metadata": {
        "id": "Jh5Ep1lygLp6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction to Matrix Multiplication using MPI\n",
        "\n",
        "Matrix multiplication is a computationally intensive task, especially when the size of matrices increases. Serial implementations become impractical when dealing with large matrices, making parallelization necessary for efficient computation.\n",
        "\n",
        "### Parallelizing Matrix Multiplication using MPI\n",
        "\n",
        "In this section, we will explore how to implement matrix multiplication using Message Passing Interface (MPI). We will begin with a basic parallel implementation where the matrix data is divided and distributed across multiple processes. Then, we will enhance this basic version by introducing advanced MPI features such as:\n",
        "- **Dynamic Process Management**: Dynamically adjusting the number of worker processes during execution.\n",
        "- **One-Sided Communication**: Using Remote Memory Access (RMA) for asynchronous communication.\n",
        "- **Persistent Communication Requests**: Reusing communication handles to reduce overhead in repetitive operations.\n",
        "- **Non-Blocking Collective Operations**: Overlapping communication with computation to optimize performance.\n",
        "\n",
        "Let's start by implementing the basic parallel matrix multiplication using MPI.\n",
        "\n",
        "### Serial Matrix Multiplication Overview\n",
        "\n",
        "In serial matrix multiplication, we compute each element of the result matrix `C` by taking the dot product of a row from matrix `A` and a column from matrix `B`. The following code illustrates this process:\n",
        "\n",
        "```c\n",
        "for (int i = 0; i < N; i++) {\n",
        "    for (int j = 0; j < P; j++) {\n",
        "        C[i][j] = 0;\n",
        "        for (int k = 0; k < M; k++) {\n",
        "            C[i][j] += A[i][k] * B[k][j];\n",
        "        }\n",
        "    }\n",
        "}\n",
        "```\n",
        "\n",
        "\n",
        "However, as the matrix size grows, the memory and computational requirements grow cubically, necessitating parallel computation.\n",
        "\n",
        "### Step 1: Basic MPI for Distributed Matrix Multiplication\n",
        "The first step in parallelizing the matrix multiplication is distributing the data across multiple processes. Each process computes a subset of the result matrix C. We use the following MPI functions:\n",
        "\n",
        "- MPI_Scatter: Distributes blocks of matrix A to each process.\n",
        "- MPI_Bcast: Broadcasts matrix B to all processes.\n",
        "- MPI_Gather: Collects the computed blocks of C back to the master process.\n"
      ],
      "metadata": {
        "id": "BvOpV0EChw1l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Writing the corrected MPI matrix multiplication example to a file\n",
        "mpi_code_matrix_multiplication = r\"\"\"\n",
        "#include <mpi.h>\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "#define MASTER 0\n",
        "\n",
        "void initialize_matrix(double* matrix, int rows, int cols) {\n",
        "    for (int i = 0; i < rows; i++) {\n",
        "        for (int j = 0; j < cols; j++) {\n",
        "            matrix[i * cols + j] = rand() % 10;  // Initialize with random values\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "int main(int argc, char *argv[]) {\n",
        "    int rank, size, N = 4, M = 4, P = 4, rows_per_proc;\n",
        "    double *A, *B, *C, *local_A, *C_part;\n",
        "\n",
        "    MPI_Init(&argc, &argv);                        // Initialize MPI\n",
        "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);          // Get process rank\n",
        "    MPI_Comm_size(MPI_COMM_WORLD, &size);          // Get total number of processes\n",
        "\n",
        "    if (N % size != 0) {\n",
        "        if (rank == MASTER) {\n",
        "            printf(\"N (%d) is not divisible by the number of processes (%d).\\n\", N, size);\n",
        "        }\n",
        "        MPI_Finalize();\n",
        "        return -1;\n",
        "    }\n",
        "\n",
        "    rows_per_proc = N / size;  // Divide the rows among processes\n",
        "\n",
        "    // Allocate memory for matrix B on all processes\n",
        "    B = (double*) malloc(M * P * sizeof(double));\n",
        "    if (B == NULL) {\n",
        "        printf(\"Process %d: Unable to allocate memory for matrix B.\\n\", rank);\n",
        "        MPI_Finalize();\n",
        "        return -1;\n",
        "    }\n",
        "\n",
        "    // Allocate memory for matrices on MASTER\n",
        "    if (rank == MASTER) {\n",
        "        A = (double*) malloc(N * M * sizeof(double));\n",
        "        C = (double*) malloc(N * P * sizeof(double));\n",
        "\n",
        "        if (A == NULL || C == NULL) {\n",
        "            printf(\"MASTER: Unable to allocate memory for matrices A or C.\\n\");\n",
        "            MPI_Finalize();\n",
        "            return -1;\n",
        "        }\n",
        "\n",
        "        initialize_matrix(A, N, M);  // Initialize matrix A\n",
        "        initialize_matrix(B, M, P);  // Initialize matrix B\n",
        "    }\n",
        "\n",
        "    // Allocate memory for local_A and C_part on all processes\n",
        "    local_A = (double*) malloc(rows_per_proc * M * sizeof(double));\n",
        "    C_part = (double*) malloc(rows_per_proc * P * sizeof(double));\n",
        "\n",
        "    if (local_A == NULL || C_part == NULL) {\n",
        "        printf(\"Process %d: Unable to allocate memory for local_A or C_part.\\n\", rank);\n",
        "        MPI_Finalize();\n",
        "        return -1;\n",
        "    }\n",
        "\n",
        "    // Distribute matrix A\n",
        "    MPI_Scatter(A, rows_per_proc * M, MPI_DOUBLE,\n",
        "                local_A, rows_per_proc * M, MPI_DOUBLE,\n",
        "                MASTER, MPI_COMM_WORLD);\n",
        "\n",
        "    // Broadcast matrix B to all processes\n",
        "    MPI_Bcast(B, M * P, MPI_DOUBLE, MASTER, MPI_COMM_WORLD);\n",
        "\n",
        "    // Perform local computation of matrix multiplication\n",
        "    for (int i = 0; i < rows_per_proc; i++) {\n",
        "        for (int j = 0; j < P; j++) {\n",
        "            C_part[i * P + j] = 0;\n",
        "            for (int k = 0; k < M; k++) {\n",
        "                C_part[i * P + j] += local_A[i * M + k] * B[k * P + j];\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // Gather the computed parts of matrix C from all processes\n",
        "    MPI_Gather(C_part, rows_per_proc * P, MPI_DOUBLE,\n",
        "               C, rows_per_proc * P, MPI_DOUBLE,\n",
        "               MASTER, MPI_COMM_WORLD);\n",
        "\n",
        "    // Optionally, MASTER can print the result\n",
        "    if (rank == MASTER) {\n",
        "        printf(\"\\\\nMatrix A:\\\\n\");\n",
        "        for(int i = 0; i < N; i++) {\n",
        "            for(int j = 0; j < M; j++) {\n",
        "                printf(\"%lf \", A[i * M + j]);\n",
        "            }\n",
        "            printf(\"\\\\n\");\n",
        "        }\n",
        "\n",
        "        printf(\"\\\\nMatrix B:\\\\n\");\n",
        "        for(int i = 0; i < M; i++) {\n",
        "            for(int j = 0; j < P; j++) {\n",
        "                printf(\"%lf \", B[i * P + j]);\n",
        "            }\n",
        "            printf(\"\\\\n\");\n",
        "        }\n",
        "\n",
        "        printf(\"\\\\nMatrix C (Result):\\\\n\");\n",
        "        for(int i = 0; i < N; i++) {\n",
        "            for(int j = 0; j < P; j++) {\n",
        "                printf(\"%lf \", C[i * P + j]);\n",
        "            }\n",
        "            printf(\"\\\\n\");\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // Free allocated memory\n",
        "    if (rank == MASTER) {\n",
        "        free(A);\n",
        "        free(C);\n",
        "    }\n",
        "    free(B);\n",
        "    free(local_A);\n",
        "    free(C_part);\n",
        "\n",
        "    MPI_Finalize();  // Finalize MPI\n",
        "    return 0;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Save the corrected C program to a file\n",
        "with open(\"mpi_matrix_multiplication.c\", \"w\") as mpi_file:\n",
        "    mpi_file.write(mpi_code_matrix_multiplication)\n",
        "\n",
        "# Compile the C program\n",
        "!mpicc -o mpi_matrix_multiplication mpi_matrix_multiplication.c\n",
        "\n",
        "# Set environment variables to allow running as root\n",
        "import os\n",
        "os.environ['OMPI_ALLOW_RUN_AS_ROOT'] = '1'\n",
        "os.environ['OMPI_ALLOW_RUN_AS_ROOT_CONFIRM'] = '1'\n",
        "\n",
        "# Run the MPI program with 4 processes\n",
        "!mpirun --allow-run-as-root --oversubscribe -np 4 ./mpi_matrix_multiplication\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3RYOQPfIhxK_",
        "outputId": "d301e872-66b5-4063-b4f3-78d50554dd7f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\\nMatrix A:\\n3.000000 6.000000 7.000000 5.000000 \\n3.000000 5.000000 6.000000 2.000000 \\n9.000000 1.000000 2.000000 7.000000 \\n0.000000 9.000000 3.000000 6.000000 \\n\\nMatrix B:\\n0.000000 6.000000 2.000000 6.000000 \\n1.000000 8.000000 7.000000 9.000000 \\n2.000000 0.000000 2.000000 3.000000 \\n7.000000 5.000000 9.000000 2.000000 \\n\\nMatrix C (Result):\\n55.000000 91.000000 107.000000 103.000000 \\n31.000000 68.000000 71.000000 85.000000 \\n54.000000 97.000000 92.000000 83.000000 \\n57.000000 102.000000 123.000000 102.000000 \\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explanation of the Basic MPI Matrix Multiplication Code\n",
        "\n",
        "1. **MPI Initialization**:\n",
        "   The program starts with `MPI_Init`, which initializes the MPI environment. Each process retrieves its rank (process identifier) and the total number of processes using `MPI_Comm_rank` and `MPI_Comm_size`.\n",
        "\n",
        "2. **Matrix Initialization**:\n",
        "   Matrix `A` and `B` are initialized by the master process (`rank == MASTER`). These matrices are then distributed to all the worker processes using `MPI_Scatter` and `MPI_Bcast`.\n",
        "\n",
        "3. **Matrix Multiplication**:\n",
        "   Each process computes a part of the resulting matrix `C`. Each process is responsible for multiplying a subset of rows of `A` with the entire matrix `B`.\n",
        "\n",
        "4. **Collecting Results**:\n",
        "   After each process completes its computation, the results are gathered back into the full matrix `C` on the master process using `MPI_Gather`.\n",
        "\n",
        "5. **Finalization**:\n",
        "   The program finalizes the MPI environment using `MPI_Finalize`, and all dynamically allocated memory is freed.\n"
      ],
      "metadata": {
        "id": "CFl5x9bFhxfF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "1_7_0ccAiEEd"
      }
    }
  ]
}