{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Practical Implementation: Distributed Deep Learning on Google Colab\n",
        "\n",
        "## Introduction\n",
        "In this exercise, we will walk through the steps to set up and train a deep learning model using **TensorFlow's `tf.distribute.Strategy` API** for distributed training. Google Colab provides free access to GPUs (like the **T4**), and we will leverage this resource for our model.\n",
        "\n",
        "In a real-world scenario, this setup can be expanded to train on multiple GPUs or nodes using high-performance computing (HPC) clusters. You will also learn how distributed training improves efficiency, reduces training time, and makes deep learning models scalable.\n",
        "\n",
        "## Learning Objectives:\n",
        "- Set up Google Colab to use a **T4 GPU**.\n",
        "- Define and configure a **distributed training strategy** using TensorFlow.\n",
        "- Implement a deep learning model for image classification using **CIFAR-10** dataset.\n",
        "- Train the model on multiple devices and observe performance.\n",
        "\n",
        "## Step 1: Set Up the Environment\n",
        "First, make sure you are using a GPU for training. You can verify this by running the following code to check if Colab is connected to a GPU.\n",
        "\n",
        "```\n",
        "import tensorflow as tf\n",
        "\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
        "```\n",
        "\n",
        "Once you verify that a GPU is available, move on to defining your distributed training strategy.\n",
        "\n",
        "##Step 2: Define the Distributed Strategy\n",
        "We will use tf.distribute.MirroredStrategy, which performs synchronous training across multiple GPUs on a single machine. Colab supports a single GPU (T4), but this strategy can be extended to multiple GPUs.\n",
        "\n",
        "```\n",
        "# Define the distribution strategy\n",
        "strategy = tf.distribute.MirroredStrategy()\n",
        "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
        "```\n",
        "\n",
        "The num_replicas_in_sync property shows the number of devices (in this case, the number of GPUs) being used.\n",
        "\n",
        "##Step 3: Load and Preprocess the Data\n",
        "We'll use the CIFAR-10 dataset, a popular dataset for image classification. It's important to preprocess the dataset and ensure that it is distributed efficiently across multiple devices.\n",
        "\n",
        "```\n",
        "# Load CIFAR-10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "# Normalize pixel values to be between 0 and 1\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "# Define batch size and create a distributed dataset\n",
        "batch_size = 64\n",
        "BUFFER_SIZE = len(x_train)\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(BUFFER_SIZE).batch(batch_size)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)\n",
        "```\n",
        "\n",
        "##Step 4: Define the Model within the Strategy Scope\n",
        "When using distributed strategies in TensorFlow, the model must be defined inside the strategy's scope(). This ensures that the variables are mirrored across devices.\n",
        "\n",
        "```\n",
        "with strategy.scope():\n",
        "    # Define a simple CNN model\n",
        "    model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
        "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dense(10)\n",
        "    ])\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                  metrics=['accuracy'])\n",
        "```\n",
        "\n",
        "\n",
        "##Step 5: Train the Model with the Distributed Strategy\n",
        "Now, we'll train the model using the distributed strategy. The training process will be parallelized across the available GPUs (or just one in the case of Colab).\n",
        "\n",
        "```\n",
        "# Train the model\n",
        "history = model.fit(train_dataset, epochs=10, validation_data=test_dataset)\n",
        "```\n",
        "\n",
        "##Step 6: Evaluate the Model\n",
        "Finally, after training, evaluate the model's performance on the test dataset.\n",
        "\n",
        "```\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(test_dataset)\n",
        "print('Test accuracy:', test_acc)\n",
        "```\n",
        "\n",
        "##Step 7: Extend to a Multi-GPU/TPU Environment (Optional)\n",
        "In a more complex setup, like in HPC environments or when using services like Google Cloud, you can extend this strategy to multiple GPUs or TPUs by adjusting the distributed strategy and using tf.distribute.TPUStrategy or tf.distribute.MultiWorkerMirroredStrategy."
      ],
      "metadata": {
        "id": "gP3IST-SfhpE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Check if GPU is available\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
        "\n",
        "# Step 2: Define the distributed strategy\n",
        "strategy = tf.distribute.MirroredStrategy()\n",
        "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
        "\n",
        "# Step 3: Load and preprocess the CIFAR-10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "# Normalize pixel values to be between 0 and 1\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "# Define batch size and create distributed datasets\n",
        "batch_size = 64\n",
        "BUFFER_SIZE = len(x_train)\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(BUFFER_SIZE).batch(batch_size)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)\n",
        "\n",
        "# Step 4: Define the model within the strategy scope\n",
        "with strategy.scope():\n",
        "    model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
        "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dense(10)\n",
        "    ])\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "# Step 5: Train the model\n",
        "history = model.fit(train_dataset, epochs=10, validation_data=test_dataset)\n",
        "\n",
        "# Step 6: Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(test_dataset)\n",
        "print('Test accuracy:', test_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-3q1qoSbodq",
        "outputId": "bee78207-cddb-4067-e014-260eacb1d805"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  1\n",
            "Number of devices: 1\n",
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - accuracy: 0.3734 - loss: 1.7419 - val_accuracy: 0.5483 - val_loss: 1.2766\n",
            "Epoch 2/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 11ms/step - accuracy: 0.5787 - loss: 1.1931 - val_accuracy: 0.6160 - val_loss: 1.0930\n",
            "Epoch 3/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 9ms/step - accuracy: 0.6381 - loss: 1.0395 - val_accuracy: 0.6541 - val_loss: 0.9990\n",
            "Epoch 4/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - accuracy: 0.6709 - loss: 0.9398 - val_accuracy: 0.6632 - val_loss: 0.9787\n",
            "Epoch 5/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.6999 - loss: 0.8782 - val_accuracy: 0.6821 - val_loss: 0.9270\n",
            "Epoch 6/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - accuracy: 0.7128 - loss: 0.8257 - val_accuracy: 0.6835 - val_loss: 0.9467\n",
            "Epoch 7/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - accuracy: 0.7311 - loss: 0.7750 - val_accuracy: 0.6794 - val_loss: 0.9386\n",
            "Epoch 8/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - accuracy: 0.7485 - loss: 0.7252 - val_accuracy: 0.6930 - val_loss: 0.8960\n",
            "Epoch 9/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - accuracy: 0.7553 - loss: 0.6976 - val_accuracy: 0.6981 - val_loss: 0.9018\n",
            "Epoch 10/10\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.7692 - loss: 0.6671 - val_accuracy: 0.7016 - val_loss: 0.8970\n",
            "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7104 - loss: 0.8721\n",
            "Test accuracy: 0.7016000151634216\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "tPfWe4sHg0g1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Distributed Training with TensorFlow's MultiWorkerMirroredStrategy\n",
        "In this exercise, you will explore how distributed training works using TensorFlow's MultiWorkerMirroredStrategy. You will learn how to set up a multi-node training environment, modify the model's architecture, and scale the learning rate appropriately. This exercise will demonstrate how to leverage distributed computing to handle more complex models, reducing training time by spreading the workload across multiple workers.\n",
        "\n",
        "## 1. Setting Up the TF_CONFIG Environment Variable\n",
        "The TF_CONFIG environment variable is essential when configuring TensorFlow to work in a multi-worker setup. This variable defines the cluster structure, specifying each worker’s role and how they communicate with each other.\n",
        "\n",
        "The cluster section includes the worker nodes, which are responsible for training the model.\n",
        "The task section defines the role of each machine, including the worker's type and index (i.e., which worker is being set up on this node).\n",
        "For instance:\n",
        "\n",
        "```\n",
        "os.environ['TF_CONFIG'] = json.dumps({\n",
        "    'cluster': {\n",
        "        'worker': [\"worker1_ip:port\", \"worker2_ip:port\", \"worker3_ip:port\"]\n",
        "    },\n",
        "    'task': {'type': 'worker', 'index': 0}  # 'index': 0 for first worker, 1 for the second, etc.\n",
        "})\n",
        "\n",
        "```\n",
        "This configuration simulates a cluster with 3 worker nodes. Each machine will have a different index value based on its role.\n",
        "\n",
        "## 2. Initializing MultiWorkerMirroredStrategy\n",
        "Once the cluster environment is set up using TF_CONFIG, the next step is to initialize the distributed strategy. The MultiWorkerMirroredStrategy enables synchronous training across multiple workers, ensuring that the model replicas are synchronized, and gradient updates happen simultaneously across all workers.\n",
        "\n",
        "```\n",
        "strategy = tf.distribute.MultiWorkerMirroredStrategy()\n",
        "```\n",
        "\n",
        "##3. Building the Model with strategy.scope()\n",
        "The model architecture and its computations must be placed inside the strategy.scope() block. This ensures that TensorFlow distributes the model and training process across all available nodes and GPUs.\n",
        "\n",
        "Here’s an example using the MNIST dataset:\n",
        "\n",
        "```\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "with strategy.scope():\n",
        "    model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dense(10, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "```\n",
        "\n",
        "## 4. Training and Evaluating the Model\n",
        "Once the model is defined, you can train it across multiple workers. The MultiWorkerMirroredStrategy distributes the dataset and ensures that each worker receives a different batch of data for training.\n",
        "\n",
        "```\n",
        "history = model.fit(x_train, y_train, epochs=5, batch_size=64)\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f'Test accuracy: {test_acc}')\n",
        "```\n",
        "\n",
        "## 5. Scaling Learning Rate\n",
        "When using distributed training with multiple workers, it's a good practice to scale the learning rate by multiplying the base rate by the number of workers. This ensures that the gradient updates are balanced and the training process remains efficient across all nodes.\n",
        "\n",
        "For example, if you have 3 workers:\n",
        "\n",
        "```\n",
        "scaled_learning_rate = base_learning_rate * num_workers\n",
        "```\n",
        "\n",
        "## 6. Experimenting with Different Model Architectures\n",
        "You can modify the architecture of the model and observe how the distributed strategy handles these changes. For instance, try adding more layers or changing the activation functions:\n",
        "\n",
        "```\n",
        "with strategy.scope():\n",
        "    model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "        tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(128, activation='relu'),\n",
        "        tf.keras.layers.Dense(10, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=scaled_learning_rate),\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "```\n",
        "\n",
        "## 7. Expected Output\n",
        "After running this exercise on a distributed environment, you should observe the following:\n",
        "\n",
        "Training time should decrease as more workers are added to the cluster, depending on the complexity of the model and dataset.\n",
        "The model's accuracy should remain consistent, provided the batch sizes and learning rate are adjusted correctly.\n",
        "The training process should leverage the available computational resources (e.g., GPUs) efficiently.\n",
        "\n",
        "##Exercise Tasks:\n",
        "Modify the TF_CONFIG: Add at least 3 workers to the cluster configuration.\n",
        "Experiment with Model Architectures: Change the architecture (e.g., add more layers or modify activation functions).\n",
        "Adjust the Learning Rate: Scale the learning rate by the number of workers and observe the impact on the model’s convergence speed and accuracy.\n",
        "\n",
        "\n",
        "Key Takeaways:\n",
        "\n",
        "- Distributed Training allows you to reduce training time by distributing the workload across multiple workers.\n",
        "\n",
        "- MultiWorkerMirroredStrategy synchronizes the training process across all workers, ensuring consistency in gradient updates.\n",
        "\n",
        "- Scaling Learning Rate is essential for maintaining effective learning when distributing training across multiple workers.\n",
        "\n",
        "- Experimenting with Architectures can help understand how more complex models behave in distributed environments and how strategies can efficiently handle them."
      ],
      "metadata": {
        "id": "v0nJVt30hPQk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import tensorflow as tf\n",
        "\n",
        "# Step 1: Define the TF_CONFIG environment variable\n",
        "# Replace the worker addresses with the actual IPs of your nodes\n",
        "os.environ['TF_CONFIG'] = json.dumps({\n",
        "    'cluster': {\n",
        "        'worker': [\"worker1_ip:port\", \"worker2_ip:port\"]\n",
        "    },\n",
        "    'task': {'type': 'worker', 'index': 0}  # Set 'index': 0 for the first worker, 1 for the second, etc.\n",
        "})\n",
        "\n",
        "# Restart runtime manually to apply the settings and re-run the next cells\n"
      ],
      "metadata": {
        "id": "oE5W855TmgH9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Step 2: Initialize the strategy after the runtime restart\n",
        "strategy = tf.distribute.MultiWorkerMirroredStrategy()\n",
        "\n",
        "# Load and preprocess the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "# Use strategy.scope to ensure computations are distributed across nodes and GPUs\n",
        "with strategy.scope():\n",
        "    # Define a simple neural network\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "        tf.keras.layers.Dense(128, activation='relu'),\n",
        "        tf.keras.layers.Dense(10, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "# Train the model on the dataset\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "\n",
        "# Evaluate the model\n",
        "model.evaluate(x_test, y_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-8HIcGfimzU",
        "outputId": "00e2b1b8-2707-41ac-8f05-625455f63ac2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.8752 - loss: 0.4347\n",
            "Epoch 2/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9633 - loss: 0.1224\n",
            "Epoch 3/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9777 - loss: 0.0771\n",
            "Epoch 4/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9817 - loss: 0.0590\n",
            "Epoch 5/5\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9872 - loss: 0.0421\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9717 - loss: 0.0898\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.07822805643081665, 0.9753000140190125]"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparing Single GPU, Multiple GPUs, and Multiple Nodes in TensorFlow\n",
        "\n",
        "This comparison highlights the key differences between training a model using **single GPU**, **multiple GPUs**, and **multiple nodes** with TensorFlow's `tf.distribute.Strategy`.\n",
        "\n",
        "## 1. Single GPU Setup\n",
        "- **Resource Usage**: Runs on a single GPU.\n",
        "- **Strategy**: No special strategy needed.\n",
        "- **Scope**: No `strategy.scope()` required.\n",
        "- **Synchronization**: Not applicable, only one device is used.\n",
        "\n",
        "## 2. Multiple GPUs Setup (MirroredStrategy)\n",
        "- **Resource Usage**: Uses multiple GPUs on the same machine.\n",
        "- **Strategy**: `MirroredStrategy` synchronizes gradient updates across all GPUs.\n",
        "- **Scope**: `strategy.scope()` is required to ensure computations are distributed across GPUs.\n",
        "- **Synchronization**: Gradients are synchronized across GPUs using an all-reduce algorithm.\n",
        "\n",
        "## 3. Multiple Nodes Setup (MultiWorkerMirroredStrategy)\n",
        "- **Resource Usage**: Uses multiple machines, each with one or more GPUs.\n",
        "- **Strategy**: `MultiWorkerMirroredStrategy` distributes the model and synchronizes gradient updates across multiple nodes.\n",
        "- **Scope**: `strategy.scope()` is required for distributing computation across nodes and GPUs.\n",
        "- **Synchronization**: Gradients are synchronized across nodes using collective communication, requiring configuration of the `TF_CONFIG` environment variable.\n",
        "\n",
        "## Summary of Differences:\n",
        "\n",
        "| **Feature**             | **Single GPU**                             | **Multiple GPUs (MirroredStrategy)**                  | **Multiple Nodes (MultiWorkerMirroredStrategy)**      |\n",
        "|-------------------------|--------------------------------------------|------------------------------------------------------|------------------------------------------------------|\n",
        "| **Execution**            | Single GPU                                 | Multiple GPUs on the same machine                    | Multiple nodes (multiple machines with GPUs)          |\n",
        "| **Strategy**             | None                                       | `MirroredStrategy`                                   | `MultiWorkerMirroredStrategy`                        |\n",
        "| **Synchronization**      | Not needed (single device)                 | Sync gradients across GPUs using all-reduce           | Sync gradients across workers using collective comm   |\n",
        "| **Scope**                | No `strategy.scope()`                      | Requires `strategy.scope()` for GPU distribution      | Requires `strategy.scope()` for multi-node distribution|\n",
        "| **Speed/Performance**    | Limited by single GPU                      | Faster, workload distributed across GPUs              | Faster, workload distributed across nodes and GPUs    |\n"
      ],
      "metadata": {
        "id": "U4fnTXlPn2An"
      }
    }
  ]
}