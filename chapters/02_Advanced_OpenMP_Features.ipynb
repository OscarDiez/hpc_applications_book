{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Introduction to OpenMP Task Creation with #pragma omp task\n",
        "\n",
        "In OpenMP, the #pragma omp task directive is a powerful feature that allows developers to express parallelism at a higher level. It enables the creation of tasks that can be executed concurrently. This is particularly useful when dealing with recursive functions, such as the computation of the Fibonacci sequence, where each task can independently compute part of the result. The use of tasks helps to exploit parallelism in irregular or dynamically structured problems.\n",
        "\n",
        "When using #pragma omp task, the program defines a task that may be executed by any available thread in the OpenMP team. Unlike simple parallel loops, tasks are more flexible and can be used for work that isn't easily expressed in loops.\n",
        "\n",
        "##Fibonacci Example Overview\n",
        "In the example provided, we calculate the Fibonacci number recursively. Without OpenMP, the recursive function would be computed sequentially. By introducing tasks using #pragma omp task, each recursive call can be parallelized, and multiple threads can execute different parts of the Fibonacci computation in parallel.\n",
        "\n",
        "We'll demonstrate three versions of the Fibonacci program:\n",
        "\n",
        "- Serial version – No parallelism.\n",
        "- Parallel version without tasks – Uses parallel regions and single directive.\n",
        "- Parallel version with tasks – Uses tasks to exploit fine-grained parallelism.\n"
      ],
      "metadata": {
        "id": "3LqZWe8AacJ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the output for the non-parallel version, the parallel version without tasks, and with tasks.\n",
        "\n",
        "import os\n",
        "\n",
        "# Define the C code for Fibonacci calculation with OpenMP tasks\n",
        "c_code = r'''\n",
        "#include <stdio.h>\n",
        "#include <omp.h>\n",
        "\n",
        "int fib_serial(int n) {\n",
        "    if (n < 2) {\n",
        "        return n;\n",
        "    } else {\n",
        "        return fib_serial(n - 1) + fib_serial(n - 2);\n",
        "    }\n",
        "}\n",
        "\n",
        "int fib_parallel_no_task(int n) {\n",
        "    if (n < 2) {\n",
        "        return n;\n",
        "    } else {\n",
        "        int x, y;\n",
        "        #pragma omp parallel sections\n",
        "        {\n",
        "            #pragma omp section\n",
        "            x = fib_parallel_no_task(n - 1);\n",
        "            #pragma omp section\n",
        "            y = fib_parallel_no_task(n - 2);\n",
        "        }\n",
        "        return x + y;\n",
        "    }\n",
        "}\n",
        "\n",
        "int fib_parallel_with_task(int n) {\n",
        "    int x, y;\n",
        "\n",
        "    if (n < 2) {\n",
        "        return n;\n",
        "    } else {\n",
        "        #pragma omp task shared(x)\n",
        "        x = fib_parallel_with_task(n - 1);\n",
        "\n",
        "        #pragma omp task shared(y)\n",
        "        y = fib_parallel_with_task(n - 2);\n",
        "\n",
        "        #pragma omp taskwait\n",
        "        return x + y;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int n = 30;\n",
        "    int result;\n",
        "    double start_time, end_time;\n",
        "\n",
        "    // Serial Fibonacci\n",
        "    start_time = omp_get_wtime();\n",
        "    result = fib_serial(n);\n",
        "    end_time = omp_get_wtime();\n",
        "    printf(\"Serial Fibonacci(%d) = %d, Time: %f seconds\\n\", n, result, end_time - start_time);\n",
        "\n",
        "    // Parallel Fibonacci without tasking\n",
        "    start_time = omp_get_wtime();\n",
        "    #pragma omp parallel\n",
        "    {\n",
        "        #pragma omp single\n",
        "        result = fib_parallel_no_task(n);\n",
        "    }\n",
        "    end_time = omp_get_wtime();\n",
        "    printf(\"Parallel Fibonacci without tasking(%d) = %d, Time: %f seconds\\n\", n, result, end_time - start_time);\n",
        "\n",
        "    // Parallel Fibonacci with tasking\n",
        "    start_time = omp_get_wtime();\n",
        "    #pragma omp parallel\n",
        "    {\n",
        "        #pragma omp single\n",
        "        result = fib_parallel_with_task(n);\n",
        "    }\n",
        "    end_time = omp_get_wtime();\n",
        "    printf(\"Parallel Fibonacci with tasking(%d) = %d, Time: %f seconds\\n\", n, result, end_time - start_time);\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "'''\n",
        "\n",
        "# Save the code to a file\n",
        "with open(\"fib_task_example.c\", \"w\") as code_file:\n",
        "    code_file.write(c_code)\n",
        "\n",
        "# Compile the C code with OpenMP support\n",
        "!gcc -fopenmp fib_task_example.c -o fib_task_example\n",
        "\n",
        "# Run the compiled program\n",
        "!./fib_task_example\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQodR7nRfChJ",
        "outputId": "071069da-2af8-444e-8386-f854adc3bc0d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Serial Fibonacci(30) = 832040, Time: 0.009567 seconds\n",
            "Parallel Fibonacci without tasking(30) = 832040, Time: 1.067699 seconds\n",
            "Parallel Fibonacci with tasking(30) = 832040, Time: 0.720553 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OpenMP Task Creation Example\n",
        "\n",
        "This notebook demonstrates the use of OpenMP tasks for parallelizing a recursive Fibonacci calculation.\n",
        "\n",
        "### Explanation of Results\n",
        "\n",
        "The results of the Fibonacci calculation show different approaches and their performance:\n",
        "- **Serial Fibonacci**: Time = `0.008623 seconds`\n",
        "- **Parallel Fibonacci Without Tasking**: Time = `0.959481 seconds`\n",
        "- **Parallel Fibonacci With Tasking**: Time = `0.614899 seconds`\n",
        "\n",
        "#### 1. **Serial Version**\n",
        "In the serial version, the Fibonacci calculation is performed without any parallelism. This version is the fastest because there is no overhead from managing threads or tasks. However, it does not utilize the available multi-core processing power, which limits its scalability for larger problems.\n",
        "\n",
        "#### 2. **Parallel Version Without Tasking**\n",
        "This version uses OpenMP sections to parallelize the recursive Fibonacci calls. Despite using multiple threads, the time is significantly slower. The overhead of creating and managing threads for each recursive call far outweighs the benefits of parallelism, especially for an algorithm that is deeply recursive like Fibonacci.\n",
        "\n",
        "#### 3. **Parallel Version With Tasking**\n",
        "Using OpenMP tasks, the performance improves compared to the non-tasking version. Tasks allow recursive calls to execute in parallel across multiple threads. This version better exploits the parallelism in the recursion, leading to faster execution compared to the section-based approach, though it is still slower than the serial version due to task management overhead.\n",
        "\n",
        "#### Key Takeaway\n",
        "While task-based parallelism adds some overhead, it offers a more scalable solution for recursive problems. It allows for better utilization of multi-core processors and will likely show more significant performance improvements as the problem size increases.\n"
      ],
      "metadata": {
        "id": "QbBuFErPfw1g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction to OpenMP Task Dependencies\n",
        "OpenMP tasks are an effective way to express parallelism, especially in algorithms with recursive or irregular workloads, like the Fibonacci example. However, when tasks are created, they may depend on one another for their results. OpenMP provides a mechanism to manage these dependencies using the depend clause with the #pragma omp task directive.\n",
        "\n",
        "In task-based parallelism, task dependencies ensure that certain tasks are completed before others begin. This is critical when tasks share data or when the result of one task is required by another. By specifying dependencies, OpenMP ensures correct execution order and minimizes synchronization overhead by waiting only when necessary.\n",
        "\n",
        "### Task Dependencies Syntax\n",
        "The depend clause allows the programmer to specify in what way tasks are dependent on each other:\n",
        "\n",
        "- in: A task will depend on another task to provide the input before starting.\n",
        "- out: A task will produce data that other tasks might consume.\n",
        "-inout: A task will both read and modify shared data.\n",
        "\n",
        "In the Fibonacci example, where two tasks compute fib(n-1) and fib(n-2), the use of task dependencies can ensure that the summing of these results occurs after both tasks have completed.\n",
        "\n"
      ],
      "metadata": {
        "id": "_Ehi_0_MgMzg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# demonstrating task dependencies in OpenMP.\n",
        "\n",
        "import os\n",
        "\n",
        "# Define the C code with OpenMP task dependencies for Fibonacci calculation\n",
        "c_code_with_dependencies = r'''\n",
        "#include <stdio.h>\n",
        "#include <omp.h>\n",
        "\n",
        "int fib_with_dependencies(int n) {\n",
        "    int x, y;\n",
        "\n",
        "    if (n < 2) {\n",
        "        return n;\n",
        "    } else {\n",
        "        #pragma omp task shared(x) depend(out: x)\n",
        "        x = fib_with_dependencies(n - 1);\n",
        "\n",
        "        #pragma omp task shared(y) depend(out: y)\n",
        "        y = fib_with_dependencies(n - 2);\n",
        "\n",
        "        #pragma omp taskwait\n",
        "        return x + y;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int result;\n",
        "    int n = 30;\n",
        "    double start_time, end_time;\n",
        "\n",
        "    start_time = omp_get_wtime();\n",
        "    #pragma omp parallel\n",
        "    {\n",
        "        #pragma omp single\n",
        "        result = fib_with_dependencies(n);\n",
        "    }\n",
        "    end_time = omp_get_wtime();\n",
        "\n",
        "    printf(\"Fibonacci with task dependencies(%d) = %d, Time: %f seconds\\n\", n, result, end_time - start_time);\n",
        "    return 0;\n",
        "}\n",
        "'''\n",
        "\n",
        "# Save the C code to a file\n",
        "with open(\"fib_task_dependencies_example.c\", \"w\") as code_file:\n",
        "    code_file.write(c_code_with_dependencies)\n",
        "\n",
        "# Compile the C code with OpenMP support\n",
        "!gcc -fopenmp fib_task_dependencies_example.c -o fib_task_dependencies_example\n",
        "\n",
        "# Run the compiled program\n",
        "!./fib_task_dependencies_example\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SrEERwc_fwaA",
        "outputId": "452b18e1-c606-4841-bfea-0e7ed0f0c83b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fibonacci with task dependencies(30) = 832040, Time: 1.044062 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explanation of Code with Task Dependencies\n",
        "In this version, we introduce task dependencies using the depend clause. The key difference between this and the previous task-based version is that the dependencies are explicitly declared, ensuring that the tasks complete in a defined order.\n",
        "\n",
        "#### How Task Dependencies Work:\n",
        "1. Creation of Tasks:\n",
        " - The first task computes fib(n-1) and is assigned an output dependency (depend(out: x)), meaning that any task depending on the value of x will wait for this task to complete.\n",
        " - The second task computes fib(n-2) and similarly uses an output dependency (depend(out: y)).\n",
        "2. Taskwait:\n",
        " - The #pragma omp taskwait ensures that the program waits for both tasks (fib(n-1) and fib(n-2)) to finish before summing their results.\n",
        "\n",
        "#### Advantages of Task Dependencies:\n",
        "- Fine-Grained Control: By explicitly defining the dependencies between tasks, OpenMP can better manage task execution, ensuring correctness while still allowing parallel execution where possible.\n",
        "- Avoiding Unnecessary Synchronization: Instead of waiting for all tasks at once (as with taskwait), the program waits only for tasks that are required for the next step, improving performance."
      ],
      "metadata": {
        "id": "ggJqgRPagyTQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Is normal that takes longer using dependencies?\n",
        "\n",
        "Yes, it is normal for the task-based Fibonacci implementation with dependencies to take longer in some cases, and this can be attributed to several factors:\n",
        "\n",
        "1. Overhead of Task Management\n",
        "Creating tasks, managing dependencies, and scheduling them involves some overhead in OpenMP. Each time a task is created, OpenMP must track the dependencies between tasks to ensure that they execute in the correct order. This tracking adds some computational overhead compared to simply running everything sequentially.\n",
        "For a recursive problem like Fibonacci, where there are many small, fine-grained tasks being created, the overhead of task creation and synchronization can dominate the actual computation, especially for smaller problem sizes like Fibonacci(30).\n",
        "2. Task Granularity\n",
        "The Fibonacci function inherently has a small computational workload for each recursive call (just a few additions). However, the overhead for creating a task and managing dependencies is relatively large compared to the actual work done inside each task.\n",
        "Tasks work best when the granularity (the amount of work done per task) is sufficiently large, as the overhead can be amortized over larger tasks. For Fibonacci, each task computes only a small part of the problem, making the overhead more significant.\n",
        "3. Recursive Nature of Fibonacci\n",
        "The Fibonacci function is highly recursive, and for Fibonacci(30), this results in a large number of tasks being created. As the recursion goes deeper, the number of tasks grows exponentially, adding further overhead.\n",
        "While task parallelism can speed up certain types of workloads, recursive functions with many small tasks, like Fibonacci, may not benefit as much unless the recursion depth is very large, where more parallelism can be exploited across multiple cores.\n",
        "4. Task Dependencies\n",
        "Although dependencies ensure correct ordering, they also limit how much parallelism can be exploited. The taskwait directive ensures that tasks complete in the correct order, but it can also introduce additional synchronization points that may cause threads to wait, reducing the efficiency of parallel execution."
      ],
      "metadata": {
        "id": "GePXQlNzhc14"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to the taskloop Directive\n",
        "The taskloop directive in OpenMP simplifies the creation of tasks for loop iterations. Instead of manually creating a task for each iteration or group of iterations, the taskloop directive automatically generates tasks for different chunks of iterations. This is particularly useful when dealing with large loop-based computations where workload distribution across multiple threads can improve performance.\n",
        "\n",
        "By specifying the grainsize or num_tasks clauses, developers can control the number of iterations that each task handles, effectively balancing task granularity and load distribution across threads. This makes taskloop an excellent tool for managing parallelism in iterative workloads.\n",
        "\n",
        "###Benefits of taskloop:\n",
        "Automated Task Generation: Automatically divides loop iterations into tasks without requiring explicit task creation for each iteration.\n",
        "Load Balancing: Allows better load distribution by controlling task size using the grainsize clause, ensuring efficient use of resources.\n",
        "Simplifies Code: Reduces the complexity of parallelizing loop-based computations by abstracting task creation.\n"
      ],
      "metadata": {
        "id": "pQyDMuVWhkJL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Define the C code with OpenMP taskloop directive\n",
        "c_code_taskloop = r'''\n",
        "#include <stdio.h>\n",
        "#include <omp.h>\n",
        "\n",
        "void process_element(int i, int *data) {\n",
        "    data[i] = i * 2;  // Example processing: multiplying by 2\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int data[1000];\n",
        "\n",
        "    #pragma omp parallel\n",
        "    {\n",
        "        #pragma omp single\n",
        "        {\n",
        "            #pragma omp taskloop grainsize(10)\n",
        "            for (int i = 0; i < 1000; i++) {\n",
        "                process_element(i, data);\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // Output a sample of the processed data\n",
        "    for (int i = 0; i < 100; i += 10) {\n",
        "        printf(\"data[%d] = %d\\n\", i, data[i]);\n",
        "    }\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "'''\n",
        "\n",
        "# Save the C code to a file\n",
        "with open(\"taskloop_example.c\", \"w\") as code_file:\n",
        "    code_file.write(c_code_taskloop)\n",
        "\n",
        "# Compile the C code with OpenMP support\n",
        "!gcc -fopenmp taskloop_example.c -o taskloop_example\n",
        "\n",
        "# Run the compiled program\n",
        "!./taskloop_example\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvsbYcOciGoY",
        "outputId": "694fc964-1bba-4b24-d09d-fe6543cd684c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data[0] = 0\n",
            "data[10] = 20\n",
            "data[20] = 40\n",
            "data[30] = 60\n",
            "data[40] = 80\n",
            "data[50] = 100\n",
            "data[60] = 120\n",
            "data[70] = 140\n",
            "data[80] = 160\n",
            "data[90] = 180\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explanation of the Taskloop Code\n",
        "In this example, we utilize the #pragma omp taskloop directive to parallelize the loop that processes elements in an array.\n",
        "\n",
        "- Taskloop Directive: The taskloop directive is applied to the loop, which generates tasks for processing array elements. Here, we specify grainsize(10), meaning that each task will process 10 iterations of the loop.\n",
        "\n",
        "- Grainsize: The grainsize clause controls the size of the chunks of iterations that each task processes. By using a grainsize of 10, we ensure that each task processes a small but reasonable amount of work, allowing better load balancing across threads.\n",
        "\n",
        "- Parallel Execution: The loop iterations are executed in parallel by multiple threads, each handling a chunk of 10 iterations. The #pragma omp single ensures that the taskloop is executed by a single thread, but the tasks generated are distributed across available threads.\n",
        "\n",
        "- Output: After the loop has processed the elements, the program prints a sample of the processed data to verify that each element in the array has been correctly updated.\n",
        "\n",
        "####Key Advantages:\n",
        "- Simplicity: The taskloop directive eliminates the need to manually create and manage tasks for each iteration, simplifying the code.\n",
        "- Scalability: By splitting the loop into tasks, the workload can be distributed across multiple threads, making the code scalable to more cores.\n",
        "- Granularity Control: With the grainsize clause, we can control the task size, allowing us to find the right balance between parallelism and overhead. Smaller grainsizes increase the number of tasks, leading to more parallelism but higher task creation overhead."
      ],
      "metadata": {
        "id": "S56X8wGBiJlx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to the taskloop Directive\n",
        "The taskloop directive in OpenMP provides an efficient way to parallelize loop-based computations by automatically generating tasks for chunks of loop iterations. Rather than creating tasks manually for each iteration or group of iterations, the taskloop directive automates this process, making it easier to exploit parallelism in loop constructs. This is particularly useful for load balancing and improving task granularity.\n",
        "\n",
        "## Key features of the taskloop directive include:\n",
        "\n",
        "- Automatic Task Generation: Automatically breaks up a loop into tasks based on the specified chunk size or number of tasks.\n",
        "Control Over Granularity: Using clauses like grainsize or num_tasks, developers can control the number of iterations handled by each task, balancing load distribution and minimizing overhead.\n",
        "- Efficient Parallelism: Tasks are generated for loop iterations and distributed among available threads, improving performance for large iterative workloads."
      ],
      "metadata": {
        "id": "uFoSO7klia2K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Define the C code for demonstrating the OpenMP taskloop directive\n",
        "c_code_taskloop = r'''\n",
        "#include <stdio.h>\n",
        "#include <omp.h>\n",
        "\n",
        "void process_element(int i, int *data) {\n",
        "    data[i] = i * i;  // Example processing: square the element\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int data[1000];\n",
        "\n",
        "    // Parallel block with a taskloop to process elements in chunks\n",
        "    #pragma omp parallel\n",
        "    {\n",
        "        #pragma omp single\n",
        "        {\n",
        "            #pragma omp taskloop grainsize(20)\n",
        "            for (int i = 0; i < 1000; i++) {\n",
        "                process_element(i, data);\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // Output some processed data\n",
        "    for (int i = 0; i < 100; i += 10) {\n",
        "        printf(\"data[%d] = %d\\n\", i, data[i]);\n",
        "    }\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "'''\n",
        "\n",
        "# Save the C code to a file\n",
        "with open(\"taskloop_directive_example.c\", \"w\") as code_file:\n",
        "    code_file.write(c_code_taskloop)\n",
        "\n",
        "# Compile the C code with OpenMP support\n",
        "!gcc -fopenmp taskloop_directive_example.c -o taskloop_directive_example\n",
        "\n",
        "# Run the compiled program\n",
        "!./taskloop_directive_example\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "slOvfgWqiaZh",
        "outputId": "632a2515-a145-4598-81cd-3c18a6a6bf4e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data[0] = 0\n",
            "data[10] = 100\n",
            "data[20] = 400\n",
            "data[30] = 900\n",
            "data[40] = 1600\n",
            "data[50] = 2500\n",
            "data[60] = 3600\n",
            "data[70] = 4900\n",
            "data[80] = 6400\n",
            "data[90] = 8100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explanation of Taskloop with grainsize Example\n",
        "This code demonstrates the usage of the OpenMP taskloop directive with the grainsize clause to control task granularity. Here's a breakdown of the key components:\n",
        "\n",
        "- Taskloop Directive: The #pragma omp taskloop grainsize(20) directive tells OpenMP to divide the loop iterations into tasks, with each task processing 20 iterations. This automatically parallelizes the loop, distributing the tasks across available threads.\n",
        "\n",
        "- Parallel Region: The #pragma omp parallel block ensures that multiple threads are available to execute the tasks generated by the taskloop directive. The #pragma omp single ensures that only one thread initiates the taskloop, but the generated tasks are executed by multiple threads.\n",
        "\n",
        "- Grainsize: The grainsize(20) clause divides the loop into tasks where each task handles 20 iterations. This balances the workload across threads by ensuring each task is sufficiently large to reduce overhead while still allowing for parallel execution.\n",
        "\n",
        "- Processing Logic: The function process_element() is applied to each element in the array. In this case, we square the array elements as a placeholder for more complex processing logic.\n",
        "\n",
        "###Output\n",
        "The program prints a subset of the processed data to verify the correct execution of the taskloop directive. Each element of the array is squared by the process_element() function and printed to the console.\n",
        "\n",
        "####Key Points\n",
        "- Simplified Parallelism: The taskloop directive eliminates the need to manually define tasks for each loop iteration, simplifying the parallelization of iterative workloads.\n",
        "- Controlled Granularity: By specifying the grainsize(20), we control how many iterations are handled by each task, ensuring a good balance between task overhead and parallelism.\n",
        "- Efficient Execution: The loop iterations are distributed across multiple threads, leveraging task-based parallelism to speed up processing."
      ],
      "metadata": {
        "id": "MrFez78Wi41K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## Introduction to SIMD Clauses for Optimization\n",
        "Single Instruction, Multiple Data (SIMD) is an essential technique in modern high-performance computing for optimizing loop-based computations. OpenMP provides several SIMD-related clauses to help developers better control how loops are vectorized. Vectorization allows a processor to perform the same operation on multiple data points simultaneously, which can significantly enhance performance, especially for large data sets.\n",
        "\n",
        "The following OpenMP SIMD clauses are commonly used to optimize loop-based computations:\n",
        "\n",
        "- safelen: Ensures that the vectorized loop is safe for up to N iterations.\n",
        "aligned: Ensures that data is memory-aligned, which improves access speed and allows the compiler to generate efficient SIMD instructions.\n",
        "- collapse: Combines multiple loops, which enables better vectorization by collapsing nested loops into a single iteration space.\n",
        "\n",
        "####Example of SIMD Clauses:\n",
        "- Memory Alignment (aligned): Aligning data ensures that it can be efficiently - loaded into SIMD registers, reducing the overhead of unaligned memory access.\n",
        "- Collapsing Loops (collapse): Collapsing nested loops into a single iteration space allows for better vectorization, particularly useful for multi-dimensional arrays or matrices.\n"
      ],
      "metadata": {
        "id": "In4Z4vq9jMBS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "\n",
        "# Define the C code that compares the three SIMD options\n",
        "c_code_comparison = r'''\n",
        "#include <stdio.h>\n",
        "#include <omp.h>\n",
        "\n",
        "#define N 1000000  // Increase the size for more noticeable differences\n",
        "\n",
        "void no_simd(double *array, int size) {\n",
        "    // Simple loop without SIMD\n",
        "    for (int i = 0; i < size; i++) {\n",
        "        array[i] = array[i] * 2;\n",
        "    }\n",
        "}\n",
        "\n",
        "void simd_basic(double *array, int size) {\n",
        "    // Basic SIMD without any clauses\n",
        "    #pragma omp simd\n",
        "    for (int i = 0; i < size; i++) {\n",
        "        array[i] = array[i] * 2;\n",
        "    }\n",
        "}\n",
        "\n",
        "void simd_optimized(double *array, int size) {\n",
        "    // Optimized SIMD with memory alignment\n",
        "    #pragma omp simd aligned(array: 32)\n",
        "    for (int i = 0; i < size; i++) {\n",
        "        array[i] = array[i] * 2;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    double array1[N], array2[N], array3[N];\n",
        "\n",
        "    // Initialize the arrays\n",
        "    for (int i = 0; i < N; i++) {\n",
        "        array1[i] = i * 1.0;\n",
        "        array2[i] = i * 1.0;\n",
        "        array3[i] = i * 1.0;\n",
        "    }\n",
        "\n",
        "    // Run the computation multiple times to average the timings\n",
        "    int repeats = 10;\n",
        "    double total_time_no_simd = 0.0;\n",
        "    double total_time_simd_basic = 0.0;\n",
        "    double total_time_simd_optimized = 0.0;\n",
        "\n",
        "    for (int r = 0; r < repeats; r++) {\n",
        "        // Measure time for no SIMD\n",
        "        double start_time = omp_get_wtime();\n",
        "        no_simd(array1, N);\n",
        "        double end_time = omp_get_wtime();\n",
        "        total_time_no_simd += (end_time - start_time);\n",
        "\n",
        "        // Measure time for basic SIMD\n",
        "        start_time = omp_get_wtime();\n",
        "        simd_basic(array2, N);\n",
        "        end_time = omp_get_wtime();\n",
        "        total_time_simd_basic += (end_time - start_time);\n",
        "\n",
        "        // Measure time for optimized SIMD with aligned clause\n",
        "        start_time = omp_get_wtime();\n",
        "        simd_optimized(array3, N);\n",
        "        end_time = omp_get_wtime();\n",
        "        total_time_simd_optimized += (end_time - start_time);\n",
        "    }\n",
        "\n",
        "    // Print a sample of the results for verification\n",
        "    printf(\"Array output sample (without SIMD):\\n\");\n",
        "    for (int i = 0; i < 10; i++) {\n",
        "        printf(\"array1[%d] = %f\\n\", i, array1[i]);\n",
        "        fflush(stdout);  // Ensure output is printed\n",
        "    }\n",
        "\n",
        "    printf(\"\\nArray output sample (with basic SIMD):\\n\");\n",
        "    for (int i = 0; i < 10; i++) {\n",
        "        printf(\"array2[%d] = %f\\n\", i, array2[i]);\n",
        "        fflush(stdout);  // Ensure output is printed\n",
        "    }\n",
        "\n",
        "    printf(\"\\nArray output sample (with SIMD + aligned):\\n\");\n",
        "    for (int i = 0; i < 10; i++) {\n",
        "        printf(\"array3[%d] = %f\\n\", i, array3[i]);\n",
        "        fflush(stdout);  // Ensure output is printed\n",
        "    }\n",
        "\n",
        "    // Print the time comparisons\n",
        "    printf(\"\\nAverage time for no SIMD: %f seconds\\n\", total_time_no_simd / repeats);\n",
        "    fflush(stdout);\n",
        "    printf(\"Average time for basic SIMD: %f seconds\\n\", total_time_simd_basic / repeats);\n",
        "    fflush(stdout);\n",
        "    printf(\"Average time for SIMD with aligned clause: %f seconds\\n\", total_time_simd_optimized / repeats);\n",
        "    fflush(stdout);\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "'''\n",
        "\n",
        "# Save the C code to a file\n",
        "with open(\"simd_comparison_example.c\", \"w\") as code_file:\n",
        "    code_file.write(c_code_comparison)\n",
        "\n",
        "# Compile the C code with OpenMP support\n",
        "compilation_status = os.system(\"gcc -fopenmp simd_comparison_example.c -o simd_comparison_example\")\n",
        "\n",
        "# Check if compilation was successful\n",
        "if compilation_status == 0:\n",
        "    print(\"Compilation successful. Running the program...\")\n",
        "    # Run the compiled program\n",
        "    os.system(\"./simd_comparison_example\")\n",
        "else:\n",
        "    print(\"Compilation failed. Please check the code for errors.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xM6JKG9lkXf8",
        "outputId": "3b995cdb-d2e7-401c-bd67-0a2b1650e231"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Compilation successful. Running the program...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import subprocess\n",
        "\n",
        "# Define the C code that compares the three SIMD options with a larger array size\n",
        "c_code_comparison = r'''\n",
        "#include <stdio.h>\n",
        "#include <omp.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "#define N 300000000  // Larger size to better illustrate SIMD benefits\n",
        "\n",
        "void no_simd(double *array, int size) {\n",
        "    // Simple loop without SIMD\n",
        "    for (int i = 0; i < size; i++) {\n",
        "        array[i] = array[i] * 2;\n",
        "    }\n",
        "}\n",
        "\n",
        "void simd_basic(double *array, int size) {\n",
        "    // Basic SIMD without any clauses\n",
        "    #pragma omp simd\n",
        "    for (int i = 0; i < size; i++) {\n",
        "        array[i] = array[i] * 2;\n",
        "    }\n",
        "}\n",
        "\n",
        "void simd_optimized(double *array, int size) {\n",
        "    // Optimized SIMD with memory alignment\n",
        "    #pragma omp simd aligned(array: 32)\n",
        "    for (int i = 0; i < size; i++) {\n",
        "        array[i] = array[i] * 2;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    double *array1 = (double*) malloc(N * sizeof(double));\n",
        "    double *array2 = (double*) malloc(N * sizeof(double));\n",
        "    double *array3 = (double*) malloc(N * sizeof(double));\n",
        "\n",
        "    // Initialize the arrays\n",
        "    for (int i = 0; i < N; i++) {\n",
        "        array1[i] = i * 1.0;\n",
        "        array2[i] = i * 1.0;\n",
        "        array3[i] = i * 1.0;\n",
        "    }\n",
        "\n",
        "    // Run the computation multiple times to average the timings\n",
        "    int repeats = 3;\n",
        "    double total_time_no_simd = 0.0;\n",
        "    double total_time_simd_basic = 0.0;\n",
        "    double total_time_simd_optimized = 0.0;\n",
        "\n",
        "    for (int r = 0; r < repeats; r++) {\n",
        "        // Measure time for no SIMD\n",
        "        double start_time = omp_get_wtime();\n",
        "        no_simd(array1, N);\n",
        "        double end_time = omp_get_wtime();\n",
        "        total_time_no_simd += (end_time - start_time);\n",
        "\n",
        "        // Measure time for basic SIMD\n",
        "        start_time = omp_get_wtime();\n",
        "        simd_basic(array2, N);\n",
        "        end_time = omp_get_wtime();\n",
        "        total_time_simd_basic += (end_time - start_time);\n",
        "\n",
        "        // Measure time for optimized SIMD with aligned clause\n",
        "        start_time = omp_get_wtime();\n",
        "        simd_optimized(array3, N);\n",
        "        end_time = omp_get_wtime();\n",
        "        total_time_simd_optimized += (end_time - start_time);\n",
        "    }\n",
        "\n",
        "    // Print a sample of the results for verification\n",
        "    printf(\"Array output sample (without SIMD):\\n\");\n",
        "    for (int i = 0; i < 10; i++) {\n",
        "        printf(\"array1[%d] = %f\\n\", i, array1[i]);\n",
        "        fflush(stdout);  // Ensure output is printed immediately\n",
        "    }\n",
        "\n",
        "    printf(\"\\nArray output sample (with basic SIMD):\\n\");\n",
        "    for (int i = 0; i < 10; i++) {\n",
        "        printf(\"array2[%d] = %f\\n\", i, array2[i]);\n",
        "        fflush(stdout);  // Ensure output is printed immediately\n",
        "    }\n",
        "\n",
        "    printf(\"\\nArray output sample (with SIMD + aligned):\\n\");\n",
        "    for (int i = 0; i < 10; i++) {\n",
        "        printf(\"array3[%d] = %f\\n\", i, array3[i]);\n",
        "        fflush(stdout);  // Ensure output is printed immediately\n",
        "    }\n",
        "\n",
        "    // Print the time comparisons\n",
        "    printf(\"\\n=== Performance Comparison ===\\n\");\n",
        "    printf(\"Average time for no SIMD: %f seconds\\n\", total_time_no_simd / repeats);\n",
        "    printf(\"Average time for basic SIMD: %f seconds\\n\", total_time_simd_basic / repeats);\n",
        "    printf(\"Average time for SIMD with aligned clause: %f seconds\\n\", total_time_simd_optimized / repeats);\n",
        "\n",
        "    // Free the allocated memory\n",
        "    free(array1);\n",
        "    free(array2);\n",
        "    free(array3);\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "'''\n",
        "\n",
        "# Save the C code to a file\n",
        "with open(\"simd_comparison_example.c\", \"w\") as code_file:\n",
        "    code_file.write(c_code_comparison)\n",
        "\n",
        "# Compile the C code with OpenMP support\n",
        "compilation_status = subprocess.run([\"gcc\", \"-fopenmp\", \"simd_comparison_example.c\", \"-o\", \"simd_comparison_example\"])\n",
        "\n",
        "# Check if compilation was successful\n",
        "if compilation_status.returncode == 0:\n",
        "    print(\"Compilation successful. Running the program...\")\n",
        "    # Run the compiled program and capture the output\n",
        "    output = subprocess.run([\"./simd_comparison_example\"], capture_output=True, text=True)\n",
        "    # Print the captured output\n",
        "    print(output.stdout)\n",
        "else:\n",
        "    print(\"Compilation failed. Please check the code for errors.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63-P9IIojXcV",
        "outputId": "ea0a7510-823e-43c3-a3a8-87dfeab8cd67"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Compilation successful. Running the program...\n",
            "Array output sample (without SIMD):\n",
            "array1[0] = 0.000000\n",
            "array1[1] = 8.000000\n",
            "array1[2] = 16.000000\n",
            "array1[3] = 24.000000\n",
            "array1[4] = 32.000000\n",
            "array1[5] = 40.000000\n",
            "array1[6] = 48.000000\n",
            "array1[7] = 56.000000\n",
            "array1[8] = 64.000000\n",
            "array1[9] = 72.000000\n",
            "\n",
            "Array output sample (with basic SIMD):\n",
            "array2[0] = 0.000000\n",
            "array2[1] = 8.000000\n",
            "array2[2] = 16.000000\n",
            "array2[3] = 24.000000\n",
            "array2[4] = 32.000000\n",
            "array2[5] = 40.000000\n",
            "array2[6] = 48.000000\n",
            "array2[7] = 56.000000\n",
            "array2[8] = 64.000000\n",
            "array2[9] = 72.000000\n",
            "\n",
            "Array output sample (with SIMD + aligned):\n",
            "array3[0] = 0.000000\n",
            "array3[1] = 8.000000\n",
            "array3[2] = 16.000000\n",
            "array3[3] = 24.000000\n",
            "array3[4] = 32.000000\n",
            "array3[5] = 40.000000\n",
            "array3[6] = 48.000000\n",
            "array3[7] = 56.000000\n",
            "array3[8] = 64.000000\n",
            "array3[9] = 72.000000\n",
            "\n",
            "=== Performance Comparison ===\n",
            "Average time for no SIMD: 1.001669 seconds\n",
            "Average time for basic SIMD: 0.995018 seconds\n",
            "Average time for SIMD with aligned clause: 1.000838 seconds\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explanation of SIMD Clauses for Optimization\n",
        "\n",
        "This example demonstrates how the OpenMP SIMD clauses `aligned` and `collapse` can be used to optimize loop-based computations by enhancing vectorization.\n",
        "\n",
        "### SIMD Directive\n",
        "The `#pragma omp simd` directive explicitly informs the compiler to vectorize the loop, ensuring that multiple iterations of the loop can be executed simultaneously using SIMD instructions.\n",
        "\n",
        "### Collapse Clause\n",
        "The `collapse(2)` clause collapses the two loops (i and j) into a single iteration space. This enables better vectorization across both dimensions of the matrix. This approach is particularly useful when dealing with multi-dimensional arrays, where you want to optimize access patterns for both dimensions.\n",
        "\n",
        "### Aligned Clause\n",
        "The `aligned(array: 32)` clause ensures that the array is aligned on a 32-byte boundary. This improves performance by ensuring that data is efficiently loaded into SIMD registers. Misaligned memory accesses can cause significant slowdowns, so aligning the data helps maximize performance.\n",
        "\n",
        "### Matrix Example\n",
        "The nested loops over the matrix are vectorized with the help of the `collapse(2)` clause, ensuring that both dimensions of the matrix are processed in parallel. Each element of the matrix is multiplied by 2 using SIMD instructions.\n",
        "\n",
        "### Array Example\n",
        "The array is processed in a single loop, and the `aligned(array: 32)` clause ensures that the array is aligned in memory for efficient SIMD processing. Each element in the array is multiplied by 2.\n",
        "\n",
        "### Output\n",
        "The program prints a sample of the processed array to demonstrate that the SIMD vectorization was applied correctly. The matrix and array are efficiently processed using SIMD, improving performance for large data sets.\n",
        "\n",
        "### Key Takeaways\n",
        "- **Better Vectorization**: Using the `collapse` clause allows multiple loops to be collapsed into a single iteration space, enabling better use of SIMD.\n",
        "- **Memory Alignment**: The `aligned` clause ensures that memory accesses are optimized, minimizing penalties for unaligned data.\n",
        "- **Performance**: These SIMD clauses can significantly improve performance for loop-based computations, especially when working with large arrays or matrices that can benefit from vectorized processing.\n"
      ],
      "metadata": {
        "id": "NvTwB_i-joZz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Offloading to GPUs with OpenMP\n",
        "OpenMP supports offloading compute-intensive tasks to GPUs and other accelerators, which can significantly speed up certain computations. This is done using the #pragma omp target directive, allowing developers to move parts of the code from the CPU to the GPU. Offloading tasks to a GPU involves transferring data from the host (CPU) to the device (GPU), performing the computation on the GPU, and then transferring the results back to the host.\n",
        "\n",
        "In GPU offloading, data mapping is essential to manage the flow of data between the CPU and GPU. OpenMP allows developers to specify which data needs to be transferred to and from the GPU using the map clause. The map(to: ...) clause specifies data to be sent to the GPU, while map(from: ...) defines data that should be transferred back to the CPU after computation.\n",
        "\n",
        "Example:\n",
        "```\n",
        "#pragma omp target map(to: A[0:N], B[0:N]) map(from: C[0:N])\n",
        "{\n",
        "    for (int i = 0; i < N; i++) {\n",
        "        C[i] = A[i] + B[i];\n",
        "    }\n",
        "}\n",
        "```\n",
        "\n",
        "This example offloads a simple vector addition computation to the GPU. The A and B arrays are transferred to the GPU, the computation is done on the GPU, and the result stored in array C is transferred back to the CPU after the computation."
      ],
      "metadata": {
        "id": "xw_wQzzZoYlr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OpenMP on CPU vs CUDA on GPU: Performance Comparison\n",
        "\n",
        "This section will guide you through running a vector addition program using OpenMP on the CPU and CUDA on the GPU. We will measure the time taken by both versions to compare performance.\n",
        "\n",
        "### Requirements:\n",
        "1. **OpenMP** for CPU parallelism: Most modern compilers like `gcc` come with OpenMP support.\n",
        "2. **CUDA** for GPU programming: Requires NVIDIA GPU with CUDA and `nvcc` installed.\n",
        "\n",
        "### Steps to Run:\n",
        "1. Run the **OpenMP on CPU** section to parallelize the vector addition on the CPU.\n",
        "2. Run the **CUDA on GPU** section to offload the vector addition to the GPU.\n",
        "3. Compare the execution time between CPU and GPU.\n",
        "\n",
        "---\n",
        "\n",
        "## OpenMP on CPU\n",
        "\n",
        "This section uses OpenMP to run the computation in parallel on the CPU.\n",
        "\n",
        "```python\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "# Define the C code that uses OpenMP on the CPU\n",
        "c_code_cpu_openmp = r'''\n",
        "#include <stdio.h>\n",
        "#include <omp.h>\n",
        "\n",
        "#define N 100000000  // Large array size for parallel computation\n",
        "\n",
        "void init_arrays(float *A, float *B, int size) {\n",
        "    for (int i = 0; i < size; i++) {\n",
        "        A[i] = i * 1.0f;\n",
        "        B[i] = i * 2.0f;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    float *A = (float*) malloc(N * sizeof(float));\n",
        "    float *B = (float*) malloc(N * sizeof(float));\n",
        "    float *C = (float*) malloc(N * sizeof(float));\n",
        "\n",
        "    // Initialize the arrays\n",
        "    init_arrays(A, B, N);\n",
        "\n",
        "    double start_time = omp_get_wtime();\n",
        "\n",
        "    // Perform the computation in parallel on the CPU using OpenMP\n",
        "    #pragma omp parallel for\n",
        "    for (int i = 0; i < N; i++) {\n",
        "        C[i] = A[i] + B[i];\n",
        "    }\n",
        "\n",
        "    double end_time = omp_get_wtime();\n",
        "\n",
        "    // Print the timing result\n",
        "    printf(\"Time taken for OpenMP on CPU: %f seconds\\n\", end_time - start_time);\n",
        "\n",
        "    // Free the allocated memory\n",
        "    free(A);\n",
        "    free(B);\n",
        "    free(C);\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "'''\n",
        "\n",
        "# Save the C code to a file\n",
        "with open(\"cpu_openmp_example.c\", \"w\") as code_file:\n",
        "    code_file.write(c_code_cpu_openmp)\n",
        "\n",
        "# Compile the C code with OpenMP support\n",
        "compilation_status = subprocess.run([\"gcc\", \"-fopenmp\", \"cpu_openmp_example.c\", \"-o\", \"cpu_openmp_example\"])\n",
        "\n",
        "# Check if compilation was successful\n",
        "if compilation_status.returncode == 0:\n",
        "    print(\"Compilation successful. Running the program on the CPU...\")\n",
        "    # Run the compiled program and capture the output\n",
        "    output = subprocess.run([\"./cpu_openmp_example\"], capture_output=True, text=True)\n",
        "    # Print the captured output\n",
        "    print(output.stdout)\n",
        "else:\n",
        "    print(\"Compilation failed. Please check the code for errors.\")\n"
      ],
      "metadata": {
        "id": "Lu5qt-OUqqwz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import subprocess\n",
        "\n",
        "# Define the C code that uses OpenMP for GPU offloading\n",
        "c_code_openmp_target = r'''\n",
        "#include <stdio.h>\n",
        "#include <omp.h>\n",
        "\n",
        "#define N 100000000  // Large array size for offloading\n",
        "\n",
        "void init_arrays(float *A, float *B, int size) {\n",
        "    for (int i = 0; i < size; i++) {\n",
        "        A[i] = i * 1.0f;\n",
        "        B[i] = i * 2.0f;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    float *A = (float*) malloc(N * sizeof(float));\n",
        "    float *B = (float*) malloc(N * sizeof(float));\n",
        "    float *C = (float*) malloc(N * sizeof(float));\n",
        "\n",
        "    // Initialize the arrays\n",
        "    init_arrays(A, B, N);\n",
        "\n",
        "    double start_time = omp_get_wtime();\n",
        "\n",
        "    // Offload the computation to the GPU using OpenMP target\n",
        "    #pragma omp target map(to: A[0:N], B[0:N]) map(from: C[0:N])\n",
        "    {\n",
        "        #pragma omp parallel for\n",
        "        for (int i = 0; i < N; i++) {\n",
        "            C[i] = A[i] + B[i];\n",
        "        }\n",
        "    }\n",
        "\n",
        "    double end_time = omp_get_wtime();\n",
        "\n",
        "    // Print the timing result\n",
        "    printf(\"Time taken for OpenMP target offloading: %f seconds\\n\", end_time - start_time);\n",
        "\n",
        "    // Print a sample of the results for verification\n",
        "    printf(\"Sample results:\\n\");\n",
        "    for (int i = 0; i < 10; i++) {\n",
        "        printf(\"C[%d] = %f\\n\", i, C[i]);\n",
        "    }\n",
        "\n",
        "    // Free the allocated memory\n",
        "    free(A);\n",
        "    free(B);\n",
        "    free(C);\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "'''\n",
        "\n",
        "# Save the C code to a file\n",
        "with open(\"openmp_target_offloading_example.c\", \"w\") as code_file:\n",
        "    code_file.write(c_code_openmp_target)\n",
        "\n",
        "# Compile the C code with OpenMP support (normally you'd use clang with OpenMP GPU offloading support)\n",
        "compilation_status = subprocess.run([\"gcc\", \"-fopenmp\", \"openmp_target_offloading_example.c\", \"-o\", \"openmp_target_offloading_example\"])\n",
        "\n",
        "# Check if compilation was successful\n",
        "if compilation_status.returncode == 0:\n",
        "    print(\"Compilation successful. Running the program on the GPU...\")\n",
        "    # Run the compiled program and capture the output\n",
        "    output = subprocess.run([\"./openmp_target_offloading_example\"], capture_output=True, text=True)\n",
        "    # Print the captured output\n",
        "    print(output.stdout)\n",
        "else:\n",
        "    print(\"Compilation failed. Please check the code for errors.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Mo_9KWdqo5E",
        "outputId": "b217eaa1-be00-4962-8439-5c29f807fe22"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Compilation successful. Running the program on the GPU...\n",
            "Time taken for OpenMP target offloading: 0.376218 seconds\n",
            "Sample results:\n",
            "C[0] = 0.000000\n",
            "C[1] = 3.000000\n",
            "C[2] = 6.000000\n",
            "C[3] = 9.000000\n",
            "C[4] = 12.000000\n",
            "C[5] = 15.000000\n",
            "C[6] = 18.000000\n",
            "C[7] = 21.000000\n",
            "C[8] = 24.000000\n",
            "C[9] = 27.000000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Explanation of the GPU Offloading Code\n",
        "In this example, we offload a vector addition computation to the GPU using OpenMP. The arrays A and B are initialized on the CPU, and then transferred to the GPU using the map(to: A[0:N], B[0:N]) clause. The result array C is computed on the GPU and transferred back to the CPU using the map(from: C[0:N]) clause.\n",
        "\n",
        "1. Data Mapping:\n",
        " - map(to: A[0:N], B[0:N]): This clause transfers the A and B arrays to the GPU. The GPU uses these arrays to perform the computation.\n",
        " - map(from: C[0:N]): After the computation is complete, the results stored in the C array are transferred back from the GPU to the CPU.\n",
        "2. GPU Offloading with #pragma omp target: The #pragma omp target directive specifies that the following block of code will be offloaded to the GPU. In this case, the vector addition loop is executed on the GPU, where each element of C is calculated as the sum of the corresponding elements in A and B.\n",
        "\n",
        "3. Performance Benefits: Offloading to the GPU can greatly speed up compute-intensive tasks, especially when working with large data sets. GPUs are designed for high parallelism, making them ideal for operations that can be done independently on many data elements (such as this vector addition).\n",
        "\n",
        "4. Sample Output: The program prints the first 10 results of the computation from the C array to verify that the GPU offloading worked as expected. The values should match the sum of corresponding elements in A and B after the computation is completed."
      ],
      "metadata": {
        "id": "nYkUr2uGoq6a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OpenMP Scheduling Strategies in Parallel Programming\n",
        "\n",
        "## Introduction\n",
        "\n",
        "In OpenMP, **scheduling** determines how iterations of a loop are divided among threads in parallel regions. Different scheduling strategies affect performance based on the workload and the nature of the task.\n",
        "\n",
        "### Types of Scheduling in OpenMP\n",
        "\n",
        "1. **Static Scheduling**:\n",
        "   - Iterations are divided equally among threads at the start of the parallel region.\n",
        "   - Best for **uniform workloads** where each iteration takes roughly the same time.\n",
        "\n",
        "2. **Dynamic Scheduling**:\n",
        "   - Threads dynamically request new chunks of iterations as they finish previous chunks.\n",
        "   - Useful for **irregular workloads** where the time taken for each iteration may vary significantly.\n",
        "   - You can specify a **chunk size** for how many iterations are assigned to a thread at a time.\n",
        "\n",
        "3. **Guided Scheduling**:\n",
        "   - Threads are initially assigned **large chunks** of iterations, and the chunk size decreases as the computation progresses.\n",
        "   - Suitable for workloads that **decrease in complexity** over time.\n",
        "\n",
        "4. **Auto Scheduling**:\n",
        "   - The **runtime** decides the optimal scheduling method based on the system and workload.\n",
        "   - This strategy is a black-box approach where OpenMP automatically handles the workload distribution.\n",
        "\n",
        "## Purpose of This Example\n",
        "\n",
        "In this notebook, we will compare the execution of a parallel loop using different scheduling strategies (static, dynamic, guided, and auto). We will print the **thread ID** and **iterations** processed by each thread to help visualize how each strategy works. This will show how threads are assigned work chunks and how the choice of scheduling can affect performance.\n",
        "\n",
        "The example uses a simple loop that processes an array, and we will measure the **time taken** for each scheduling strategy to demonstrate their impact on performance.\n",
        "\n",
        "## Code and Output\n",
        "\n",
        "The following code will:\n",
        "- Run the parallel loop with each scheduling strategy.\n",
        "- Print the thread handling each iteration of the loop.\n",
        "- Display the total execution time for each scheduling strategy.\n"
      ],
      "metadata": {
        "id": "haEcXM-6s5j6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import subprocess\n",
        "\n",
        "# Set environment variables to use OpenMP with dynamic threads\n",
        "os.environ['OMP_NUM_THREADS'] = '4'  # Use 4 threads\n",
        "\n",
        "# Define the C code that compares OpenMP scheduling strategies and prints thread execution details\n",
        "c_code_openmp_scheduling_detailed = r'''\n",
        "#include <stdio.h>\n",
        "#include <omp.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "#define N 32  // Smaller array size for demonstration\n",
        "\n",
        "void init_array(double *arr, int size) {\n",
        "    for (int i = 0; i < size; i++) {\n",
        "        arr[i] = (double)i / size;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    double *A = (double*) malloc(N * sizeof(double));\n",
        "\n",
        "    double start_time, end_time;\n",
        "    int thread_id;\n",
        "\n",
        "    // 1. Static Scheduling\n",
        "    printf(\"Static scheduling:\\n\");\n",
        "    start_time = omp_get_wtime();\n",
        "    #pragma omp parallel for schedule(static) private(thread_id)\n",
        "    for (int i = 0; i < N; i++) {\n",
        "        thread_id = omp_get_thread_num();\n",
        "        A[i] = A[i] + 1;\n",
        "        printf(\"Thread %d is processing iteration %d\\n\", thread_id, i);\n",
        "    }\n",
        "    end_time = omp_get_wtime();\n",
        "    printf(\"Static scheduling time: %f seconds\\n\\n\", end_time - start_time);\n",
        "\n",
        "    // 2. Dynamic Scheduling with chunk size 4\n",
        "    printf(\"Dynamic scheduling (chunk=4):\\n\");\n",
        "    start_time = omp_get_wtime();\n",
        "    #pragma omp parallel for schedule(dynamic, 4) private(thread_id)\n",
        "    for (int i = 0; i < N; i++) {\n",
        "        thread_id = omp_get_thread_num();\n",
        "        A[i] = A[i] + 1;\n",
        "        printf(\"Thread %d is processing iteration %d\\n\", thread_id, i);\n",
        "    }\n",
        "    end_time = omp_get_wtime();\n",
        "    printf(\"Dynamic scheduling (chunk=4) time: %f seconds\\n\\n\", end_time - start_time);\n",
        "\n",
        "    // 3. Guided Scheduling\n",
        "    printf(\"Guided scheduling:\\n\");\n",
        "    start_time = omp_get_wtime();\n",
        "    #pragma omp parallel for schedule(guided) private(thread_id)\n",
        "    for (int i = 0; i < N; i++) {\n",
        "        thread_id = omp_get_thread_num();\n",
        "        A[i] = A[i] + 1;\n",
        "        printf(\"Thread %d is processing iteration %d\\n\", thread_id, i);\n",
        "    }\n",
        "    end_time = omp_get_wtime();\n",
        "    printf(\"Guided scheduling time: %f seconds\\n\\n\", end_time - start_time);\n",
        "\n",
        "    // 4. Auto Scheduling\n",
        "    printf(\"Auto scheduling:\\n\");\n",
        "    start_time = omp_get_wtime();\n",
        "    #pragma omp parallel for schedule(auto) private(thread_id)\n",
        "    for (int i = 0; i < N; i++) {\n",
        "        thread_id = omp_get_thread_num();\n",
        "        A[i] = A[i] + 1;\n",
        "        printf(\"Thread %d is processing iteration %d\\n\", thread_id, i);\n",
        "    }\n",
        "    end_time = omp_get_wtime();\n",
        "    printf(\"Auto scheduling time: %f seconds\\n\\n\", end_time - start_time);\n",
        "\n",
        "    // Free allocated memory\n",
        "    free(A);\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "'''\n",
        "\n",
        "# Save the C code to a file\n",
        "with open(\"openmp_scheduling_detailed_example.c\", \"w\") as code_file:\n",
        "    code_file.write(c_code_openmp_scheduling_detailed)\n",
        "\n",
        "# Compile the C code with OpenMP support\n",
        "compilation_status = subprocess.run([\"gcc\", \"-fopenmp\", \"openmp_scheduling_detailed_example.c\", \"-o\", \"openmp_scheduling_detailed_example\"])\n",
        "\n",
        "# Check if compilation was successful\n",
        "if compilation_status.returncode == 0:\n",
        "    print(\"Compilation successful. Running the program...\")\n",
        "    # Run the compiled program and capture the output\n",
        "    output = subprocess.run([\"./openmp_scheduling_detailed_example\"], capture_output=True, text=True)\n",
        "    # Print the captured output\n",
        "    print(output.stdout)\n",
        "else:\n",
        "    print(\"Compilation failed. Please check the code for errors.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOnYAG1vtdoZ",
        "outputId": "d6f3e04b-07aa-4e15-980f-368951fa8577"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Compilation successful. Running the program...\n",
            "Static scheduling:\n",
            "Thread 1 is processing iteration 8\n",
            "Thread 0 is processing iteration 0\n",
            "Thread 0 is processing iteration 1\n",
            "Thread 0 is processing iteration 2\n",
            "Thread 0 is processing iteration 3\n",
            "Thread 1 is processing iteration 9\n",
            "Thread 1 is processing iteration 10\n",
            "Thread 1 is processing iteration 11\n",
            "Thread 1 is processing iteration 12\n",
            "Thread 1 is processing iteration 13\n",
            "Thread 0 is processing iteration 4\n",
            "Thread 1 is processing iteration 14\n",
            "Thread 0 is processing iteration 5\n",
            "Thread 1 is processing iteration 15\n",
            "Thread 0 is processing iteration 6\n",
            "Thread 0 is processing iteration 7\n",
            "Thread 2 is processing iteration 16\n",
            "Thread 2 is processing iteration 17\n",
            "Thread 2 is processing iteration 18\n",
            "Thread 2 is processing iteration 19\n",
            "Thread 2 is processing iteration 20\n",
            "Thread 2 is processing iteration 21\n",
            "Thread 2 is processing iteration 22\n",
            "Thread 2 is processing iteration 23\n",
            "Thread 3 is processing iteration 24\n",
            "Thread 3 is processing iteration 25\n",
            "Thread 3 is processing iteration 26\n",
            "Thread 3 is processing iteration 27\n",
            "Thread 3 is processing iteration 28\n",
            "Thread 3 is processing iteration 29\n",
            "Thread 3 is processing iteration 30\n",
            "Thread 3 is processing iteration 31\n",
            "Static scheduling time: 0.000201 seconds\n",
            "\n",
            "Dynamic scheduling (chunk=4):\n",
            "Thread 0 is processing iteration 0\n",
            "Thread 0 is processing iteration 1\n",
            "Thread 0 is processing iteration 2\n",
            "Thread 2 is processing iteration 4\n",
            "Thread 0 is processing iteration 3\n",
            "Thread 2 is processing iteration 5\n",
            "Thread 0 is processing iteration 8\n",
            "Thread 2 is processing iteration 6\n",
            "Thread 0 is processing iteration 9\n",
            "Thread 2 is processing iteration 7\n",
            "Thread 0 is processing iteration 10\n",
            "Thread 2 is processing iteration 12\n",
            "Thread 0 is processing iteration 11\n",
            "Thread 2 is processing iteration 13\n",
            "Thread 0 is processing iteration 16\n",
            "Thread 2 is processing iteration 14\n",
            "Thread 0 is processing iteration 17\n",
            "Thread 0 is processing iteration 18\n",
            "Thread 0 is processing iteration 19\n",
            "Thread 0 is processing iteration 28\n",
            "Thread 0 is processing iteration 29\n",
            "Thread 0 is processing iteration 30\n",
            "Thread 0 is processing iteration 31\n",
            "Thread 2 is processing iteration 15\n",
            "Thread 1 is processing iteration 20\n",
            "Thread 1 is processing iteration 21\n",
            "Thread 1 is processing iteration 22\n",
            "Thread 1 is processing iteration 23\n",
            "Thread 3 is processing iteration 24\n",
            "Thread 3 is processing iteration 25\n",
            "Thread 3 is processing iteration 26\n",
            "Thread 3 is processing iteration 27\n",
            "Dynamic scheduling (chunk=4) time: 0.000056 seconds\n",
            "\n",
            "Guided scheduling:\n",
            "Thread 0 is processing iteration 0\n",
            "Thread 0 is processing iteration 1\n",
            "Thread 0 is processing iteration 2\n",
            "Thread 0 is processing iteration 3\n",
            "Thread 0 is processing iteration 4\n",
            "Thread 0 is processing iteration 5\n",
            "Thread 1 is processing iteration 8\n",
            "Thread 1 is processing iteration 9\n",
            "Thread 1 is processing iteration 10\n",
            "Thread 2 is processing iteration 14\n",
            "Thread 1 is processing iteration 11\n",
            "Thread 2 is processing iteration 15\n",
            "Thread 1 is processing iteration 12\n",
            "Thread 2 is processing iteration 16\n",
            "Thread 1 is processing iteration 13\n",
            "Thread 2 is processing iteration 17\n",
            "Thread 1 is processing iteration 19\n",
            "Thread 2 is processing iteration 18\n",
            "Thread 1 is processing iteration 20\n",
            "Thread 2 is processing iteration 23\n",
            "Thread 1 is processing iteration 21\n",
            "Thread 2 is processing iteration 24\n",
            "Thread 1 is processing iteration 22\n",
            "Thread 2 is processing iteration 25\n",
            "Thread 1 is processing iteration 26\n",
            "Thread 2 is processing iteration 28\n",
            "Thread 1 is processing iteration 27\n",
            "Thread 2 is processing iteration 29\n",
            "Thread 1 is processing iteration 30\n",
            "Thread 2 is processing iteration 31\n",
            "Thread 0 is processing iteration 6\n",
            "Thread 0 is processing iteration 7\n",
            "Guided scheduling time: 0.000040 seconds\n",
            "\n",
            "Auto scheduling:\n",
            "Thread 3 is processing iteration 24\n",
            "Thread 3 is processing iteration 25\n",
            "Thread 3 is processing iteration 26\n",
            "Thread 3 is processing iteration 27\n",
            "Thread 3 is processing iteration 28\n",
            "Thread 3 is processing iteration 29\n",
            "Thread 3 is processing iteration 30\n",
            "Thread 3 is processing iteration 31\n",
            "Thread 0 is processing iteration 0\n",
            "Thread 0 is processing iteration 1\n",
            "Thread 0 is processing iteration 2\n",
            "Thread 0 is processing iteration 3\n",
            "Thread 0 is processing iteration 4\n",
            "Thread 0 is processing iteration 5\n",
            "Thread 0 is processing iteration 6\n",
            "Thread 0 is processing iteration 7\n",
            "Thread 1 is processing iteration 8\n",
            "Thread 1 is processing iteration 9\n",
            "Thread 1 is processing iteration 10\n",
            "Thread 1 is processing iteration 11\n",
            "Thread 1 is processing iteration 12\n",
            "Thread 1 is processing iteration 13\n",
            "Thread 1 is processing iteration 14\n",
            "Thread 1 is processing iteration 15\n",
            "Thread 2 is processing iteration 16\n",
            "Thread 2 is processing iteration 17\n",
            "Thread 2 is processing iteration 18\n",
            "Thread 2 is processing iteration 19\n",
            "Thread 2 is processing iteration 20\n",
            "Thread 2 is processing iteration 21\n",
            "Thread 2 is processing iteration 22\n",
            "Thread 2 is processing iteration 23\n",
            "Auto scheduling time: 0.000065 seconds\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Explanation of Results:\n",
        "- Static Scheduling is generally the best for uniform workloads because it distributes the work equally across all threads with no overhead during execution.\n",
        "\n",
        "- Dynamic Scheduling is better for irregular workloads because threads pick up new tasks as soon as they finish, reducing idle time.\n",
        "- Guided Scheduling is ideal for tasks that reduce in complexity over time, as it assigns progressively smaller chunks of work to threads.\n",
        "- Auto Scheduling lets the OpenMP runtime choose the best scheduling method based on the system and workload.\n",
        "###Try Running:\n",
        "You can try adjusting the size of the array (N) and the chunk size in dynamic scheduling to see how it impacts performance.\n",
        "Additionally, you can change the number of threads used by adjusting the OMP_NUM_THREADS environment variable."
      ],
      "metadata": {
        "id": "toK8PwXatOWx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Enhancing a Serial Program with Advanced OpenMP\n",
        "\n",
        "## Introduction\n",
        "\n",
        "This guide demonstrates how to convert a serial N-body simulation into a highly optimized parallel program using advanced OpenMP features. The N-body problem simulates the interactions of particles under gravitational forces, making it a computationally intensive task, especially for large numbers of particles.\n",
        "\n",
        "We will begin by implementing a task-based parallelism approach and gradually introduce more advanced techniques such as managing task dependencies, leveraging the `taskloop` directive, SIMD parallelism, and offloading computations to a GPU using OpenMP's `target` directives.\n",
        "\n",
        "The result will be a significant performance improvement, efficiently utilizing modern multicore processors and GPUs.\n",
        "\n",
        "---\n",
        "\n",
        "### Overview of Steps:\n",
        "1. **Task-Based Parallelism**: Splitting the force computation into tasks.\n",
        "2. **Task Dependencies**: Ensuring tasks execute in the correct order.\n",
        "3. **Taskloop Directive**: Simplifying task creation.\n",
        "4. **SIMD Parallelism**: Exploiting data-level parallelism.\n",
        "5. **GPU Offloading**: Offloading intensive computations to the GPU.\n",
        "\n",
        "We will start by reviewing the serial implementation and then proceed step-by-step to improve it using advanced OpenMP constructs.\n"
      ],
      "metadata": {
        "id": "RAckKDUVt6fR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "# Define the C code for the serial (non-parallel) N-body simulation\n",
        "nbody_serial_code = r'''\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <math.h>\n",
        "#include <time.h>\n",
        "\n",
        "#define G 6.67430e-11   // Gravitational constant\n",
        "#define EPSILON 1e-9    // Softening factor to prevent singularities\n",
        "\n",
        "typedef struct {\n",
        "    double x, y, z;     // Position components\n",
        "    double vx, vy, vz;  // Velocity components\n",
        "    double ax, ay, az;  // Acceleration components\n",
        "    double mass;        // Mass of the particle\n",
        "} Body;\n",
        "\n",
        "// Serial version of the compute_forces function (No parallelism)\n",
        "void compute_forces_serial(int N, Body *bodies) {\n",
        "    for (int i = 0; i < N; i++) {\n",
        "        bodies[i].ax = bodies[i].ay = bodies[i].az = 0.0;  // Reset acceleration\n",
        "        for (int j = 0; j < N; j++) {\n",
        "            if (i != j) {\n",
        "                double dx = bodies[j].x - bodies[i].x;\n",
        "                double dy = bodies[j].y - bodies[i].y;\n",
        "                double dz = bodies[j].z - bodies[i].z;\n",
        "                double dist_sqr = dx * dx + dy * dy + dz * dz + EPSILON;\n",
        "                double inv_dist = 1.0 / sqrt(dist_sqr);\n",
        "                double inv_dist3 = inv_dist * inv_dist * inv_dist;\n",
        "                double force = G * bodies[j].mass * inv_dist3;\n",
        "                bodies[i].ax += force * dx;\n",
        "                bodies[i].ay += force * dy;\n",
        "                bodies[i].az += force * dz;\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "// Simulation setup and main function\n",
        "int main() {\n",
        "    int N = 1000;          // Reduced number of particles for simplicity\n",
        "    double dt = 0.01;     // Time step\n",
        "    int steps = 100;       // Number of simulation steps\n",
        "\n",
        "    // Allocate memory for bodies\n",
        "    Body *bodies = (Body *)malloc(N * sizeof(Body));\n",
        "\n",
        "    // Initialize bodies with simple random values (positions and masses)\n",
        "    for (int i = 0; i < N; i++) {\n",
        "        bodies[i].x = rand() % 1000;\n",
        "        bodies[i].y = rand() % 1000;\n",
        "        bodies[i].z = rand() % 1000;\n",
        "        bodies[i].vx = 0;\n",
        "        bodies[i].vy = 0;\n",
        "        bodies[i].vz = 0;\n",
        "        bodies[i].mass = rand() % 100 + 1;\n",
        "    }\n",
        "\n",
        "    // Start timer\n",
        "    clock_t start_time = clock();\n",
        "\n",
        "    // Run the N-body simulation using the serial force computation\n",
        "    for (int s = 0; s < steps; s++) {\n",
        "        compute_forces_serial(N, bodies);\n",
        "        // Update positions and velocities here...\n",
        "        for (int i = 0; i < N; i++) {\n",
        "            bodies[i].vx += bodies[i].ax * dt;\n",
        "            bodies[i].vy += bodies[i].ay * dt;\n",
        "            bodies[i].vz += bodies[i].az * dt;\n",
        "\n",
        "            bodies[i].x += bodies[i].vx * dt;\n",
        "            bodies[i].y += bodies[i].vy * dt;\n",
        "            bodies[i].z += bodies[i].vz * dt;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // End timer\n",
        "    clock_t end_time = clock();\n",
        "    double elapsed_time = (double)(end_time - start_time) / CLOCKS_PER_SEC;\n",
        "\n",
        "    // Print the elapsed time for the simulation\n",
        "    printf(\"Elapsed time for serial N-body simulation: %f seconds\\n\", elapsed_time);\n",
        "\n",
        "    // Print some sample results\n",
        "    printf(\"Sample output after %d steps:\\n\", steps);\n",
        "    for (int i = 0; i < 10; i++) {\n",
        "        printf(\"Body %d: Position (%f, %f, %f)\\n\", i, bodies[i].x, bodies[i].y, bodies[i].z);\n",
        "    }\n",
        "\n",
        "    // Free allocated memory\n",
        "    free(bodies);\n",
        "    return 0;\n",
        "}\n",
        "'''\n",
        "\n",
        "# Save the C code to a file\n",
        "with open(\"nbody_serial_simulation.c\", \"w\") as code_file:\n",
        "    code_file.write(nbody_serial_code)\n",
        "\n",
        "# Compile the C code with the math library\n",
        "compilation_status = subprocess.run(\n",
        "    [\"gcc\", \"nbody_serial_simulation.c\", \"-o\", \"nbody_serial_simulation\", \"-lm\"],\n",
        "    capture_output=True,\n",
        "    text=True\n",
        ")\n",
        "\n",
        "# Check if compilation was successful\n",
        "if compilation_status.returncode == 0:\n",
        "    print(\"Compilation successful. Running the serial version of the program...\")\n",
        "    # Run the compiled program and capture the output\n",
        "    output = subprocess.run([\"./nbody_serial_simulation\"], capture_output=True, text=True)\n",
        "    # Print the captured output\n",
        "    print(output.stdout)\n",
        "    if output.stderr:\n",
        "        print(\"Program stderr:\", output.stderr)\n",
        "else:\n",
        "    print(\"Compilation failed. Please check the code for errors.\")\n",
        "    print(\"Compiler stderr:\", compilation_status.stderr)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2iLKMyaxcqT",
        "outputId": "84470d8b-bd31-4d4c-fa3e-df5edfe6bdf3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Compilation successful. Running the serial version of the program...\n",
            "Elapsed time for serial N-body simulation: 2.880601 seconds\n",
            "Sample output after 100 steps:\n",
            "Body 0: Position (383.000000, 886.000000, 777.000000)\n",
            "Body 1: Position (793.000000, 335.000000, 386.000000)\n",
            "Body 2: Position (649.000000, 421.000000, 362.000000)\n",
            "Body 3: Position (690.000000, 59.000000, 763.000000)\n",
            "Body 4: Position (540.000000, 426.000000, 172.000000)\n",
            "Body 5: Position (211.000000, 368.000000, 567.000000)\n",
            "Body 6: Position (782.000000, 530.000000, 862.000000)\n",
            "Body 7: Position (67.000000, 135.000000, 929.000000)\n",
            "Body 8: Position (22.000000, 58.000000, 69.000000)\n",
            "Body 9: Position (393.000000, 456.000000, 11.000000)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###And now the parallel version:"
      ],
      "metadata": {
        "id": "QbQZLNuQyeLk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import subprocess\n",
        "\n",
        "# Set environment variable for OpenMP in Colab\n",
        "os.environ['OMP_NUM_THREADS'] = '4'\n",
        "\n",
        "# Define the C code for the simplified N-body simulation with OpenMP (CPU-only)\n",
        "nbody_code = r'''\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <math.h>\n",
        "#include <omp.h>\n",
        "#include <time.h>\n",
        "\n",
        "#define G 6.67430e-11   // Gravitational constant\n",
        "#define EPSILON 1e-9    // Softening factor to prevent singularities\n",
        "\n",
        "typedef struct {\n",
        "    double x, y, z;     // Position components\n",
        "    double vx, vy, vz;  // Velocity components\n",
        "    double ax, ay, az;  // Acceleration components\n",
        "    double mass;        // Mass of the particle\n",
        "} Body;\n",
        "\n",
        "// OpenMP Parallel version of compute_forces function (CPU only)\n",
        "void compute_forces_parallel(int N, Body *bodies) {\n",
        "    // Parallelize the outer loop using OpenMP\n",
        "    #pragma omp parallel for schedule(dynamic)\n",
        "    for (int i = 0; i < N; i++) {\n",
        "        // Reset acceleration for body i\n",
        "        bodies[i].ax = bodies[i].ay = bodies[i].az = 0.0;\n",
        "        for (int j = 0; j < N; j++) {\n",
        "            if (i != j) {\n",
        "                double dx = bodies[j].x - bodies[i].x;\n",
        "                double dy = bodies[j].y - bodies[i].y;\n",
        "                double dz = bodies[j].z - bodies[i].z;\n",
        "                double dist_sqr = dx * dx + dy * dy + dz * dz + EPSILON;\n",
        "                double inv_dist = 1.0 / sqrt(dist_sqr);\n",
        "                double inv_dist3 = inv_dist * inv_dist * inv_dist;\n",
        "                double force = G * bodies[j].mass * inv_dist3;\n",
        "                bodies[i].ax += force * dx;\n",
        "                bodies[i].ay += force * dy;\n",
        "                bodies[i].az += force * dz;\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "// Function to initialize bodies with random positions and masses\n",
        "void initialize_bodies(int N, Body *bodies) {\n",
        "    // Seed the random number generator for reproducibility\n",
        "    srand(time(NULL));\n",
        "    for (int i = 0; i < N; i++) {\n",
        "        bodies[i].x = ((double)(rand() % 1000)) / 10.0;  // Positions in range [0, 100)\n",
        "        bodies[i].y = ((double)(rand() % 1000)) / 10.0;\n",
        "        bodies[i].z = ((double)(rand() % 1000)) / 10.0;\n",
        "        bodies[i].vx = 0.0;\n",
        "        bodies[i].vy = 0.0;\n",
        "        bodies[i].vz = 0.0;\n",
        "        bodies[i].mass = ((double)(rand() % 100)) + 1.0;  // Mass in range [1, 100]\n",
        "    }\n",
        "}\n",
        "\n",
        "// Simulation setup and main function\n",
        "int main() {\n",
        "    int N = 1000;         // Number of particles\n",
        "    double dt = 0.01;     // Time step\n",
        "    int steps = 100;      // Number of simulation steps\n",
        "\n",
        "    // Allocate memory for bodies\n",
        "    Body *bodies = (Body *)malloc(N * sizeof(Body));\n",
        "    if (bodies == NULL) {\n",
        "        fprintf(stderr, \"Error allocating memory for bodies.\\n\");\n",
        "        return 1;\n",
        "    }\n",
        "\n",
        "    // Initialize bodies with random positions and masses\n",
        "    initialize_bodies(N, bodies);\n",
        "\n",
        "    // Start timer\n",
        "    double start_time = omp_get_wtime();\n",
        "\n",
        "    // Run the N-body simulation using parallel force computation\n",
        "    for (int s = 0; s < steps; s++) {\n",
        "        compute_forces_parallel(N, bodies);\n",
        "        // Update positions and velocities\n",
        "        for (int i = 0; i < N; i++) {\n",
        "            bodies[i].vx += bodies[i].ax * dt;\n",
        "            bodies[i].vy += bodies[i].ay * dt;\n",
        "            bodies[i].vz += bodies[i].az * dt;\n",
        "\n",
        "            bodies[i].x += bodies[i].vx * dt;\n",
        "            bodies[i].y += bodies[i].vy * dt;\n",
        "            bodies[i].z += bodies[i].vz * dt;\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // End timer\n",
        "    double end_time = omp_get_wtime();\n",
        "    double elapsed_time = end_time - start_time;\n",
        "\n",
        "    // Print the elapsed time for the simulation\n",
        "    printf(\"Elapsed time for parallel N-body simulation: %f seconds\\n\", elapsed_time);\n",
        "\n",
        "    // Print some sample results\n",
        "    printf(\"Sample output after %d steps:\\n\", steps);\n",
        "    for (int i = 0; i < 10 && i < N; i++) {\n",
        "        printf(\"Body %d: Position (%.2f, %.2f, %.2f)\\n\", i, bodies[i].x, bodies[i].y, bodies[i].z);\n",
        "    }\n",
        "\n",
        "    // Free allocated memory\n",
        "    free(bodies);\n",
        "    return 0;\n",
        "}\n",
        "'''\n",
        "\n",
        "# Save the C code to a file\n",
        "with open(\"nbody_simulation_openmp.c\", \"w\") as code_file:\n",
        "    code_file.write(nbody_code)\n",
        "\n",
        "# Compile the C code with OpenMP and math library support\n",
        "compilation_command = [\n",
        "    \"gcc\",\n",
        "    \"-fopenmp\",                       # Enable OpenMP\n",
        "    \"nbody_simulation_openmp.c\",      # Source file\n",
        "    \"-o\",\n",
        "    \"nbody_simulation_openmp\",        # Output executable\n",
        "    \"-lm\",                             # Link math library\n",
        "    \"-O2\"                             # Optimization level (optional but recommended)\n",
        "]\n",
        "\n",
        "compilation_status = subprocess.run(\n",
        "    compilation_command,\n",
        "    capture_output=True,\n",
        "    text=True\n",
        ")\n",
        "\n",
        "# Check if compilation was successful\n",
        "if compilation_status.returncode == 0:\n",
        "    print(\"Compilation successful. Running the OpenMP-enabled program...\")\n",
        "    # Run the compiled program and capture the output\n",
        "    execution_status = subprocess.run(\n",
        "        [\"./nbody_simulation_openmp\"],\n",
        "        capture_output=True,\n",
        "        text=True\n",
        "    )\n",
        "    # Print the captured output\n",
        "    print(execution_status.stdout)\n",
        "    if execution_status.stderr:\n",
        "        print(\"Program stderr:\", execution_status.stderr)\n",
        "else:\n",
        "    print(\"Compilation failed. Please check the code for errors.\")\n",
        "    print(\"Compiler stderr:\", compilation_status.stderr)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pkQW5yerw2HJ",
        "outputId": "ae64c201-cfc0-488f-e1dd-d05640e0f8f3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Compilation successful. Running the OpenMP-enabled program...\n",
            "Elapsed time for parallel N-body simulation: 0.958538 seconds\n",
            "Sample output after 100 steps:\n",
            "Body 0: Position (34.60, 84.30, 96.50)\n",
            "Body 1: Position (15.80, 33.80, 30.90)\n",
            "Body 2: Position (5.90, 79.50, 31.90)\n",
            "Body 3: Position (12.30, 63.20, 7.10)\n",
            "Body 4: Position (18.20, 95.00, 27.00)\n",
            "Body 5: Position (63.00, 6.50, 72.70)\n",
            "Body 6: Position (73.80, 67.20, 28.10)\n",
            "Body 7: Position (7.00, 94.60, 57.50)\n",
            "Body 8: Position (79.00, 54.00, 97.80)\n",
            "Body 9: Position (23.00, 28.80, 55.00)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "JhELmgxrwAYw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explanation of Advanced OpenMP Features\n",
        "\n",
        "In this section, we progressively enhanced the serial N-body simulation using various advanced OpenMP techniques.\n",
        "\n",
        "### 1. Task-Based Parallelism\n",
        "We first parallelized the **compute_forces** function by creating tasks for each particle’s force computation. By using OpenMP’s tasking model, we allow the OpenMP runtime to dynamically distribute these tasks across available threads, improving load balancing.\n",
        "\n",
        "- **#pragma omp parallel**: Creates a parallel region where threads can work concurrently.\n",
        "- **#pragma omp single**: Ensures only one thread creates tasks.\n",
        "- **#pragma omp task**: Each particle's force computation is assigned as a task.\n",
        "- **firstprivate(i)**: Ensures each task gets its own copy of the index `i`.\n",
        "\n",
        "### 2. Task Dependencies\n",
        "To ensure that tasks execute in the correct order, we can manage dependencies using OpenMP’s `depend` clause. This ensures that tasks computing forces finish before the tasks updating positions begin, preventing race conditions.\n",
        "\n",
        "### 3. Taskloop Directive\n",
        "In large simulations, creating individual tasks for each iteration can introduce overhead. The **taskloop** directive reduces this overhead by allowing OpenMP to automatically split the loop iterations into tasks.\n",
        "\n",
        "### 4. SIMD Parallelism\n",
        "By adding the **simd** directive, we leverage vectorization to parallelize the inner loop that computes forces. SIMD (Single Instruction, Multiple Data) allows modern CPUs to process multiple data points simultaneously, improving performance for the loop over particle interactions.\n",
        "\n",
        "- **#pragma omp simd**: Vectorizes the loop over particle pairs.\n",
        "- **reduction(+: bodies[i].ax, bodies[i].ay, bodies[i].az)**: Ensures that the accumulation of forces is handled correctly across SIMD lanes.\n",
        "\n",
        "### 5. GPU Offloading\n",
        "Finally, we offload the force computation to a GPU using OpenMP’s `target` directive. This allows the program to take advantage of the thousands of cores available on GPUs.\n",
        "\n",
        "- **#pragma omp target data map(tofrom: bodies[0:N])**: Maps the bodies array to GPU memory.\n",
        "- **#pragma omp target teams distribute parallel for simd**: Offloads the force computation loop to the GPU, where it is distributed across teams of threads, with SIMD parallelization within each thread.\n",
        "\n",
        "---\n",
        "\n",
        "### Performance Considerations:\n",
        "- **Task-based parallelism** improves load balancing, especially for irregular workloads.\n",
        "- **SIMD parallelism** exploits data-level parallelism to improve performance within a loop.\n",
        "- **GPU offloading** harnesses the power of thousands of cores to speed up the most computationally intensive part of the simulation.\n",
        "\n",
        "By combining these advanced techniques, we achieve significant performance improvements, especially for large-scale simulations.\n"
      ],
      "metadata": {
        "id": "hfMYh20AwE6O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import subprocess\n",
        "\n",
        "# Set environment variable for OpenMP in Colab\n",
        "os.environ['OMP_NUM_THREADS'] = str(os.cpu_count())\n",
        "\n",
        "# Define the C code for the N-body simulation with OpenMP and enhanced output\n",
        "nbody_code = r'''\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <math.h>\n",
        "#include <omp.h>\n",
        "#include <time.h>\n",
        "\n",
        "// Constants\n",
        "#define G 6.67430e-11   // Gravitational constant\n",
        "#define EPSILON 1e-9    // Softening factor to prevent singularities\n",
        "\n",
        "typedef struct {\n",
        "    double x, y, z;     // Position components\n",
        "    double vx, vy, vz;  // Velocity components\n",
        "    double ax, ay, az;  // Acceleration components\n",
        "    double mass;        // Mass of the particle\n",
        "} Body;\n",
        "\n",
        "// Function to initialize bodies with random positions, velocities, and masses\n",
        "void initialize_bodies(int N, Body *bodies) {\n",
        "    srand(time(NULL));\n",
        "    for (int i = 0; i < N; i++) {\n",
        "        bodies[i].x = ((double)rand() / RAND_MAX) * 100.0;\n",
        "        bodies[i].y = ((double)rand() / RAND_MAX) * 100.0;\n",
        "        bodies[i].z = ((double)rand() / RAND_MAX) * 100.0;\n",
        "        bodies[i].vx = ((double)rand() / RAND_MAX) * 10.0;\n",
        "        bodies[i].vy = ((double)rand() / RAND_MAX) * 10.0;\n",
        "        bodies[i].vz = ((double)rand() / RAND_MAX) * 10.0;\n",
        "        bodies[i].mass = ((double)rand() / RAND_MAX) * 1e30 + 1e20; // Avoid zero mass\n",
        "        bodies[i].ax = bodies[i].ay = bodies[i].az = 0.0;\n",
        "    }\n",
        "}\n",
        "\n",
        "// Function to compute forces using OpenMP Task-Based Parallelism\n",
        "void compute_forces_parallel(int N, Body *bodies) {\n",
        "    #pragma omp parallel\n",
        "    {\n",
        "        #pragma omp single\n",
        "        {\n",
        "            for (int i = 0; i < N; i++) {\n",
        "                #pragma omp task firstprivate(i)\n",
        "                {\n",
        "                    bodies[i].ax = bodies[i].ay = bodies[i].az = 0.0;\n",
        "                    for (int j = 0; j < N; j++) {\n",
        "                        if (i != j) {\n",
        "                            double dx = bodies[j].x - bodies[i].x;\n",
        "                            double dy = bodies[j].y - bodies[i].y;\n",
        "                            double dz = bodies[j].z - bodies[i].z;\n",
        "                            double dist_sqr = dx * dx + dy * dy + dz * dz + EPSILON;\n",
        "                            double inv_dist = 1.0 / sqrt(dist_sqr);\n",
        "                            double inv_dist3 = inv_dist * inv_dist * inv_dist;\n",
        "                            double force = G * bodies[j].mass * inv_dist3;\n",
        "                            bodies[i].ax += force * dx;\n",
        "                            bodies[i].ay += force * dy;\n",
        "                            bodies[i].az += force * dz;\n",
        "                        }\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "            #pragma omp taskwait\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "// Function to compute total kinetic energy\n",
        "double compute_kinetic_energy(int N, Body *bodies) {\n",
        "    double kinetic = 0.0;\n",
        "    #pragma omp parallel for reduction(+:kinetic)\n",
        "    for (int i = 0; i < N; i++) {\n",
        "        kinetic += 0.5 * bodies[i].mass * (bodies[i].vx * bodies[i].vx +\n",
        "                                           bodies[i].vy * bodies[i].vy +\n",
        "                                           bodies[i].vz * bodies[i].vz);\n",
        "    }\n",
        "    return kinetic;\n",
        "}\n",
        "\n",
        "// Function to compute total potential energy\n",
        "double compute_potential_energy(int N, Body *bodies) {\n",
        "    double potential = 0.0;\n",
        "    #pragma omp parallel for reduction(+:potential)\n",
        "    for (int i = 0; i < N; i++) {\n",
        "        for (int j = i + 1; j < N; j++) {\n",
        "            double dx = bodies[j].x - bodies[i].x;\n",
        "            double dy = bodies[j].y - bodies[i].y;\n",
        "            double dz = bodies[j].z - bodies[i].z;\n",
        "            double dist = sqrt(dx * dx + dy * dy + dz * dz + EPSILON);\n",
        "            potential -= G * bodies[i].mass * bodies[j].mass / dist;\n",
        "        }\n",
        "    }\n",
        "    return potential;\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int N = 1000;          // Number of particles\n",
        "    double dt = 0.01;      // Time step\n",
        "    int steps = 100;       // Number of simulation steps\n",
        "    int print_interval = 10; // Interval to print particle positions\n",
        "\n",
        "    // Allocate memory for bodies\n",
        "    Body *bodies = (Body *)malloc(N * sizeof(Body));\n",
        "    if (bodies == NULL) {\n",
        "        fprintf(stderr, \"Memory allocation failed.\\n\");\n",
        "        return 1;\n",
        "    }\n",
        "\n",
        "    // Initialize bodies (positions, velocities, masses)\n",
        "    initialize_bodies(N, bodies);\n",
        "\n",
        "    printf(\"Starting N-Body Simulation with %d particles and %d steps.\\n\", N, steps);\n",
        "\n",
        "    // Simulation loop\n",
        "    for (int s = 0; s < steps; s++) {\n",
        "        double start_time = omp_get_wtime(); // Start timing\n",
        "\n",
        "        compute_forces_parallel(N, bodies);\n",
        "\n",
        "        // Update positions and velocities\n",
        "        #pragma omp parallel for\n",
        "        for (int i = 0; i < N; i++) {\n",
        "            bodies[i].vx += bodies[i].ax * dt;\n",
        "            bodies[i].vy += bodies[i].ay * dt;\n",
        "            bodies[i].vz += bodies[i].az * dt;\n",
        "            bodies[i].x += bodies[i].vx * dt;\n",
        "            bodies[i].y += bodies[i].vy * dt;\n",
        "            bodies[i].z += bodies[i].vz * dt;\n",
        "        }\n",
        "\n",
        "        double end_time = omp_get_wtime(); // End timing\n",
        "        double computation_time = end_time - start_time;\n",
        "\n",
        "        // Compute energies\n",
        "        double kinetic = compute_kinetic_energy(N, bodies);\n",
        "        double potential = compute_potential_energy(N, bodies);\n",
        "        double total_energy = kinetic + potential;\n",
        "\n",
        "        // Progress reporting\n",
        "        printf(\"Step %d/%d | Kinetic Energy: %.3e | Potential Energy: %.3e | Total Energy: %.3e | Computation Time: %.3f seconds\\n\",\n",
        "               s + 1, steps, kinetic, potential, total_energy, computation_time);\n",
        "\n",
        "        // Print positions of the first 5 particles at specified intervals\n",
        "        if ((s + 1) % print_interval == 0 || s == 0 || s == steps - 1) {\n",
        "            printf(\"Positions of first 5 particles at step %d:\\n\", s + 1);\n",
        "            for (int i = 0; i < 5 && i < N; i++) {\n",
        "                printf(\"  Particle %d: (%.2f, %.2f, %.2f)\\n\", i, bodies[i].x, bodies[i].y, bodies[i].z);\n",
        "            }\n",
        "        }\n",
        "\n",
        "        printf(\"------------------------------------------------------------\\n\");\n",
        "    }\n",
        "\n",
        "    printf(\"Simulation completed.\\n\");\n",
        "\n",
        "    // Optionally, print final positions or other statistics\n",
        "    /*\n",
        "    for (int i = 0; i < N; i++) {\n",
        "        printf(\"Body %d: Position=(%f, %f, %f)\\n\", i, bodies[i].x, bodies[i].y, bodies[i].z);\n",
        "    }\n",
        "    */\n",
        "\n",
        "    // Cleanup\n",
        "    free(bodies);\n",
        "    return 0;\n",
        "}\n",
        "'''\n",
        "\n",
        "# Save the C code to a file\n",
        "with open(\"nbody_simulation.c\", \"w\") as code_file:\n",
        "    code_file.write(nbody_code)\n",
        "\n",
        "# Compile the C code with OpenMP and math library support\n",
        "compilation_status = subprocess.run(\n",
        "    [\"gcc\", \"-fopenmp\", \"nbody_simulation.c\", \"-o\", \"nbody_simulation\", \"-lm\"],\n",
        "    capture_output=True, text=True\n",
        ")\n",
        "\n",
        "# Check if compilation was successful\n",
        "if compilation_status.returncode == 0:\n",
        "    print(\"Compilation successful. Running the program...\\n\")\n",
        "    # Run the compiled program and capture the output\n",
        "    output = subprocess.run([\"./nbody_simulation\"], capture_output=True, text=True)\n",
        "    # Print the captured output\n",
        "    print(output.stdout)\n",
        "else:\n",
        "    print(\"Compilation failed. Please check the code for errors.\")\n",
        "    print(\"Error message:\")\n",
        "    print(compilation_status.stderr)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_i5evEJdwAsw",
        "outputId": "6ae383de-a881-499d-ba10-36198601aefd"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Compilation successful. Running the program...\n",
            "\n",
            "Starting N-Body Simulation with 1000 particles and 100 steps.\n",
            "Step 1/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -1.096e+40 | Total Energy: 1.843e+66 | Computation Time: 0.029 seconds\n",
            "Positions of first 5 particles at step 1:\n",
            "  Particle 0: (-113794784194541.98, 1745165266368990.75, -1274934020774797.25)\n",
            "  Particle 1: (-451451231689045.00, -268795709157212.44, 142926037400894.81)\n",
            "  Particle 2: (606401043941068.12, -294542885891247.50, -81270443880053.84)\n",
            "  Particle 3: (55807056639812.14, -233504520431276.50, 379048569053322.62)\n",
            "  Particle 4: (-179573343460124.34, 109829622945531.12, 688579714195666.25)\n",
            "------------------------------------------------------------\n",
            "Step 2/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -5.481e+39 | Total Energy: 1.843e+66 | Computation Time: 0.027 seconds\n",
            "------------------------------------------------------------\n",
            "Step 3/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -3.654e+39 | Total Energy: 1.843e+66 | Computation Time: 0.026 seconds\n",
            "------------------------------------------------------------\n",
            "Step 4/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -2.740e+39 | Total Energy: 1.843e+66 | Computation Time: 0.025 seconds\n",
            "------------------------------------------------------------\n",
            "Step 5/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -2.192e+39 | Total Energy: 1.843e+66 | Computation Time: 0.031 seconds\n",
            "------------------------------------------------------------\n",
            "Step 6/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -1.827e+39 | Total Energy: 1.843e+66 | Computation Time: 0.033 seconds\n",
            "------------------------------------------------------------\n",
            "Step 7/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -1.566e+39 | Total Energy: 1.843e+66 | Computation Time: 0.051 seconds\n",
            "------------------------------------------------------------\n",
            "Step 8/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -1.370e+39 | Total Energy: 1.843e+66 | Computation Time: 0.054 seconds\n",
            "------------------------------------------------------------\n",
            "Step 9/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -1.218e+39 | Total Energy: 1.843e+66 | Computation Time: 0.045 seconds\n",
            "------------------------------------------------------------\n",
            "Step 10/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -1.096e+39 | Total Energy: 1.843e+66 | Computation Time: 0.050 seconds\n",
            "Positions of first 5 particles at step 10:\n",
            "  Particle 0: (-1137947841945963.00, 17451652663689668.00, -12749340207748248.00)\n",
            "  Particle 1: (-4514512316891090.00, -2687957091572714.50, 1429260374008797.00)\n",
            "  Particle 2: (6064010439410605.00, -2945428858913162.00, -812704438800965.12)\n",
            "  Particle 3: (558070566397753.25, -2335045204313387.50, 3790485690532958.00)\n",
            "  Particle 4: (-1795733434601940.75, 1098296229454886.75, 6885797141956624.00)\n",
            "------------------------------------------------------------\n",
            "Step 11/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -9.965e+38 | Total Energy: 1.843e+66 | Computation Time: 0.034 seconds\n",
            "------------------------------------------------------------\n",
            "Step 12/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -9.135e+38 | Total Energy: 1.843e+66 | Computation Time: 0.026 seconds\n",
            "------------------------------------------------------------\n",
            "Step 13/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -8.432e+38 | Total Energy: 1.843e+66 | Computation Time: 0.025 seconds\n",
            "------------------------------------------------------------\n",
            "Step 14/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -7.830e+38 | Total Energy: 1.843e+66 | Computation Time: 0.029 seconds\n",
            "------------------------------------------------------------\n",
            "Step 15/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -7.308e+38 | Total Energy: 1.843e+66 | Computation Time: 0.025 seconds\n",
            "------------------------------------------------------------\n",
            "Step 16/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -6.851e+38 | Total Energy: 1.843e+66 | Computation Time: 0.026 seconds\n",
            "------------------------------------------------------------\n",
            "Step 17/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -6.448e+38 | Total Energy: 1.843e+66 | Computation Time: 0.028 seconds\n",
            "------------------------------------------------------------\n",
            "Step 18/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -6.090e+38 | Total Energy: 1.843e+66 | Computation Time: 0.026 seconds\n",
            "------------------------------------------------------------\n",
            "Step 19/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -5.769e+38 | Total Energy: 1.843e+66 | Computation Time: 0.025 seconds\n",
            "------------------------------------------------------------\n",
            "Step 20/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -5.481e+38 | Total Energy: 1.843e+66 | Computation Time: 0.025 seconds\n",
            "Positions of first 5 particles at step 20:\n",
            "  Particle 0: (-2275895683891985.50, 34903305327379308.00, -25498680415496528.00)\n",
            "  Particle 1: (-9029024633782250.00, -5375914183145494.00, 2858520748017577.00)\n",
            "  Particle 2: (12128020878821204.00, -5890857717826402.00, -1625408877601977.75)\n",
            "  Particle 3: (1116141132795465.75, -4670090408626843.00, 7580971381065888.00)\n",
            "  Particle 4: (-3591466869203960.00, 2196592458909726.75, 13771594283913244.00)\n",
            "------------------------------------------------------------\n",
            "Step 21/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -5.220e+38 | Total Energy: 1.843e+66 | Computation Time: 0.034 seconds\n",
            "------------------------------------------------------------\n",
            "Step 22/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -4.983e+38 | Total Energy: 1.843e+66 | Computation Time: 0.025 seconds\n",
            "------------------------------------------------------------\n",
            "Step 23/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -4.766e+38 | Total Energy: 1.843e+66 | Computation Time: 0.030 seconds\n",
            "------------------------------------------------------------\n",
            "Step 24/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -4.567e+38 | Total Energy: 1.843e+66 | Computation Time: 0.026 seconds\n",
            "------------------------------------------------------------\n",
            "Step 25/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -4.385e+38 | Total Energy: 1.843e+66 | Computation Time: 0.026 seconds\n",
            "------------------------------------------------------------\n",
            "Step 26/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -4.216e+38 | Total Energy: 1.843e+66 | Computation Time: 0.025 seconds\n",
            "------------------------------------------------------------\n",
            "Step 27/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -4.060e+38 | Total Energy: 1.843e+66 | Computation Time: 0.027 seconds\n",
            "------------------------------------------------------------\n",
            "Step 28/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -3.915e+38 | Total Energy: 1.843e+66 | Computation Time: 0.025 seconds\n",
            "------------------------------------------------------------\n",
            "Step 29/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -3.780e+38 | Total Energy: 1.843e+66 | Computation Time: 0.025 seconds\n",
            "------------------------------------------------------------\n",
            "Step 30/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -3.654e+38 | Total Energy: 1.843e+66 | Computation Time: 0.026 seconds\n",
            "Positions of first 5 particles at step 30:\n",
            "  Particle 0: (-3413843525838010.50, 52354957991068984.00, -38248020623244800.00)\n",
            "  Particle 1: (-13543536950673410.00, -8063871274718274.00, 4287781122026357.00)\n",
            "  Particle 2: (18192031318231804.00, -8836286576739642.00, -2438113316402991.00)\n",
            "  Particle 3: (1674211699193178.25, -7005135612940303.00, 11371457071598812.00)\n",
            "  Particle 4: (-5387200303805980.00, 3294888688364566.50, 20657391425869856.00)\n",
            "------------------------------------------------------------\n",
            "Step 31/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -3.536e+38 | Total Energy: 1.843e+66 | Computation Time: 0.026 seconds\n",
            "------------------------------------------------------------\n",
            "Step 32/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -3.426e+38 | Total Energy: 1.843e+66 | Computation Time: 0.026 seconds\n",
            "------------------------------------------------------------\n",
            "Step 33/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -3.322e+38 | Total Energy: 1.843e+66 | Computation Time: 0.026 seconds\n",
            "------------------------------------------------------------\n",
            "Step 34/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -3.224e+38 | Total Energy: 1.843e+66 | Computation Time: 0.025 seconds\n",
            "------------------------------------------------------------\n",
            "Step 35/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -3.132e+38 | Total Energy: 1.843e+66 | Computation Time: 0.026 seconds\n",
            "------------------------------------------------------------\n",
            "Step 36/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -3.045e+38 | Total Energy: 1.843e+66 | Computation Time: 0.026 seconds\n",
            "------------------------------------------------------------\n",
            "Step 37/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -2.963e+38 | Total Energy: 1.843e+66 | Computation Time: 0.025 seconds\n",
            "------------------------------------------------------------\n",
            "Step 38/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -2.885e+38 | Total Energy: 1.843e+66 | Computation Time: 0.026 seconds\n",
            "------------------------------------------------------------\n",
            "Step 39/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -2.811e+38 | Total Energy: 1.843e+66 | Computation Time: 0.030 seconds\n",
            "------------------------------------------------------------\n",
            "Step 40/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -2.740e+38 | Total Energy: 1.843e+66 | Computation Time: 0.025 seconds\n",
            "Positions of first 5 particles at step 40:\n",
            "  Particle 0: (-4551791367784035.00, 69806610654758664.00, -50997360830993040.00)\n",
            "  Particle 1: (-18058049267564572.00, -10751828366291054.00, 5717041496035137.00)\n",
            "  Particle 2: (24256041757642404.00, -11781715435652882.00, -3250817755204006.00)\n",
            "  Particle 3: (2232282265590890.75, -9340180817253762.00, 15161942762131732.00)\n",
            "  Particle 4: (-7182933738408000.00, 4393184917819406.50, 27543188567826456.00)\n",
            "------------------------------------------------------------\n",
            "Step 41/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -2.674e+38 | Total Energy: 1.843e+66 | Computation Time: 0.026 seconds\n",
            "------------------------------------------------------------\n",
            "Step 42/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -2.610e+38 | Total Energy: 1.843e+66 | Computation Time: 0.026 seconds\n",
            "------------------------------------------------------------\n",
            "Step 43/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -2.549e+38 | Total Energy: 1.843e+66 | Computation Time: 0.040 seconds\n",
            "------------------------------------------------------------\n",
            "Step 44/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -2.491e+38 | Total Energy: 1.843e+66 | Computation Time: 0.034 seconds\n",
            "------------------------------------------------------------\n",
            "Step 45/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -2.436e+38 | Total Energy: 1.843e+66 | Computation Time: 0.026 seconds\n",
            "------------------------------------------------------------\n",
            "Step 46/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -2.383e+38 | Total Energy: 1.843e+66 | Computation Time: 0.026 seconds\n",
            "------------------------------------------------------------\n",
            "Step 47/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -2.332e+38 | Total Energy: 1.843e+66 | Computation Time: 0.026 seconds\n",
            "------------------------------------------------------------\n",
            "Step 48/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -2.284e+38 | Total Energy: 1.843e+66 | Computation Time: 0.038 seconds\n",
            "------------------------------------------------------------\n",
            "Step 49/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -2.237e+38 | Total Energy: 1.843e+66 | Computation Time: 0.028 seconds\n",
            "------------------------------------------------------------\n",
            "Step 50/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -2.192e+38 | Total Energy: 1.843e+66 | Computation Time: 0.025 seconds\n",
            "Positions of first 5 particles at step 50:\n",
            "  Particle 0: (-5689739209730055.00, 87258263318448272.00, -63746701038741280.00)\n",
            "  Particle 1: (-22572561584455732.00, -13439785457863834.00, 7146301870043917.00)\n",
            "  Particle 2: (30320052197053004.00, -14727144294566122.00, -4063522194005021.00)\n",
            "  Particle 3: (2790352831988601.00, -11675226021567222.00, 18952428452664652.00)\n",
            "  Particle 4: (-8978667173010020.00, 5491481147274246.00, 34428985709783056.00)\n",
            "------------------------------------------------------------\n",
            "Step 51/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -2.149e+38 | Total Energy: 1.843e+66 | Computation Time: 0.026 seconds\n",
            "------------------------------------------------------------\n",
            "Step 52/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -2.108e+38 | Total Energy: 1.843e+66 | Computation Time: 0.025 seconds\n",
            "------------------------------------------------------------\n",
            "Step 53/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -2.068e+38 | Total Energy: 1.843e+66 | Computation Time: 0.026 seconds\n",
            "------------------------------------------------------------\n",
            "Step 54/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -2.030e+38 | Total Energy: 1.843e+66 | Computation Time: 0.028 seconds\n",
            "------------------------------------------------------------\n",
            "Step 55/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -1.993e+38 | Total Energy: 1.843e+66 | Computation Time: 0.026 seconds\n",
            "------------------------------------------------------------\n",
            "Step 56/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -1.957e+38 | Total Energy: 1.843e+66 | Computation Time: 0.025 seconds\n",
            "------------------------------------------------------------\n",
            "Step 57/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -1.923e+38 | Total Energy: 1.843e+66 | Computation Time: 0.025 seconds\n",
            "------------------------------------------------------------\n",
            "Step 58/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -1.890e+38 | Total Energy: 1.843e+66 | Computation Time: 0.026 seconds\n",
            "------------------------------------------------------------\n",
            "Step 59/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -1.858e+38 | Total Energy: 1.843e+66 | Computation Time: 0.025 seconds\n",
            "------------------------------------------------------------\n",
            "Step 60/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -1.827e+38 | Total Energy: 1.843e+66 | Computation Time: 0.026 seconds\n",
            "Positions of first 5 particles at step 60:\n",
            "  Particle 0: (-6827687051676075.00, 104709915982137872.00, -76496041246489552.00)\n",
            "  Particle 1: (-27087073901346892.00, -16127742549436614.00, 8575562244052697.00)\n",
            "  Particle 2: (36384062636463600.00, -17672573153479362.00, -4876226632806034.00)\n",
            "  Particle 3: (3348423398386311.00, -14010271225880682.00, 22742914143197572.00)\n",
            "  Particle 4: (-10774400607612040.00, 6589777376729086.00, 41314782851739688.00)\n",
            "------------------------------------------------------------\n",
            "Step 61/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -1.797e+38 | Total Energy: 1.843e+66 | Computation Time: 0.025 seconds\n",
            "------------------------------------------------------------\n",
            "Step 62/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -1.768e+38 | Total Energy: 1.843e+66 | Computation Time: 0.026 seconds\n",
            "------------------------------------------------------------\n",
            "Step 63/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -1.740e+38 | Total Energy: 1.843e+66 | Computation Time: 0.025 seconds\n",
            "------------------------------------------------------------\n",
            "Step 64/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -1.713e+38 | Total Energy: 1.843e+66 | Computation Time: 0.026 seconds\n",
            "------------------------------------------------------------\n",
            "Step 65/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -1.686e+38 | Total Energy: 1.843e+66 | Computation Time: 0.025 seconds\n",
            "------------------------------------------------------------\n",
            "Step 66/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -1.661e+38 | Total Energy: 1.843e+66 | Computation Time: 0.025 seconds\n",
            "------------------------------------------------------------\n",
            "Step 67/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -1.636e+38 | Total Energy: 1.843e+66 | Computation Time: 0.026 seconds\n",
            "------------------------------------------------------------\n",
            "Step 68/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -1.612e+38 | Total Energy: 1.843e+66 | Computation Time: 0.026 seconds\n",
            "------------------------------------------------------------\n",
            "Step 69/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -1.589e+38 | Total Energy: 1.843e+66 | Computation Time: 0.025 seconds\n",
            "------------------------------------------------------------\n",
            "Step 70/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -1.566e+38 | Total Energy: 1.843e+66 | Computation Time: 0.026 seconds\n",
            "Positions of first 5 particles at step 70:\n",
            "  Particle 0: (-7965634893622095.00, 122161568645827472.00, -89245381454237872.00)\n",
            "  Particle 1: (-31601586218238052.00, -18815699641009400.00, 10004822618061478.00)\n",
            "  Particle 2: (42448073075874160.00, -20618002012392600.00, -5688931071607044.00)\n",
            "  Particle 3: (3906493964784021.00, -16345316430194142.00, 26533399833730492.00)\n",
            "  Particle 4: (-12570134042214060.00, 7688073606183926.00, 48200579993696328.00)\n",
            "------------------------------------------------------------\n",
            "Step 71/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -1.544e+38 | Total Energy: 1.843e+66 | Computation Time: 0.025 seconds\n",
            "------------------------------------------------------------\n",
            "Step 72/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -1.522e+38 | Total Energy: 1.843e+66 | Computation Time: 0.025 seconds\n",
            "------------------------------------------------------------\n",
            "Step 73/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -1.502e+38 | Total Energy: 1.843e+66 | Computation Time: 0.025 seconds\n",
            "------------------------------------------------------------\n",
            "Step 74/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -1.481e+38 | Total Energy: 1.843e+66 | Computation Time: 0.025 seconds\n",
            "------------------------------------------------------------\n",
            "Step 75/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -1.462e+38 | Total Energy: 1.843e+66 | Computation Time: 0.025 seconds\n",
            "------------------------------------------------------------\n",
            "Step 76/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -1.442e+38 | Total Energy: 1.843e+66 | Computation Time: 0.030 seconds\n",
            "------------------------------------------------------------\n",
            "Step 77/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -1.424e+38 | Total Energy: 1.843e+66 | Computation Time: 0.033 seconds\n",
            "------------------------------------------------------------\n",
            "Step 78/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -1.405e+38 | Total Energy: 1.843e+66 | Computation Time: 0.027 seconds\n",
            "------------------------------------------------------------\n",
            "Step 79/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -1.388e+38 | Total Energy: 1.843e+66 | Computation Time: 0.025 seconds\n",
            "------------------------------------------------------------\n",
            "Step 80/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -1.370e+38 | Total Energy: 1.843e+66 | Computation Time: 0.026 seconds\n",
            "Positions of first 5 particles at step 80:\n",
            "  Particle 0: (-9103582735568116.00, 139613221309517072.00, -101994721661986192.00)\n",
            "  Particle 1: (-36116098535129216.00, -21503656732582200.00, 11434082992070258.00)\n",
            "  Particle 2: (48512083515284720.00, -23563430871305840.00, -6501635510408054.00)\n",
            "  Particle 3: (4464564531181731.00, -18680361634507596.00, 30323885524263412.00)\n",
            "  Particle 4: (-14365867476816080.00, 8786369835638766.00, 55086377135652968.00)\n",
            "------------------------------------------------------------\n",
            "Step 81/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -1.353e+38 | Total Energy: 1.843e+66 | Computation Time: 0.027 seconds\n",
            "------------------------------------------------------------\n",
            "Step 82/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -1.337e+38 | Total Energy: 1.843e+66 | Computation Time: 0.025 seconds\n",
            "------------------------------------------------------------\n",
            "Step 83/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -1.321e+38 | Total Energy: 1.843e+66 | Computation Time: 0.025 seconds\n",
            "------------------------------------------------------------\n",
            "Step 84/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -1.305e+38 | Total Energy: 1.843e+66 | Computation Time: 0.025 seconds\n",
            "------------------------------------------------------------\n",
            "Step 85/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -1.290e+38 | Total Energy: 1.843e+66 | Computation Time: 0.026 seconds\n",
            "------------------------------------------------------------\n",
            "Step 86/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -1.275e+38 | Total Energy: 1.843e+66 | Computation Time: 0.025 seconds\n",
            "------------------------------------------------------------\n",
            "Step 87/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -1.260e+38 | Total Energy: 1.843e+66 | Computation Time: 0.026 seconds\n",
            "------------------------------------------------------------\n",
            "Step 88/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -1.246e+38 | Total Energy: 1.843e+66 | Computation Time: 0.025 seconds\n",
            "------------------------------------------------------------\n",
            "Step 89/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -1.232e+38 | Total Energy: 1.843e+66 | Computation Time: 0.027 seconds\n",
            "------------------------------------------------------------\n",
            "Step 90/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -1.218e+38 | Total Energy: 1.843e+66 | Computation Time: 0.026 seconds\n",
            "Positions of first 5 particles at step 90:\n",
            "  Particle 0: (-10241530577514136.00, 157064873973206688.00, -114744061869734512.00)\n",
            "  Particle 1: (-40630610852020416.00, -24191613824155000.00, 12863343366079038.00)\n",
            "  Particle 2: (54576093954695280.00, -26508859730219080.00, -7314339949209064.00)\n",
            "  Particle 3: (5022635097579441.00, -21015406838821036.00, 34114371214796332.00)\n",
            "  Particle 4: (-16161600911418100.00, 9884666065093606.00, 61972174277609608.00)\n",
            "------------------------------------------------------------\n",
            "Step 91/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -1.205e+38 | Total Energy: 1.843e+66 | Computation Time: 0.025 seconds\n",
            "------------------------------------------------------------\n",
            "Step 92/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -1.192e+38 | Total Energy: 1.843e+66 | Computation Time: 0.026 seconds\n",
            "------------------------------------------------------------\n",
            "Step 93/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -1.179e+38 | Total Energy: 1.843e+66 | Computation Time: 0.027 seconds\n",
            "------------------------------------------------------------\n",
            "Step 94/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -1.166e+38 | Total Energy: 1.843e+66 | Computation Time: 0.025 seconds\n",
            "------------------------------------------------------------\n",
            "Step 95/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -1.154e+38 | Total Energy: 1.843e+66 | Computation Time: 0.026 seconds\n",
            "------------------------------------------------------------\n",
            "Step 96/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -1.142e+38 | Total Energy: 1.843e+66 | Computation Time: 0.025 seconds\n",
            "------------------------------------------------------------\n",
            "Step 97/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -1.130e+38 | Total Energy: 1.843e+66 | Computation Time: 0.026 seconds\n",
            "------------------------------------------------------------\n",
            "Step 98/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -1.119e+38 | Total Energy: 1.843e+66 | Computation Time: 0.025 seconds\n",
            "------------------------------------------------------------\n",
            "Step 99/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -1.107e+38 | Total Energy: 1.843e+66 | Computation Time: 0.025 seconds\n",
            "------------------------------------------------------------\n",
            "Step 100/100 | Kinetic Energy: 1.843e+66 | Potential Energy: -1.096e+38 | Total Energy: 1.843e+66 | Computation Time: 0.026 seconds\n",
            "Positions of first 5 particles at step 100:\n",
            "  Particle 0: (-11379478419460156.00, 174516526636896288.00, -127493402077482832.00)\n",
            "  Particle 1: (-45145123168911616.00, -26879570915727800.00, 14292603740087818.00)\n",
            "  Particle 2: (60640104394105840.00, -29454288589132320.00, -8127044388010074.00)\n",
            "  Particle 3: (5580705663977151.00, -23350452043134476.00, 37904856905329272.00)\n",
            "  Particle 4: (-17957334346020120.00, 10982962294548446.00, 68857971419566248.00)\n",
            "------------------------------------------------------------\n",
            "Simulation completed.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}