{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Hybrid MPI + OpenMP + CUDA Example: Solving Large System of Linear Equations\n",
        "\n",
        "In this notebook, we will explore a hybrid parallel programming example. The problem we're solving is a large system of linear equations. To do this, we'll use a combination of **MPI**, **OpenMP**, and **CUDA**.\n",
        "\n",
        "## Problem\n",
        "\n",
        "The system of linear equations can be represented in matrix form as:\n",
        "\n",
        "\\[ A \\times x = b \\]\n",
        "\n",
        "Where:\n",
        "- `A` is a matrix of coefficients,\n",
        "- `x` is the vector of unknowns,\n",
        "- `b` is the known vector of results.\n",
        "\n",
        "The goal is to parallelize this process across multiple **nodes** using **MPI**, within each node using **OpenMP**, and to accelerate matrix operations using **CUDA** on the GPU.\n",
        "\n",
        "### Steps:\n",
        "1. **Domain Decomposition with MPI**: Split the problem across multiple nodes.\n",
        "2. **Multi-threading with OpenMP**: Perform parallel matrix calculations within each node.\n",
        "3. **GPU Acceleration with CUDA**: Offload intensive matrix operations to the GPU.\n",
        "\n",
        "We will first initialize MPI, then allocate memory and perform calculations with OpenMP. CUDA will handle the heavy matrix multiplications.\n",
        "\n",
        "---\n",
        "\n",
        "### MPI Setup\n",
        "\n",
        "We start by initializing MPI, which will allow communication between different nodes. MPI will also handle splitting the work between different processors.\n"
      ],
      "metadata": {
        "id": "yL05idQkYA83"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install OpenMPI\n",
        "!apt-get update\n",
        "!apt-get install -y openmpi-bin openmpi-common libopenmpi-dev\n",
        "\n",
        "# Verify CUDA installation\n",
        "!nvcc --version\n",
        "\n",
        "# ** Locate MPI Headers and Libraries**\n",
        "# Find mpi.h\n",
        "mpi_h_paths = !find /usr -name mpi.h\n",
        "print(\"MPI Header Paths:\")\n",
        "for path in mpi_h_paths:\n",
        "    print(path)\n",
        "\n",
        "# Find libmpi.so\n",
        "mpi_lib_paths = !find /usr -name libmpi.so\n",
        "print(\"\\nMPI Library Paths:\")\n",
        "for path in mpi_lib_paths:\n",
        "    print(path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9IbWIXgaZpa",
        "outputId": "f6fc05e9-5f3d-4b1d-a169-9db2e2644cff"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:4 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Ign:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:8 https://r2u.stat.illinois.edu/ubuntu jammy Release\n",
            "Hit:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "52 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "nvidia-cuda-toolkit is already the newest version (11.5.1-1ubuntu1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 52 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "libopenmpi-dev is already the newest version (4.1.2-2ubuntu1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 52 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HLfrROk4WdG4",
        "outputId": "39bd6852-e6ab-41ca-f7ee-ab0a3502d4dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MPI Header Paths:\n",
            "/usr/src/linux-headers-5.15.0-124/include/linux/mpi.h\n",
            "/usr/lib/x86_64-linux-gnu/openmpi/include/mpi.h\n",
            "/usr/lib/x86_64-linux-gnu/fortran/gfortran-mod-15/openmpi/mpi.h\n",
            "\n",
            "MPI Library Paths:\n",
            "/usr/lib/x86_64-linux-gnu/libmpi.so\n",
            "/usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi.so\n",
            "Compilation Command:\n",
            "nvcc hybrid_mpi_openmp_cuda.cu -o cuda_mpi -Xcompiler \"-fopenmp -I/usr/lib/x86_64-linux-gnu/openmpi/include -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi\" -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi\n",
            "C[0] = 263.281668\n",
            "C[N*N-1] = 253.729415\n"
          ]
        }
      ],
      "source": [
        "# ** Write the Updated C Code**\n",
        "cuda_mpi_code = \"\"\"\n",
        "#include <mpi.h>\n",
        "#include <omp.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "// CUDA Kernel for matrix multiplication\n",
        "__global__ void gpu_matrix_mult(double *A, double *B, double *C, int N) {\n",
        "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if (row < N && col < N) {\n",
        "        double sum = 0;\n",
        "        for (int k = 0; k < N; ++k) {\n",
        "            sum += A[row * N + k] * B[k * N + col];\n",
        "        }\n",
        "        C[row * N + col] = sum;\n",
        "    }\n",
        "}\n",
        "\n",
        "int main(int argc, char* argv[]) {\n",
        "    // Initialize MPI\n",
        "    MPI_Init(&argc, &argv);\n",
        "\n",
        "    int world_size, rank;\n",
        "    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n",
        "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
        "\n",
        "    int N = 1024;\n",
        "    double *A, *B, *C;\n",
        "\n",
        "    // Allocate memory for matrices\n",
        "    A = (double*)malloc(N * N * sizeof(double));\n",
        "    B = (double*)malloc(N * N * sizeof(double));\n",
        "    C = (double*)malloc(N * N * sizeof(double));\n",
        "\n",
        "    // Initialize matrices in parallel with OpenMP\n",
        "    #pragma omp parallel for\n",
        "    for (int i = 0; i < N * N; i++) {\n",
        "        A[i] = rand() / (double)RAND_MAX;\n",
        "        B[i] = rand() / (double)RAND_MAX;\n",
        "    }\n",
        "\n",
        "    // Allocate device memory for CUDA\n",
        "    double *d_A, *d_B, *d_C;\n",
        "    cudaMalloc((void **)&d_A, N * N * sizeof(double));\n",
        "    cudaMalloc((void **)&d_B, N * N * sizeof(double));\n",
        "    cudaMalloc((void **)&d_C, N * N * sizeof(double));\n",
        "\n",
        "    // Copy data to GPU\n",
        "    cudaMemcpy(d_A, A, N * N * sizeof(double), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_B, B, N * N * sizeof(double), cudaMemcpyHostToDevice);\n",
        "\n",
        "    // CUDA kernel configuration\n",
        "    dim3 threadsPerBlock(16, 16);\n",
        "    dim3 blocksPerGrid((N + threadsPerBlock.x - 1) / threadsPerBlock.x,\n",
        "                       (N + threadsPerBlock.y - 1) / threadsPerBlock.y);\n",
        "\n",
        "    // Launch CUDA kernel for matrix multiplication\n",
        "    gpu_matrix_mult<<<blocksPerGrid, threadsPerBlock>>>(d_A, d_B, d_C, N);\n",
        "\n",
        "    // Wait for GPU to finish\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    // Copy result back to host\n",
        "    cudaMemcpy(C, d_C, N * N * sizeof(double), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // (Optional) Verify a few results\n",
        "    if(rank == 0) {\n",
        "        printf(\"C[0] = %f\\\\n\", C[0]);\n",
        "        printf(\"C[N*N-1] = %f\\\\n\", C[N*N-1]);\n",
        "    }\n",
        "\n",
        "    // Clean up\n",
        "    cudaFree(d_A);\n",
        "    cudaFree(d_B);\n",
        "    cudaFree(d_C);\n",
        "    free(A);\n",
        "    free(B);\n",
        "    free(C);\n",
        "\n",
        "    // Finalize MPI\n",
        "    MPI_Finalize();\n",
        "    return 0;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "with open(\"hybrid_mpi_openmp_cuda.cu\", \"w\") as cuda_file:\n",
        "    cuda_file.write(cuda_mpi_code)\n",
        "\n",
        "# **Cell 5: Compile the CUDA-MPI Program**\n",
        "# Automatically detect MPI include and library paths using mpicc\n",
        "# Get MPI compile flags (includes)\n",
        "mpi_cflags = !mpicc --showme:compile\n",
        "# Get MPI link flags (libraries)\n",
        "mpi_libs = !mpicc --showme:link\n",
        "\n",
        "# Combine the flags into strings\n",
        "mpi_cflags = \" \".join(mpi_cflags)\n",
        "mpi_libs = \" \".join(mpi_libs)\n",
        "\n",
        "# Compile the CUDA-MPI program with dynamic flags\n",
        "compile_command = f\"nvcc hybrid_mpi_openmp_cuda.cu -o cuda_mpi -Xcompiler \\\"-fopenmp {mpi_cflags}\\\" {mpi_libs}\"\n",
        "print(\"Compilation Command:\")\n",
        "print(compile_command)\n",
        "!{compile_command}\n",
        "\n",
        "# **Cell 6: Set Environment Variables for MPI**\n",
        "import os\n",
        "os.environ['OMPI_ALLOW_RUN_AS_ROOT'] = '1'\n",
        "os.environ['OMPI_ALLOW_RUN_AS_ROOT_CONFIRM'] = '1'\n",
        "\n",
        "# **Cell 7: Run the CUDA-MPI Program**\n",
        "!mpirun --allow-run-as-root --oversubscribe -np 2 ./cuda_mpi\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "### Markdown Explanation of the Provided Code\n",
        "\n",
        "Here's the markdown explanation for the code you've provided:\n",
        "\n",
        "```markdown\n",
        "# Explanation of the MPI-OpenMP-CUDA Code\n",
        "\n",
        "The code below is a hybrid MPI-OpenMP-CUDA program designed for distributed matrix multiplication using both CPU and GPU resources. Here's a breakdown of each part of the code:\n",
        "\n",
        "1. **CUDA Kernel for Matrix Multiplication**  \n",
        "   The `gpu_matrix_mult` function is a CUDA kernel that performs matrix multiplication on the GPU. It computes a single element of the result matrix `C` by multiplying the corresponding rows and columns of matrices `A` and `B`. The kernel uses 2D thread and block indices to map the computation to the correct element in the matrix.\n",
        "\n",
        "2. **MPI Initialization**  \n",
        "   The program starts by initializing MPI with `MPI_Init`, obtaining the total number of MPI processes (`world_size`) and the rank of the current process (`rank`). This is necessary to manage distributed execution across multiple nodes or processors.\n",
        "\n",
        "3. **Memory Allocation and Initialization**  \n",
        "   Matrices `A`, `B`, and `C` are allocated in the host (CPU) memory using `malloc`. These matrices are initialized with random values in parallel using OpenMP's `#pragma omp parallel for` directive. This parallelizes the initialization process to utilize multiple CPU cores.\n",
        "\n",
        "4. **CUDA Memory Allocation and Data Transfer**  \n",
        "   Memory for matrices `A`, `B`, and `C` is allocated on the GPU using `cudaMalloc`. The data for matrices `A` and `B` is then copied from the host memory to the GPU using `cudaMemcpy`.\n",
        "\n",
        "5. **Launching the CUDA Kernel**  \n",
        "   The CUDA kernel `gpu_matrix_mult` is launched to compute the result matrix `C` on the GPU. The grid and block dimensions are configured using `dim3 threadsPerBlock(16, 16)` and `dim3 blocksPerGrid`, which ensures that the entire matrix is processed in parallel by the GPU threads.\n",
        "\n",
        "6. **Synchronization and Data Transfer Back to Host**  \n",
        "   After the GPU finishes executing the kernel, the program synchronizes with `cudaDeviceSynchronize`. The result matrix `C` is copied back from the GPU to the host using `cudaMemcpy`.\n",
        "\n",
        "7. **Result Verification**  \n",
        "   For debugging purposes, the program prints two elements from the result matrix `C` (the first and last elements). This is done only by the MPI process with rank 0 to avoid duplicate outputs from multiple processes.\n",
        "\n",
        "8. **Cleanup**  \n",
        "   Once the computation is done, the allocated memory on both the host and device is freed using `free` and `cudaFree` respectively. MPI is finalized with `MPI_Finalize`.\n",
        "\n",
        "9. **Compilation and Execution**  \n",
        "   The program is compiled using `nvcc` with appropriate MPI and OpenMP flags. It is then executed using `mpirun` with multiple processes, utilizing both distributed computing (MPI) and parallelism (OpenMP and CUDA).\n",
        "\n",
        "By working through this code, you will learn how to effectively combine MPI for distributed execution, OpenMP for multi-threading on the CPU, and CUDA for GPU acceleration.\n"
      ],
      "metadata": {
        "id": "SFfrZepugCv5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise: Modifying the MPI-OpenMP-CUDA Code\n",
        "\n",
        "In this exercise, you will modify the existing MPI-OpenMP-CUDA hybrid code to further explore how distributed computing works in combination with GPU acceleration. Follow the steps below:\n",
        "\n",
        "1. **Task 1: Add Output for Verification**  \n",
        "   Modify the code to print more elements from the result matrix `C`. Currently, the code only prints `C[0]` and `C[N*N-1]`.  \n",
        "   - Add additional print statements to display elements like `C[N/2]` and `C[N*N/2]`. This will help verify that the matrix multiplication is computed correctly across more elements.\n",
        "\n",
        "2. **Task 2: Distribute Matrix Computation with MPI**  \n",
        "   Currently, all MPI processes are doing the same work. Modify the code so that each MPI process computes a different section of the matrix `C`.  \n",
        "   - For example, divide the matrix `C` into two parts, where the first MPI process computes the first half and the second MPI process computes the second half. This will involve adjusting the indices for each MPI rank.\n",
        "\n",
        "3. **Task 3: Experiment with OpenMP Threads**  \n",
        "   Adjust the number of OpenMP threads used in the code. Use the `OMP_NUM_THREADS` environment variable to experiment with different thread counts, and observe how it affects performance.\n",
        "\n",
        "Make sure to test your changes by running the program with different numbers of MPI processes and OpenMP threads. Discuss in your report how each modification affects the performance and correctness of the program.\n"
      ],
      "metadata": {
        "id": "AgGjTvhffc3B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution to the Exercise: Modifying the MPI-OpenMP-CUDA Code\n",
        "\n",
        "1. **Task 1: Add Output for Verification**\n",
        "\n",
        "   To print more elements from the result matrix `C`, you can modify the `if(rank == 0)` block to include additional print statements. Here's an example:\n",
        "\n",
        "   ```c\n",
        "   if(rank == 0) {\n",
        "       printf(\"C[0] = %f\\n\", C[0]);\n",
        "       printf(\"C[N/2] = %f\\n\", C[N/2]);\n",
        "       printf(\"C[N*N/2] = %f\\n\", C[N*N/2]);\n",
        "       printf(\"C[N*N-1] = %f\\n\", C[N*N-1]);\n",
        "   }\n"
      ],
      "metadata": {
        "id": "2pknd301f9eh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OpenACC MPI Hybrid Code\n",
        "\n",
        "This version of the code leverages **OpenACC** to offload matrix multiplication computations to a GPU, replacing the CUDA-specific implementation. OpenACC is designed to make parallel programming easier by allowing developers to write portable code that can run on CPUs and GPUs without needing to manage low-level GPU specifics like memory allocation or kernel launches. Here's a breakdown of the code:\n",
        "\n",
        "### 1. MPI Initialization\n",
        "The program initializes MPI with the following commands:\n",
        "```c\n",
        "MPI_Init(&argc, &argv);\n",
        "MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n",
        "MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
        "MPI_Init initializes the MPI environment, and MPI_Comm_size and MPI_Comm_rank retrieve the number of processes and the rank of the current process. This is important for distributed memory systems.\n",
        "2. Memory Allocation\n",
        "Memory for the matrices A, B, and C is dynamically allocated on the host (CPU):\n",
        "\n",
        "c\n",
        "Copy code\n",
        "A = (double*)malloc(N * N * sizeof(double));\n",
        "B = (double*)malloc(N * N * sizeof(double));\n",
        "C = (double*)malloc(N * N * sizeof(double));\n",
        "These matrices are later used for matrix multiplication, and OpenACC will handle their offloading to the GPU.\n",
        "\n",
        "3. Matrix Initialization\n",
        "The matrices are initialized using OpenMP on the CPU:\n",
        "\n",
        "c\n",
        "Copy code\n",
        "#pragma omp parallel for\n",
        "for (int i = 0; i < N * N; i++) {\n",
        "    A[i] = rand() / (double)RAND_MAX;\n",
        "    B[i] = rand() / (double)RAND_MAX;\n",
        "}\n",
        "This allows the initialization of the matrices to take advantage of multi-threading on the CPU before the computation is offloaded to the GPU.\n",
        "\n",
        "4. GPU Offloading with OpenACC\n",
        "The matrix multiplication is offloaded to the GPU using the OpenACC directive:\n",
        "\n",
        "c\n",
        "Copy code\n",
        "#pragma acc parallel loop collapse(2) copyin(A[0:N*N], B[0:N*N]) copyout(C[0:N*N])\n",
        "for (int row = 0; row < N; ++row) {\n",
        "    for (int col = 0; col < N; ++col) {\n",
        "        double sum = 0;\n",
        "        for (int i = 0; N; ++i) {\n",
        "            sum += A[row * N + i] * B[i * N + col];\n",
        "        }\n",
        "        C[row * N + col] = sum;\n",
        "    }\n",
        "}\n",
        "#pragma acc parallel loop: This directive tells the compiler to parallelize the loop and offload it to the GPU.\n",
        "collapse(2): Combines the two outer loops (row and col) into a single loop for better parallelization.\n",
        "copyin(A[0:N*N], B[0:N*N]): This copies the matrices A and B from the CPU memory to the GPU memory.\n",
        "copyout(C[0:N*N]): This copies the result matrix C from the GPU back to the CPU after the computation.\n",
        "5. Optional Verification\n",
        "The program prints the values of some elements in matrix C for verification. This is done by the process with rank 0:\n",
        "\n",
        "c\n",
        "Copy code\n",
        "if(rank == 0) {\n",
        "    printf(\"C[0] = %f\\n\", C[0]);\n",
        "    printf(\"C[N*N-1] = %f\\n\", C[N*N-1]);\n",
        "}\n",
        "6. Memory Cleanup\n",
        "After the matrix multiplication is complete, the dynamically allocated memory for the matrices is freed:\n",
        "\n",
        "c\n",
        "Copy code\n",
        "free(A); free(B); free(C);\n",
        "Finally, MPI is finalized with MPI_Finalize(), which terminates the MPI environment."
      ],
      "metadata": {
        "id": "SFWi6vWFjQeJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ** Write the  OpenACC-MPI Code**\n",
        "openacc_mpi_code = \"\"\"\n",
        "#include <mpi.h>\n",
        "#include <openacc.h>\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "// OpenACC kernel for matrix multiplication\n",
        "void gpu_matrix_mult(double *A, double *B, double *C, int N) {\n",
        "    #pragma acc parallel loop collapse(2) copyin(A[0:N*N], B[0:N*N]) copyout(C[0:N*N])\n",
        "    for (int row = 0; row < N; ++row) {\n",
        "        for (int col = 0; col < N; ++col) {\n",
        "            double sum = 0;\n",
        "            for (int i = 0; i < N; ++i) {\n",
        "                sum += A[row * N + i] * B[i * N + col];\n",
        "            }\n",
        "            C[row * N + col] = sum;\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "int main(int argc, char* argv[]) {\n",
        "    MPI_Init(&argc, &argv);\n",
        "\n",
        "    int world_size, rank;\n",
        "    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n",
        "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
        "\n",
        "    int N = 1024;\n",
        "    double *A, *B, *C;\n",
        "\n",
        "    A = (double*)malloc(N * N * sizeof(double));\n",
        "    B = (double*)malloc(N * N * sizeof(double));\n",
        "    C = (double*)malloc(N * N * sizeof(double));\n",
        "\n",
        "    #pragma omp parallel for\n",
        "    for (int i = 0; i < N * N; i++) {\n",
        "        A[i] = rand() / (double)RAND_MAX;\n",
        "        B[i] = rand() / (double)RAND_MAX;\n",
        "    }\n",
        "\n",
        "    gpu_matrix_mult(A, B, C, N);\n",
        "\n",
        "    if(rank == 0) {\n",
        "        printf(\"C[0] = %f\\\\n\", C[0]);\n",
        "        printf(\"C[N*N-1] = %f\\\\n\", C[N*N-1]);\n",
        "    }\n",
        "\n",
        "    free(A); free(B); free(C);\n",
        "\n",
        "    MPI_Finalize();\n",
        "    return 0;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Save the OpenACC-MPI code to a file\n",
        "with open(\"openacc_mpi.c\", \"w\") as openacc_file:\n",
        "    openacc_file.write(openacc_mpi_code)\n",
        "# Get MPI compile flags (includes)\n",
        "mpi_cflags = !mpicc --showme:compile\n",
        "# Get MPI link flags (libraries)\n",
        "mpi_libs = !mpicc --showme:link\n",
        "\n",
        "# Combine the flags into strings\n",
        "mpi_cflags = \" \".join(mpi_cflags)\n",
        "mpi_libs = \" \".join(mpi_libs)\n",
        "\n",
        "# Compile the OpenACC-MPI program with dynamic flags\n",
        "compile_command = f\"mpicc -fopenacc openacc_mpi.c -o openacc_mpi {mpi_cflags} {mpi_libs}\"\n",
        "print(\"Compilation Command:\")\n",
        "print(compile_command)\n",
        "!{compile_command}\n",
        "\n",
        "import os\n",
        "# Set the environment variables required by MPI\n",
        "os.environ['OMPI_ALLOW_RUN_AS_ROOT'] = '1'\n",
        "os.environ['OMPI_ALLOW_RUN_AS_ROOT_CONFIRM'] = '1'\n",
        "# Run the compiled OpenACC-MPI program\n",
        "!mpirun --allow-run-as-root --oversubscribe -np 2 ./openacc_mpi\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ZxlX9wmjRCy",
        "outputId": "bd0d9507-1734-4f9d-8fd7-36a7f4b5c74d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Compilation Command:\n",
            "mpicc -fopenacc openacc_mpi.c -o openacc_mpi -I/usr/lib/x86_64-linux-gnu/openmpi/include -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi\n",
            "C[0] = 257.255635\n",
            "C[N*N-1] = 256.381071\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OpenMP Offload Version of MPI-OpenMP Hybrid Code\n",
        "\n",
        "This modified version of the code uses **OpenMP Offload** instead of CUDA for GPU acceleration. OpenMP Offload allows you to offload computational work to an available GPU or accelerator using OpenMP directives. Here's a breakdown of the key changes:\n",
        "\n",
        "1. **OpenMP Offload for GPU**  \n",
        "   The `gpu_matrix_mult` function, which previously used a CUDA kernel, now uses OpenMP's offload features. The `#pragma omp target teams distribute parallel for` directive offloads the computation to the GPU:\n",
        "   ```cpp\n",
        "   #pragma omp target teams distribute parallel for collapse(2) map(to: A[0:N*N], B[0:N*N]) map(from: C[0:N*N])\n",
        "   for (int row = 0; row < N; ++row) {\n",
        "       for (int col = 0; col < N; ++col) {\n",
        "           double sum = 0;\n",
        "           for (int i = 0; i < N; ++i) {\n",
        "               sum += A[row * N + i] * B[i * N + col];\n",
        "           }\n",
        "           C[row * N + col] = sum;\n",
        "       }\n",
        "   }\n"
      ],
      "metadata": {
        "id": "lN4fAt4LgoVD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ** Write the Updated OpenMP Offload Code**\n",
        "openmp_mpi_code = \"\"\"\n",
        "#include <mpi.h>\n",
        "#include <omp.h>\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "// OpenMP Offload kernel for matrix multiplication\n",
        "void gpu_matrix_mult(double *A, double *B, double *C, int N) {\n",
        "    #pragma omp target teams distribute parallel for collapse(2) map(to: A[0:N*N], B[0:N*N]) map(from: C[0:N*N])\n",
        "    for (int row = 0; row < N; ++row) {\n",
        "        for (int col = 0; col < N; ++col) {\n",
        "            double sum = 0;\n",
        "            for (int i = 0; i < N; ++i) {\n",
        "                sum += A[row * N + i] * B[i * N + col];\n",
        "            }\n",
        "            C[row * N + col] = sum;\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "int main(int argc, char* argv[]) {\n",
        "    // Initialize MPI\n",
        "    MPI_Init(&argc, &argv);\n",
        "\n",
        "    int world_size, rank;\n",
        "    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n",
        "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
        "\n",
        "    int N = 1024;\n",
        "    double *A, *B, *C;\n",
        "\n",
        "    // Allocate memory for matrices\n",
        "    A = (double*)malloc(N * N * sizeof(double));\n",
        "    B = (double*)malloc(N * N * sizeof(double));\n",
        "    C = (double*)malloc(N * N * sizeof(double));\n",
        "\n",
        "    // Initialize matrices in parallel with OpenMP\n",
        "    #pragma omp parallel for\n",
        "    for (int i = 0; i < N * N; i++) {\n",
        "        A[i] = rand() / (double)RAND_MAX;\n",
        "        B[i] = rand() / (double)RAND_MAX;\n",
        "    }\n",
        "\n",
        "    // Offload matrix multiplication to GPU\n",
        "    gpu_matrix_mult(A, B, C, N);\n",
        "\n",
        "    // (Optional) Verify a few results\n",
        "    if(rank == 0) {\n",
        "        printf(\"C[0] = %f\\\\n\", C[0]);\n",
        "        printf(\"C[N*N-1] = %f\\\\n\", C[N*N-1]);\n",
        "    }\n",
        "\n",
        "    // Clean up\n",
        "    free(A);\n",
        "    free(B);\n",
        "    free(C);\n",
        "\n",
        "    // Finalize MPI\n",
        "    MPI_Finalize();\n",
        "    return 0;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Save the OpenMP Offload code to a file\n",
        "with open(\"openmp_mpi_offload.c\", \"w\") as openmp_file:\n",
        "    openmp_file.write(openmp_mpi_code)\n"
      ],
      "metadata": {
        "id": "dKDc_2kphGHl"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get MPI compile flags (includes)\n",
        "mpi_cflags = !mpicc --showme:compile\n",
        "# Get MPI link flags (libraries)\n",
        "mpi_libs = !mpicc --showme:link\n",
        "\n",
        "# Combine the flags into strings\n",
        "mpi_cflags = \" \".join(mpi_cflags)\n",
        "mpi_libs = \" \".join(mpi_libs)\n",
        "\n",
        "# Compile the OpenMP-MPI program with dynamic flags\n",
        "compile_command = f\"mpicc openmp_mpi_offload.c -o openmp_mpi_offload -fopenmp {mpi_cflags} {mpi_libs}\"\n",
        "print(\"Compilation Command:\")\n",
        "print(compile_command)\n",
        "!{compile_command}\n",
        "import os\n",
        "# Set the environment variables required by OpenMP and MPI\n",
        "os.environ['OMPI_ALLOW_RUN_AS_ROOT'] = '1'\n",
        "os.environ['OMPI_ALLOW_RUN_AS_ROOT_CONFIRM'] = '1'\n",
        "\n",
        "# Set the number of OpenMP threads (optional)\n",
        "os.environ['OMP_NUM_THREADS'] = '4'\n",
        "# Run the compiled OpenMP-MPI program\n",
        "!mpirun --allow-run-as-root --oversubscribe -np 2 ./openmp_mpi_offload\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AqhExTNXhRtN",
        "outputId": "0109acca-7677-46b2-db0e-ae943b730032"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Compilation Command:\n",
            "mpicc openmp_mpi_offload.c -o openmp_mpi_offload -fopenmp -I/usr/lib/x86_64-linux-gnu/openmpi/include -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi\n",
            "C[0] = 249.665562\n",
            "C[N*N-1] = 254.369895\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step-by-Step Guide: Converting a Serial Program into a Hybrid Application\n",
        "\n",
        "This section provides a practical guide on converting a serial 2D heat conduction simulation into a hybrid parallel application using MPI, OpenMP, and GPU acceleration. By breaking down the process step-by-step, this guide aims to help students understand how to apply hybrid computing techniques in real-world scenarios, optimizing performance and scalability.\n",
        "\n",
        "## Introduction to the Problem: Parallelizing a 2D Heat Conduction Simulation\n",
        "\n",
        "Simulating heat distribution over a two-dimensional plate is a fundamental problem in computational physics and engineering. It involves solving the heat equation—a partial differential equation that describes how heat diffuses through a given region over time. This problem has real-world applications in material science, thermodynamics, and mechanical engineering.\n",
        "\n",
        "### Problem Overview:\n",
        "Imagine a metal plate with fixed temperatures along its edges and an initial temperature distribution within. Over time, heat will flow from hotter regions to cooler ones until the plate reaches thermal equilibrium. Our goal is to model this process by calculating the temperature at each point on the plate at successive time steps.\n",
        "\n",
        "### Limitations of the Serial Implementation:\n",
        "- **Long Execution Time**: Large grid sizes and many time steps result in long execution times.\n",
        "- **Memory Constraints**: Large simulations may exceed the memory capacity of a single machine.\n",
        "- **Inefficiency for Real-World Applications**: Serial approaches are inefficient for real-time simulations or high-resolution grids.\n",
        "\n",
        "To overcome these limitations, we'll parallelize the simulation using a hybrid parallelization strategy involving MPI for distributed memory parallelism, OpenMP for multi-threading, and GPU acceleration for compute-intensive tasks.\n"
      ],
      "metadata": {
        "id": "QVieThYUmTAM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Serial Version: 2D Heat Conduction Simulation\n",
        "\n",
        "In this section, we provide the serial version of the 2D heat conduction simulation. This code calculates the temperature distribution over a 2D plate using the finite difference method. It runs on a single CPU core without any parallelization, and can be used as a baseline for comparing the performance improvements achieved with MPI, OpenMP, and CUDA in the parallel versions.\n",
        "\n",
        "## Serial Code in C\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mgfd_QJ-mvpV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explanation of the Serial 2D Heat Conduction Code\n",
        "\n",
        "This serial version of the 2D heat conduction simulation models the heat distribution over a plate using a finite difference method. The temperature at each point on the plate is updated iteratively based on the temperatures of its neighboring points.\n",
        "\n",
        "## Key Components of the Code:\n",
        "\n",
        "### 1. Grid Initialization (`initialize()` function):\n",
        "- The grid (or 2D array `temp`) represents the temperature at each point on the plate.\n",
        "- **Boundary Conditions**: The temperature at the edges of the grid is fixed at 100°C, representing a heated boundary. The interior of the grid is initialized to 0°C.\n",
        "    ```c\n",
        "    if (i == 0 || i == NY-1 || j == 0 || j == NX-1) {\n",
        "        temp[i][j] = 100.0; // Hot edges\n",
        "    }\n",
        "    ```\n",
        "\n",
        "### 2. Temperature Update (`update()` function):\n",
        "- The **finite difference method (FDM)** is used to compute the temperature at each interior grid point based on the temperatures of its neighbors.\n",
        "    ```c\n",
        "    temp_new[i][j] = temp[i][j] + alpha * (\n",
        "        (temp[i+1][j] - 2 * temp[i][j] + temp[i-1][j]) / (dx * dx) +\n",
        "        (temp[i][j+1] - 2 * temp[i][j] + temp[i][j-1]) / (dy * dy)\n",
        "    );\n",
        "    ```\n",
        "- The constant `alpha` represents the thermal diffusivity, and `dx` and `dy` represent the grid spacing.\n",
        "- **Time-stepping loop**: The temperature is updated for `NSTEPS` iterations, simulating the heat diffusion over time.\n",
        "\n",
        "### 3. Computational Limitations:\n",
        "- **Long Execution Time**: For large grids (e.g., `1000x1000`) and many time steps, this serial version can take a long time to run.\n",
        "- **Memory Usage**: A large grid requires significant memory, which could be a limitation for larger simulations.\n",
        "- **No Parallelism**: The serial version does not exploit multi-core CPUs or GPUs, making it inefficient for larger or more complex simulations.\n",
        "\n",
        "---\n",
        "\n",
        "## Next Steps:\n",
        "- The serial code provides a baseline for performance. We can measure the execution time of this version and compare it against parallel versions using MPI, OpenMP, and CUDA to see the performance gains from parallelism.\n"
      ],
      "metadata": {
        "id": "uoiy8AK6m0_s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This script performs the following:\n",
        "# 1. Writes and compiles a serial C program for 2D heat conduction.\n",
        "# 2. Writes and compiles an OpenMP-parallelized version of the same program.\n",
        "# 3. Runs both programs, captures their execution times.\n",
        "# 4. Compares the execution times and calculates speedup and efficiency.\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "import re\n",
        "\n",
        "# Function to write C code to a file\n",
        "def write_c_code(filename, code):\n",
        "    with open(filename, \"w\") as f:\n",
        "        f.write(code)\n",
        "\n",
        "# Function to compile C code\n",
        "def compile_c_code(source, output, flags=[]):\n",
        "    compile_command = [\"gcc\"] + flags + [source, \"-o\", output]\n",
        "    result = subprocess.run(compile_command, capture_output=True, text=True)\n",
        "    if result.returncode != 0:\n",
        "        print(f\"Error compiling {source}:\\n{result.stderr}\")\n",
        "        sys.exit(1)\n",
        "    else:\n",
        "        print(f\"Compiled {source} successfully.\")\n",
        "\n",
        "# Function to run executable and capture output\n",
        "def run_executable(exec_path):\n",
        "    result = subprocess.run([f\"./{exec_path}\"], capture_output=True, text=True)\n",
        "    if result.returncode != 0:\n",
        "        print(f\"Error running {exec_path}:\\n{result.stderr}\")\n",
        "        sys.exit(1)\n",
        "    else:\n",
        "        return result.stdout.strip()\n",
        "\n",
        "# 1. Write Serial C Code\n",
        "serial_code = \"\"\"\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <time.h>\n",
        "\n",
        "#define NX 3000    // Number of grid points in X-direction\n",
        "#define NY 3000    // Number of grid points in Y-direction\n",
        "#define NSTEPS 500 // Number of time steps\n",
        "\n",
        "double temp[NY][NX];\n",
        "double temp_new[NY][NX];\n",
        "\n",
        "void initialize() {\n",
        "    for (int i = 0; i < NY; i++) {\n",
        "        for (int j = 0; j < NX; j++) {\n",
        "            temp[i][j] = 0.0; // Initial temperature\n",
        "            // Set boundary conditions\n",
        "            if (i == 0 || i == NY - 1 || j == 0 || j == NX - 1) {\n",
        "                temp[i][j] = 100.0; // Hot edges\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "void update() {\n",
        "    double alpha = 0.01; // Thermal diffusivity\n",
        "    double dx = 1.0;\n",
        "    double dy = 1.0;\n",
        "\n",
        "    for (int step = 0; step < NSTEPS; step++) {\n",
        "        for (int i = 1; i < NY - 1; i++) {\n",
        "            for (int j = 1; j < NX - 1; j++) {\n",
        "                temp_new[i][j] = temp[i][j] + alpha * (\n",
        "                    (temp[i+1][j] - 2 * temp[i][j] + temp[i-1][j]) / (dx * dx) +\n",
        "                    (temp[i][j+1] - 2 * temp[i][j] + temp[i][j-1]) / (dy * dy)\n",
        "                );\n",
        "            }\n",
        "        }\n",
        "        // Copy temp_new to temp for the next iteration\n",
        "        for (int i = 1; i < NY - 1; i++) {\n",
        "            for (int j = 1; j < NX - 1; j++) {\n",
        "                temp[i][j] = temp_new[i][j];\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    clock_t start, end;\n",
        "    double cpu_time_used;\n",
        "\n",
        "    initialize();\n",
        "\n",
        "    start = clock();\n",
        "    update();\n",
        "    end = clock();\n",
        "\n",
        "    cpu_time_used = ((double) (end - start)) / CLOCKS_PER_SEC;\n",
        "    printf(\"Serial Execution Time: %f seconds\\\\n\", cpu_time_used);\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "serial_filename = \"heat_serial.c\"\n",
        "write_c_code(serial_filename, serial_code)\n",
        "print(\"Serial C code written to heat_serial.c\")\n",
        "\n",
        "# 2. Compile Serial Code\n",
        "compile_c_code(serial_filename, \"heat_serial\")\n",
        "print()\n",
        "\n",
        "# 3. Run Serial Executable\n",
        "print(\"Running Serial Executable...\")\n",
        "serial_output = run_executable(\"heat_serial\")\n",
        "print(serial_output)\n",
        "print()\n",
        "\n",
        "# Extract Serial Execution Time using regex\n",
        "serial_time_match = re.search(r\"Serial Execution Time:\\s+([0-9.]+)\\s+seconds\", serial_output)\n",
        "if serial_time_match:\n",
        "    serial_time = float(serial_time_match.group(1))\n",
        "else:\n",
        "    print(\"Failed to extract serial execution time.\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# 4. Write OpenMP C Code\n",
        "openmp_code = \"\"\"\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <time.h>\n",
        "#include <omp.h>\n",
        "\n",
        "#define NX 1000    // Number of grid points in X-direction\n",
        "#define NY 1000    // Number of grid points in Y-direction\n",
        "#define NSTEPS 500 // Number of time steps\n",
        "\n",
        "double temp[NY][NX];\n",
        "double temp_new[NY][NX];\n",
        "\n",
        "void initialize() {\n",
        "    #pragma omp parallel for collapse(2)\n",
        "    for (int i = 0; i < NY; i++) {\n",
        "        for (int j = 0; j < NX; j++) {\n",
        "            temp[i][j] = 0.0; // Initial temperature\n",
        "            // Set boundary conditions\n",
        "            if (i == 0 || i == NY - 1 || j == 0 || j == NX - 1) {\n",
        "                temp[i][j] = 100.0; // Hot edges\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "void update() {\n",
        "    double alpha = 0.01; // Thermal diffusivity\n",
        "    double dx = 1.0;\n",
        "    double dy = 1.0;\n",
        "\n",
        "    for (int step = 0; step < NSTEPS; step++) {\n",
        "        #pragma omp parallel for collapse(2)\n",
        "        for (int i = 1; i < NY - 1; i++) {\n",
        "            for (int j = 1; j < NX - 1; j++) {\n",
        "                temp_new[i][j] = temp[i][j] + alpha * (\n",
        "                    (temp[i+1][j] - 2 * temp[i][j] + temp[i-1][j]) / (dx * dx) +\n",
        "                    (temp[i][j+1] - 2 * temp[i][j] + temp[i][j-1]) / (dy * dy)\n",
        "                );\n",
        "            }\n",
        "        }\n",
        "        // Copy temp_new to temp for the next iteration\n",
        "        #pragma omp parallel for collapse(2)\n",
        "        for (int i = 1; i < NY - 1; i++) {\n",
        "            for (int j = 1; j < NX - 1; j++) {\n",
        "                temp[i][j] = temp_new[i][j];\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    clock_t start, end;\n",
        "    double cpu_time_used;\n",
        "    int num_threads = 2; // You can adjust the number of threads\n",
        "\n",
        "    omp_set_num_threads(num_threads);\n",
        "\n",
        "    initialize();\n",
        "\n",
        "    start = clock();\n",
        "    update();\n",
        "    end = clock();\n",
        "\n",
        "    cpu_time_used = ((double) (end - start)) / CLOCKS_PER_SEC;\n",
        "    printf(\"OpenMP Execution Time with %d threads: %f seconds\\\\n\", num_threads, cpu_time_used);\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "openmp_filename = \"heat_openmp.c\"\n",
        "write_c_code(openmp_filename, openmp_code)\n",
        "print(\"OpenMP C code written to heat_openmp.c\")\n",
        "\n",
        "# 5. Compile OpenMP Code\n",
        "compile_c_code(openmp_filename, \"heat_openmp\", flags=[\"-fopenmp\"])\n",
        "print()\n",
        "\n",
        "# 6. Run OpenMP Executable\n",
        "print(\"Running OpenMP Executable...\")\n",
        "openmp_output = run_executable(\"heat_openmp\")\n",
        "print(openmp_output)\n",
        "print()\n",
        "\n",
        "# Extract OpenMP Execution Time and Number of Threads using regex\n",
        "openmp_match = re.search(r\"OpenMP Execution Time with (\\d+) threads:\\s+([0-9.]+)\\s+seconds\", openmp_output)\n",
        "if openmp_match:\n",
        "    openmp_threads = int(openmp_match.group(1))\n",
        "    openmp_time = float(openmp_match.group(2))\n",
        "else:\n",
        "    print(\"Failed to extract OpenMP execution time or number of threads.\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# 7. Compare and Print Results\n",
        "print(\"--- Execution Time Comparison ---\")\n",
        "print(f\"Serial Execution Time: {serial_time:.6f} seconds\")\n",
        "print(f\"OpenMP Execution Time with {openmp_threads} threads: {openmp_time:.6f} seconds\")\n",
        "\n",
        "# Calculate Speedup and Efficiency\n",
        "speedup = serial_time / openmp_time if openmp_time > 0 else float('inf')\n",
        "efficiency = (speedup / openmp_threads) * 100 if openmp_threads > 0 else 0\n",
        "\n",
        "print(f\"Speedup: {speedup:.2f}x\")\n",
        "print(f\"Efficiency: {efficiency:.2f}%\")\n",
        "print()\n",
        "\n",
        "# 8. Analyze Performance Discrepancy\n",
        "if openmp_time > serial_time:\n",
        "    print(\"**Observation:** The OpenMP version is slower than the serial version.\")\n",
        "    print(\"**Possible Reasons:**\")\n",
        "    print(\"- Overhead from thread creation and synchronization.\")\n",
        "    print(\"- Limited number of physical CPU cores in the Colab environment.\")\n",
        "    print(\"- Inefficient parallelization or cache contention.\")\n",
        "    print(\"- The problem size may not be large enough to benefit from parallelization.\")\n",
        "    print(\"- OpenMP directives might not be optimally placed.\")\n",
        "    print()\n",
        "    print(\"**Recommendations:**\")\n",
        "    print(\"- Increase the problem size (e.g., larger grid or more time steps) to better utilize parallelism.\")\n",
        "    print(\"- Experiment with different numbers of threads to find the optimal count.\")\n",
        "    print(\"- Optimize OpenMP directives, such as using appropriate scheduling strategies.\")\n",
        "    print(\"- Profile the code to identify and address bottlenecks.\")\n",
        "else:\n",
        "    print(\"**Observation:** The OpenMP version is faster than the serial version.\")\n",
        "    print(\"**Performance Benefits Achieved Through OpenMP Parallelization.**\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HN-QlfzCpW9R",
        "outputId": "025100f0-d202-4313-9f16-e5074359ed6f"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Serial C code written to heat_serial.c\n",
            "Compiled heat_serial.c successfully.\n",
            "\n",
            "Running Serial Executable...\n",
            "Serial Execution Time: 64.020981 seconds\n",
            "\n",
            "OpenMP C code written to heat_openmp.c\n",
            "Compiled heat_openmp.c successfully.\n",
            "\n",
            "Running OpenMP Executable...\n",
            "OpenMP Execution Time with 2 threads: 12.964147 seconds\n",
            "\n",
            "--- Execution Time Comparison ---\n",
            "Serial Execution Time: 64.020981 seconds\n",
            "OpenMP Execution Time with 2 threads: 12.964147 seconds\n",
            "Speedup: 4.94x\n",
            "Efficiency: 246.92%\n",
            "\n",
            "**Observation:** The OpenMP version is faster than the serial version.\n",
            "**Performance Benefits Achieved Through OpenMP Parallelization.**\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Explanation: Parallelizing 2D Heat Conduction with MPI, OpenMP, and CUDA\n",
        "\n",
        "## 1. **MPI for Distributed Memory Parallelism**\n",
        "- **Domain Decomposition**: The 2D grid is split across MPI processes. Each process manages a subset of rows (`local_NY`) to parallelize computation.\n",
        "- **MPI Initialization**:\n",
        "    ```c\n",
        "    MPI_Init(&argc, &argv);\n",
        "    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n",
        "    MPI_Comm_size(MPI_COMM_WORLD, &size);\n",
        "    ```\n",
        "    These functions initialize the MPI environment and determine the rank and number of processes.\n",
        "  \n",
        "- **Data Partitioning**: Each process works on a portion of the grid. The grid is divided along the Y-dimension, and each process computes a subset of rows.\n",
        "\n",
        "## 2. **OpenMP for Intra-node Parallelism**\n",
        "- **OpenMP Directives**: OpenMP is used within each MPI process for initializing the grid and performing the update in parallel. OpenMP ensures that loops over grid points are distributed across multiple CPU threads.\n",
        "\n",
        "## 3. **CUDA for GPU Acceleration**\n",
        "- **GPU Offloading**: The temperature update step is offloaded to the GPU using a CUDA kernel. The kernel computes the new temperatures in parallel across grid points.\n",
        "    ```c\n",
        "    __global__ void update_kernel(double *temp, double *temp_new, int NX, int NY, double alpha);\n",
        "    ```\n",
        "\n",
        "- **Kernel Execution**: The kernel is launched with a 2D grid and block configuration using:\n",
        "    ```c\n",
        "    dim3 blockDim(16, 16);\n",
        "    dim3 gridDim((NX + blockDim.x - 1) / blockDim.x, (local_NY + blockDim.y - 1) / blockDim.y);\n",
        "    update_kernel<<<gridDim, blockDim>>>(d_temp, d_temp_new, NX, local_NY, alpha);\n",
        "    ```\n",
        "\n",
        "## 4. **MPI Communication**\n",
        "- Each process exchanges boundary rows with neighboring processes to ensure the boundary conditions are correctly handled across the grid. In this simplified version, we do not show the MPI communication for boundary exchange, but it can be added with `MPI_Isend` and `MPI_Irecv` for non-blocking communication.\n",
        "\n",
        "## 5. **Hybrid Approach**\n",
        "- The combination of **MPI for inter-node communication**, **OpenMP for intra-node parallelism**, and **CUDA for GPU acceleration** ensures efficient utilization of hardware resources, enabling scalable performance for large simulations.\n",
        "\n",
        "### Key Concepts:\n",
        "- **Distributed Memory Parallelism**: MPI splits the grid across multiple nodes, reducing the memory load on a single machine.\n",
        "- **Shared Memory Parallelism**: OpenMP ensures that CPU cores within a node work together efficiently.\n",
        "- **GPU Acceleration**: CUDA accelerates the most compute-intensive parts of the simulation, providing massive parallelism.\n"
      ],
      "metadata": {
        "id": "CLHOhfvmmjDj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Installs MPI and verifies CUDA installation.\n",
        "# ------------------------------\n",
        "# 1. Install MPI and Verify CUDA\n",
        "# ------------------------------\n",
        "print(\"Installing MPI...\")\n",
        "# Update package lists and install MPI\n",
        "!apt-get update -y\n",
        "!apt-get install -y mpi-default-bin mpi-default-dev\n",
        "\n",
        "print(\"\\nVerifying MPI installation:\")\n",
        "# Check MPI compiler version\n",
        "!mpicc --version\n",
        "\n",
        "print(\"\\nVerifying CUDA installation:\")\n",
        "# Check CUDA compiler version\n",
        "!nvcc --version"
      ],
      "metadata": {
        "id": "IurmxT-rtn3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ### **2D Heat Conduction Simulation: Serial, OpenMP + MPI, and OpenMP + MPI + CUDA Versions**\n",
        "\n",
        "# This script performs the following:\n",
        "\n",
        "# 2. Writes and compiles the **Serial** C program with a grid size of 3000x3000.\n",
        "# 3. Writes and compiles the **OpenMP + MPI** C program with a grid size of 3000x3000.\n",
        "# 4. Writes and compiles the **OpenMP + MPI + CUDA** C program with a grid size of 3000x3000.\n",
        "# 5. Executes the Serial and OpenMP + MPI programs.\n",
        "# 6. Captures and displays their execution times.\n",
        "# 7. Attempts to compile the OpenMP + MPI + CUDA program (execution is skipped due to Colab limitations).\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "import re\n",
        "\n",
        "# Function to write C/CUDA code to a file\n",
        "def write_code(filename, code):\n",
        "    with open(filename, \"w\") as f:\n",
        "        f.write(code)\n",
        "    print(f\"Code written to {filename}\")\n",
        "\n",
        "# Function to compile C code\n",
        "def compile_c(source, output, flags=[]):\n",
        "    compile_command = [\"gcc\"] + flags + [source, \"-o\", output]\n",
        "    result = subprocess.run(compile_command, capture_output=True, text=True)\n",
        "    if result.returncode != 0:\n",
        "        print(f\"Error compiling {source}:\\n{result.stderr}\")\n",
        "        return False\n",
        "    else:\n",
        "        print(f\"Compiled {source} successfully.\")\n",
        "        return True\n",
        "\n",
        "# Function to compile MPI + OpenMP C code\n",
        "def compile_mpi_openmp(source, output, flags=[]):\n",
        "    compile_command = [\"mpicc\"] + flags + [source, \"-o\", output]\n",
        "    result = subprocess.run(compile_command, capture_output=True, text=True)\n",
        "    if result.returncode != 0:\n",
        "        print(f\"Error compiling {source}:\\n{result.stderr}\")\n",
        "        return False\n",
        "    else:\n",
        "        print(f\"Compiled {source} successfully.\")\n",
        "        return True\n",
        "\n",
        "# Function to compile MPI + OpenMP + CUDA C code\n",
        "def compile_mpi_openmp_cuda(source, output, flags=[]):\n",
        "    # Locate mpi.h\n",
        "    mpi_include = \"/usr/include/openmpi\"  # Common path for OpenMPI\n",
        "    mpi_lib = \"/usr/lib/x86_64-linux-gnu\"  # Common path for OpenMPI libraries\n",
        "\n",
        "    # Check if mpi.h exists in the expected directory\n",
        "    if not os.path.exists(f\"{mpi_include}/mpi.h\"):\n",
        "        # Attempt to find mpi.h\n",
        "        mpi_h_path = subprocess.getoutput(\"find /usr/include -name mpi.h\")\n",
        "        if mpi_h_path:\n",
        "            mpi_include = os.path.dirname(mpi_h_path)\n",
        "            print(f\"Found mpi.h at {mpi_h_path}\")\n",
        "        else:\n",
        "            print(\"mpi.h not found. Cannot compile CUDA program with MPI.\")\n",
        "            return False\n",
        "\n",
        "    # Compile using nvcc with appropriate include and library paths\n",
        "    compile_command = [\n",
        "        \"nvcc\",\n",
        "        \"-Xcompiler\", \"-fopenmp\",  # Pass OpenMP flag to host compiler\n",
        "        source,\n",
        "        \"-o\",\n",
        "        output,\n",
        "        f\"-I{mpi_include}\",          # Include path for MPI headers\n",
        "        f\"-L{mpi_lib}\",              # Library path for MPI\n",
        "        \"-lmpi\"                       # Link against MPI library\n",
        "    ]\n",
        "    result = subprocess.run(compile_command, capture_output=True, text=True)\n",
        "    if result.returncode != 0:\n",
        "        print(f\"Error compiling {source}:\\n{result.stderr}\")\n",
        "        return False\n",
        "    else:\n",
        "        print(f\"Compiled {source} successfully.\")\n",
        "        return True\n",
        "\n",
        "# Function to run an executable and capture output\n",
        "def run_executable(exec_path):\n",
        "    try:\n",
        "        result = subprocess.run([f\"./{exec_path}\"], capture_output=True, text=True, check=True)\n",
        "        return result.stdout.strip()\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Error running {exec_path}:\\n{e.output}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "\n",
        "# ------------------------------\n",
        "# 2. Serial C Implementation\n",
        "# ------------------------------\n",
        "serial_code = \"\"\"\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <time.h>\n",
        "\n",
        "#define NX 3000    // Number of grid points in X-direction\n",
        "#define NY 3000    // Number of grid points in Y-direction\n",
        "#define NSTEPS 500 // Number of time steps\n",
        "\n",
        "double temp[NY][NX];\n",
        "double temp_new[NY][NX];\n",
        "\n",
        "void initialize() {\n",
        "    for (int i = 0; i < NY; i++) {\n",
        "        for (int j = 0; j < NX; j++) {\n",
        "            temp[i][j] = 0.0; // Initial temperature\n",
        "            // Set boundary conditions\n",
        "            if (i == 0 || i == NY - 1 || j == 0 || j == NX - 1) {\n",
        "                temp[i][j] = 100.0; // Hot edges\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "void update() {\n",
        "    double alpha = 0.01; // Thermal diffusivity\n",
        "    double dx = 1.0;\n",
        "    double dy = 1.0;\n",
        "\n",
        "    for (int step = 0; step < NSTEPS; step++) {\n",
        "        for (int i = 1; i < NY - 1; i++) {\n",
        "            for (int j = 1; j < NX - 1; j++) {\n",
        "                temp_new[i][j] = temp[i][j] + alpha * (\n",
        "                    (temp[i+1][j] - 2 * temp[i][j] + temp[i-1][j]) / (dx * dx) +\n",
        "                    (temp[i][j+1] - 2 * temp[i][j] + temp[i][j-1]) / (dy * dy)\n",
        "                );\n",
        "            }\n",
        "        }\n",
        "        // Copy temp_new to temp for the next iteration\n",
        "        for (int i = 1; i < NY - 1; i++) {\n",
        "            for (int j = 1; j < NX - 1; j++) {\n",
        "                temp[i][j] = temp_new[i][j];\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    clock_t start, end;\n",
        "    double cpu_time_used;\n",
        "\n",
        "    initialize();\n",
        "\n",
        "    start = clock();\n",
        "    update();\n",
        "    end = clock();\n",
        "\n",
        "    cpu_time_used = ((double) (end - start)) / CLOCKS_PER_SEC;\n",
        "    printf(\"Serial Execution Time: %f seconds\\\\n\", cpu_time_used);\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Write Serial C Code\n",
        "write_code(\"heat_serial.c\", serial_code)\n",
        "\n",
        "# Compile Serial C Code\n",
        "if compile_c(\"heat_serial.c\", \"heat_serial\"):\n",
        "    # Run Serial Executable\n",
        "    print(\"\\nRunning Serial Executable...\")\n",
        "    serial_output = run_executable(\"heat_serial\")\n",
        "    if serial_output:\n",
        "        print(serial_output)\n",
        "        # Extract Serial Execution Time\n",
        "        serial_time_match = re.search(r\"Serial Execution Time:\\s+([0-9.]+)\\s+seconds\", serial_output)\n",
        "        if serial_time_match:\n",
        "            serial_time = float(serial_time_match.group(1))\n",
        "            print(f\"Serial Execution Time: {serial_time:.6f} seconds\\n\")\n",
        "        else:\n",
        "            print(\"Failed to extract serial execution time.\\n\")\n",
        "    else:\n",
        "        print(\"Serial executable did not run successfully.\\n\")\n",
        "else:\n",
        "    print(\"Serial compilation failed.\\n\")\n",
        "\n",
        "# ------------------------------\n",
        "# 3. OpenMP + MPI C Implementation\n",
        "# ------------------------------\n",
        "openmp_mpi_code = \"\"\"\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <time.h>\n",
        "#include <mpi.h>\n",
        "#include <omp.h>\n",
        "\n",
        "#define NX 3000    // Number of grid points in X-direction\n",
        "#define NY 3000    // Number of grid points in Y-direction\n",
        "#define NSTEPS 500 // Number of time steps\n",
        "\n",
        "double **temp;\n",
        "double **temp_new;\n",
        "\n",
        "// Function to allocate a 2D array dynamically\n",
        "double** allocate_2D(int rows, int cols) {\n",
        "    double **array = (double**) malloc(rows * sizeof(double*));\n",
        "    for(int i = 0; i < rows; i++) {\n",
        "        array[i] = (double*) malloc(cols * sizeof(double));\n",
        "    }\n",
        "    return array;\n",
        "}\n",
        "\n",
        "// Function to free a 2D array\n",
        "void free_2D(double **array, int rows) {\n",
        "    for(int i = 0; i < rows; i++) {\n",
        "        free(array[i]);\n",
        "    }\n",
        "    free(array);\n",
        "}\n",
        "\n",
        "// Initialize the temperature grid\n",
        "void initialize(int rank, int size, int local_NY, int start_row) {\n",
        "    #pragma omp parallel for collapse(2)\n",
        "    for(int i = 1; i <= local_NY; i++) { // 1 to local_NY inclusive\n",
        "        for(int j = 0; j < NX; j++) {\n",
        "            temp[i][j] = 0.0; // Initial temperature\n",
        "            // Set boundary conditions\n",
        "            if ((start_row + i -1 == 0) || (start_row + i -1 == NY -1) || j == 0 || j == NX -1) {\n",
        "                temp[i][j] = 100.0; // Hot edges\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "// Exchange ghost rows with neighboring MPI processes\n",
        "void exchange_ghost_rows(int rank, int size, int local_NY, MPI_Comm comm) {\n",
        "    MPI_Request requests[4];\n",
        "    int req_count = 0;\n",
        "\n",
        "    // Send to upper neighbor, receive from lower neighbor\n",
        "    if(rank != 0){\n",
        "        MPI_Isend(temp[1], NX, MPI_DOUBLE, rank -1, 0, comm, &requests[req_count++]);\n",
        "        MPI_Irecv(temp[0], NX, MPI_DOUBLE, rank -1, 1, comm, &requests[req_count++]);\n",
        "    }\n",
        "\n",
        "    // Send to lower neighbor, receive from upper neighbor\n",
        "    if(rank != size -1){\n",
        "        MPI_Isend(temp[local_NY], NX, MPI_DOUBLE, rank +1, 1, comm, &requests[req_count++]);\n",
        "        MPI_Irecv(temp[local_NY +1], NX, MPI_DOUBLE, rank +1, 0, comm, &requests[req_count++]);\n",
        "    }\n",
        "\n",
        "    MPI_Waitall(req_count, requests, MPI_STATUSES_IGNORE);\n",
        "}\n",
        "\n",
        "// Update the temperature grid\n",
        "void update(int local_NY) {\n",
        "    double alpha = 0.01; // Thermal diffusivity\n",
        "    double dx = 1.0;\n",
        "    double dy = 1.0;\n",
        "\n",
        "    #pragma omp parallel for collapse(2)\n",
        "    for(int i =1; i <= local_NY; i++) {\n",
        "        for(int j =1; j < NX -1; j++) {\n",
        "            temp_new[i][j] = temp[i][j] + alpha * (\n",
        "                (temp[i+1][j] - 2 * temp[i][j] + temp[i-1][j]) / (dx * dx) +\n",
        "                (temp[i][j+1] - 2 * temp[i][j] + temp[i][j-1]) / (dy * dy)\n",
        "            );\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // Swap temp and temp_new\n",
        "    #pragma omp parallel for collapse(2)\n",
        "    for(int i =1; i <= local_NY; i++) {\n",
        "        for(int j =1; j < NX -1; j++) {\n",
        "            temp[i][j] = temp_new[i][j];\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "int main(int argc, char *argv[]) {\n",
        "    int rank, size;\n",
        "    double start_time, end_time;\n",
        "    int local_NY, start_row;\n",
        "    MPI_Init(&argc, &argv);\n",
        "    MPI_Comm comm = MPI_COMM_WORLD;\n",
        "    MPI_Comm_rank(comm, &rank);\n",
        "    MPI_Comm_size(comm, &size);\n",
        "\n",
        "    // Determine the number of rows per process\n",
        "    local_NY = NY / size;\n",
        "    int remainder = NY % size;\n",
        "    start_row = rank * local_NY + (rank < remainder ? rank : remainder);\n",
        "    local_NY += (rank < remainder) ? 1 : 0;\n",
        "\n",
        "    // Allocate local arrays with ghost rows\n",
        "    temp = allocate_2D(local_NY +2, NX);      // +2 for ghost rows\n",
        "    temp_new = allocate_2D(local_NY +2, NX);\n",
        "\n",
        "    // Initialize local grid\n",
        "    initialize(rank, size, local_NY, start_row);\n",
        "\n",
        "    MPI_Barrier(comm); // Synchronize before starting the timer\n",
        "    if(rank ==0){\n",
        "        start_time = MPI_Wtime();\n",
        "    }\n",
        "\n",
        "    // Simulation loop\n",
        "    for(int step =0; step < NSTEPS; step++) {\n",
        "        exchange_ghost_rows(rank, size, local_NY, comm);\n",
        "        update(local_NY);\n",
        "    }\n",
        "\n",
        "    MPI_Barrier(comm); // Synchronize before stopping the timer\n",
        "    if(rank ==0){\n",
        "        end_time = MPI_Wtime();\n",
        "        printf(\"OpenMP + MPI Execution Time: %f seconds\\\\n\", end_time - start_time);\n",
        "    }\n",
        "\n",
        "    // Free allocated memory\n",
        "    free_2D(temp, local_NY +2);\n",
        "    free_2D(temp_new, local_NY +2);\n",
        "\n",
        "    MPI_Finalize();\n",
        "    return 0;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Write OpenMP + MPI C Code\n",
        "write_code(\"heat_openmp_mpi.c\", openmp_mpi_code)\n",
        "\n",
        "# Compile OpenMP + MPI C Code\n",
        "if compile_mpi_openmp(\"heat_openmp_mpi.c\", \"heat_openmp_mpi\", flags=[\"-fopenmp\"]):\n",
        "    # Run OpenMP + MPI Executable with appropriate MPI processes and OpenMP threads\n",
        "    print(\"\\nRunning OpenMP + MPI Executable with adjusted MPI processes and OpenMP threads...\")\n",
        "\n",
        "    # Set OpenMP threads\n",
        "    os.environ[\"OMP_NUM_THREADS\"] = \"2\"\n",
        "\n",
        "    # Determine available CPU cores\n",
        "    cpu_count = subprocess.check_output(\"nproc\", shell=True).decode().strip()\n",
        "    cpu_count = int(cpu_count)\n",
        "    print(f\"Number of available CPU cores: {cpu_count}\")\n",
        "\n",
        "    # Set number of MPI processes and OpenMP threads based on available cores to prevent oversubscription\n",
        "    if cpu_count >= 4:\n",
        "        mpi_processes = 2\n",
        "        threads_per_process = 2\n",
        "    elif cpu_count >= 2:\n",
        "        mpi_processes = 2\n",
        "        threads_per_process = 1\n",
        "    else:\n",
        "        mpi_processes = 1\n",
        "        threads_per_process = 2  # All threads in one process\n",
        "\n",
        "    print(f\"Setting number of MPI processes to {mpi_processes} with {threads_per_process} OpenMP threads each.\")\n",
        "\n",
        "    # Update OMP_NUM_THREADS accordingly\n",
        "    os.environ[\"OMP_NUM_THREADS\"] = str(threads_per_process)\n",
        "\n",
        "    # Execute the OpenMP + MPI program\n",
        "    try:\n",
        "        # Using subprocess to capture the output\n",
        "        # Use --oversubscribe to allow running more MPI processes than available slots if necessary\n",
        "        openmp_mpi_command = [\"mpirun\", \"--oversubscribe\", \"-np\", str(mpi_processes), \"./heat_openmp_mpi\"]\n",
        "        openmp_mpi_output = subprocess.check_output(openmp_mpi_command, stderr=subprocess.STDOUT, text=True)\n",
        "        print(openmp_mpi_output)\n",
        "        # Extract OpenMP + MPI Execution Time\n",
        "        openmp_mpi_time_match = re.search(r\"OpenMP \\+ MPI Execution Time:\\s+([0-9.]+)\\s+seconds\", openmp_mpi_output)\n",
        "        if openmp_mpi_time_match:\n",
        "            openmp_mpi_time = float(openmp_mpi_time_match.group(1))\n",
        "            print(f\"OpenMP + MPI Execution Time: {openmp_mpi_time:.6f} seconds\\n\")\n",
        "        else:\n",
        "            print(\"Failed to extract OpenMP + MPI execution time.\\n\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Error running OpenMP + MPI executable:\\n{e.output}\\n\")\n",
        "else:\n",
        "    print(\"OpenMP + MPI compilation failed.\\n\")\n",
        "\n",
        "# ------------------------------\n",
        "# 4. OpenMP + MPI + CUDA C Implementation\n",
        "# ------------------------------\n",
        "openmp_mpi_cuda_code = \"\"\"\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <time.h>\n",
        "#include <mpi.h>\n",
        "#include <omp.h>\n",
        "#include <cuda.h>\n",
        "\n",
        "// CUDA kernel for updating temperature\n",
        "__global__ void update_kernel(double *temp, double *temp_new, int NX, int NY, double alpha, double dx, double dy) {\n",
        "    int j = blockIdx.x * blockDim.x + threadIdx.x + 1; // +1 to skip boundary\n",
        "    int i = blockIdx.y * blockDim.y + threadIdx.y + 1; // +1 to skip boundary\n",
        "\n",
        "    if(i < NY -1 && j < NX -1){\n",
        "        int idx = i * NX + j;\n",
        "        temp_new[idx] = temp[idx] + alpha * (\n",
        "            (temp[idx + NX] - 2 * temp[idx] + temp[idx - NX]) / (dx * dx) +\n",
        "            (temp[idx +1] - 2 * temp[idx] + temp[idx -1]) / (dy * dy)\n",
        "        );\n",
        "    }\n",
        "}\n",
        "\n",
        "// Function to allocate 1D arrays on the host\n",
        "double* allocate_1D(int size){\n",
        "    double *array;\n",
        "    cudaMallocHost(&array, size * sizeof(double));\n",
        "    return array;\n",
        "}\n",
        "\n",
        "// Function to free 1D arrays on the host\n",
        "void free_1D(double *array){\n",
        "    cudaFreeHost(array);\n",
        "}\n",
        "\n",
        "int main(int argc, char *argv[]) {\n",
        "    int rank, size;\n",
        "    double start_time, end_time;\n",
        "    int local_NY, start_row;\n",
        "    MPI_Init(&argc, &argv);\n",
        "    MPI_Comm comm = MPI_COMM_WORLD;\n",
        "    MPI_Comm_rank(comm, &rank);\n",
        "    MPI_Comm_size(comm, &size);\n",
        "\n",
        "    // Determine the number of rows per process\n",
        "    local_NY = NY / size;\n",
        "    int remainder = NY % size;\n",
        "    start_row = rank * local_NY + (rank < remainder ? rank : remainder);\n",
        "    local_NY += (rank < remainder) ? 1 : 0;\n",
        "\n",
        "    // Allocate host memory with ghost rows\n",
        "    int total_rows = local_NY + 2; // +2 for ghost rows\n",
        "    double *h_temp = allocate_1D(total_rows * NX);\n",
        "    double *h_temp_new = allocate_1D(total_rows * NX);\n",
        "\n",
        "    // Initialize the grid\n",
        "    for(int i =1; i <= local_NY; i++) {\n",
        "        for(int j =0; j < NX; j++) {\n",
        "            h_temp[i * NX + j] = 0.0; // Initial temperature\n",
        "            // Set boundary conditions\n",
        "            if((start_row + i -1 == 0) || (start_row + i -1 == NY -1) || j ==0 || j == NX -1){\n",
        "                h_temp[i * NX + j] = 100.0; // Hot edges\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // Allocate device memory\n",
        "    double *d_temp, *d_temp_new;\n",
        "    size_t size_bytes = total_rows * NX * sizeof(double);\n",
        "    cudaMalloc((void**)&d_temp, size_bytes);\n",
        "    cudaMalloc((void**)&d_temp_new, size_bytes);\n",
        "\n",
        "    // Copy initial data to device\n",
        "    cudaMemcpy(d_temp, h_temp, size_bytes, cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Define CUDA grid and block dimensions\n",
        "    dim3 blockDim(16,16);\n",
        "    dim3 gridDim( (NX + blockDim.x -1)/blockDim.x, (local_NY + blockDim.y -1)/blockDim.y );\n",
        "\n",
        "    double alpha =0.01;\n",
        "    double dx =1.0, dy =1.0;\n",
        "\n",
        "    MPI_Barrier(comm); // Synchronize before starting the timer\n",
        "    if(rank ==0){\n",
        "        start_time = MPI_Wtime();\n",
        "    }\n",
        "\n",
        "    // Simulation loop\n",
        "    for(int step =0; step < NSTEPS; step++) {\n",
        "        // Exchange ghost rows with neighboring MPI processes\n",
        "        MPI_Request requests[4];\n",
        "        int req_count =0;\n",
        "\n",
        "        // Send to upper neighbor, receive from lower neighbor\n",
        "        if(rank !=0){\n",
        "            MPI_Isend(&h_temp[1 * NX], NX, MPI_DOUBLE, rank -1, 0, comm, &requests[req_count++]);\n",
        "            MPI_Irecv(&h_temp[0 * NX], NX, MPI_DOUBLE, rank -1, 1, comm, &requests[req_count++]);\n",
        "        }\n",
        "\n",
        "        // Send to lower neighbor, receive from upper neighbor\n",
        "        if(rank != size -1){\n",
        "            MPI_Isend(&h_temp[local_NY * NX], NX, MPI_DOUBLE, rank +1, 1, comm, &requests[req_count++]);\n",
        "            MPI_Irecv(&h_temp[(local_NY +1) * NX], NX, MPI_DOUBLE, rank +1, 0, comm, &requests[req_count++]);\n",
        "        }\n",
        "\n",
        "        // Wait for all non-blocking operations to complete\n",
        "        MPI_Waitall(req_count, requests, MPI_STATUSES_IGNORE);\n",
        "\n",
        "        // Copy updated ghost rows to device\n",
        "        cudaMemcpy(d_temp, h_temp, size_bytes, cudaMemcpyHostToDevice);\n",
        "\n",
        "        // Launch CUDA kernel to update temperature\n",
        "        update_kernel<<<gridDim, blockDim>>>(d_temp, d_temp_new, NX, NY, alpha, dx, dy);\n",
        "        cudaDeviceSynchronize();\n",
        "\n",
        "        // Copy updated data back to host\n",
        "        cudaMemcpy(h_temp_new, d_temp_new, size_bytes, cudaMemcpyDeviceToHost);\n",
        "\n",
        "        // Swap h_temp and h_temp_new pointers\n",
        "        double *temp_ptr = h_temp;\n",
        "        h_temp = h_temp_new;\n",
        "        h_temp_new = temp_ptr;\n",
        "    }\n",
        "\n",
        "    MPI_Barrier(comm); // Synchronize before stopping the timer\n",
        "    if(rank ==0){\n",
        "        end_time = MPI_Wtime();\n",
        "        printf(\"OpenMP + MPI + CUDA Execution Time: %f seconds\\\\n\", end_time - start_time);\n",
        "    }\n",
        "\n",
        "    // Free device and host memory\n",
        "    cudaFree(d_temp);\n",
        "    cudaFree(d_temp_new);\n",
        "    free_1D(h_temp);\n",
        "    free_1D(h_temp_new);\n",
        "\n",
        "    MPI_Finalize();\n",
        "    return 0;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Write OpenMP + MPI + CUDA C Code\n",
        "write_code(\"heat_openmp_mpi_cuda.cu\", openmp_mpi_cuda_code)\n",
        "\n",
        "# Compile OpenMP + MPI + CUDA C Code\n",
        "print(\"\\n**Note:** Compiling the OpenMP + MPI + CUDA version in Google Colab is not recommended due to environment constraints.\\n\")\n",
        "print(\"Attempting to compile OpenMP + MPI + CUDA C code (This may fail in Colab)...\")\n",
        "\n",
        "if compile_mpi_openmp_cuda(\"heat_openmp_mpi_cuda.cu\", \"heat_openmp_mpi_cuda\", flags=[\"-Xcompiler\", \"-fopenmp\"]):\n",
        "    print(\"Compiled heat_openmp_mpi_cuda.cu successfully.\")\n",
        "    # **Execution is skipped due to Colab limitations**\n",
        "    print(\"**Execution of OpenMP + MPI + CUDA executable is skipped in Colab.**\\n\")\n",
        "else:\n",
        "    print(\"Compilation failed for OpenMP + MPI + CUDA executable.\\n\")\n",
        "\n",
        "# ------------------------------\n",
        "# 5. Performance Comparison\n",
        "# ------------------------------\n",
        "print(\"--- Execution Time Comparison ---\")\n",
        "if 'serial_time' in locals():\n",
        "    print(f\"Serial Execution Time: {serial_time:.6f} seconds\")\n",
        "else:\n",
        "    print(\"Serial Execution Time: Not Available\")\n",
        "\n",
        "if 'openmp_mpi_time' in locals():\n",
        "    print(f\"OpenMP + MPI Execution Time: {openmp_mpi_time:.6f} seconds\")\n",
        "else:\n",
        "    print(\"OpenMP + MPI Execution Time: Not Available\")\n",
        "\n",
        "# Calculate Speedup and Efficiency\n",
        "if 'serial_time' in locals() and 'openmp_mpi_time' in locals() and openmp_mpi_time > 0:\n",
        "    speedup = serial_time / openmp_mpi_time\n",
        "    total_threads = mpi_processes * threads_per_process\n",
        "    efficiency = (speedup / total_threads) * 100\n",
        "    print(f\"Speedup: {speedup:.2f}x\")\n",
        "    print(f\"Efficiency: {efficiency:.2f}%\")\n",
        "else:\n",
        "    print(\"Insufficient data to calculate Speedup and Efficiency.\")\n",
        "\n",
        "# ------------------------------\n",
        "# 6. Recommendations and Observations\n",
        "# ------------------------------\n",
        "print(\"\\n--- Recommendations and Observations ---\")\n",
        "if 'serial_time' in locals() and 'openmp_mpi_time' in locals():\n",
        "    if openmp_mpi_time < serial_time:\n",
        "        print(\"**Observation:** The OpenMP + MPI version is faster than the serial version.\")\n",
        "        print(\"**Performance Benefits Achieved Through OpenMP + MPI Parallelization.**\")\n",
        "    else:\n",
        "        print(\"**Observation:** The OpenMP + MPI version is slower than the serial version.\")\n",
        "        print(\"**Possible Reasons:**\")\n",
        "        print(\"- Overhead from thread creation and synchronization.\")\n",
        "        print(\"- Limited number of physical CPU cores in the Colab environment.\")\n",
        "        print(\"- Inefficient parallelization or cache contention.\")\n",
        "        print(\"- The problem size may not be large enough to benefit from parallelization.\")\n",
        "        print(\"- OpenMP directives might not be optimally placed.\\n\")\n",
        "        print(\"**Recommendations:**\")\n",
        "        print(\"- Increase the problem size (e.g., larger grid or more time steps) to better utilize parallelism.\")\n",
        "        print(\"- Experiment with different numbers of threads to find the optimal count.\")\n",
        "        print(\"- Optimize OpenMP directives, such as using appropriate scheduling strategies.\")\n",
        "        print(\"- Profile the code to identify and address bottlenecks.\")\n",
        "else:\n",
        "    print(\"Insufficient data to provide observations and recommendations.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "btLcUOTlrND2",
        "outputId": "6f875ed0-8971-48f8-b4c3-50927f3d5c26"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing MPI...\n",
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Ign:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:8 https://r2u.stat.illinois.edu/ubuntu jammy Release\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "mpi-default-bin is already the newest version (1.14).\n",
            "mpi-default-dev is already the newest version (1.14).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 52 not upgraded.\n",
            "\n",
            "Verifying MPI installation:\n",
            "gcc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "Copyright (C) 2021 Free Software Foundation, Inc.\n",
            "This is free software; see the source for copying conditions.  There is NO\n",
            "warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n",
            "\n",
            "\n",
            "Verifying CUDA installation:\n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n",
            "Code written to heat_serial.c\n",
            "Compiled heat_serial.c successfully.\n",
            "\n",
            "Running Serial Executable...\n",
            "Serial Execution Time: 65.417590 seconds\n",
            "Serial Execution Time: 65.417590 seconds\n",
            "\n",
            "Code written to heat_openmp_mpi.c\n",
            "Compiled heat_openmp_mpi.c successfully.\n",
            "\n",
            "Running OpenMP + MPI Executable with adjusted MPI processes and OpenMP threads...\n",
            "Number of available CPU cores: 2\n",
            "Setting number of MPI processes to 2 with 1 OpenMP threads each.\n",
            "OpenMP + MPI Execution Time: 73.421402 seconds\n",
            "\n",
            "OpenMP + MPI Execution Time: 73.421402 seconds\n",
            "\n",
            "Code written to heat_openmp_mpi_cuda.cu\n",
            "\n",
            "**Note:** Compiling the OpenMP + MPI + CUDA version in Google Colab is not recommended due to environment constraints.\n",
            "\n",
            "Attempting to compile OpenMP + MPI + CUDA C code (This may fail in Colab)...\n",
            "Found mpi.h at /usr/include/x86_64-linux-gnu/mpich/mpi.h\n",
            "Error compiling heat_openmp_mpi_cuda.cu:\n",
            "heat_openmp_mpi_cuda.cu(45): error: identifier \"NY\" is undefined\n",
            "      local_NY = NY / size;\n",
            "                 ^\n",
            "\n",
            "heat_openmp_mpi_cuda.cu(52): error: identifier \"NX\" is undefined\n",
            "      double *h_temp = allocate_1D(total_rows * NX);\n",
            "                                                ^\n",
            "\n",
            "heat_openmp_mpi_cuda.cu(88): error: identifier \"NSTEPS\" is undefined\n",
            "      for(int step =0; step < NSTEPS; step++) {\n",
            "                              ^\n",
            "\n",
            "3 errors detected in the compilation of \"heat_openmp_mpi_cuda.cu\".\n",
            "\n",
            "Compilation failed for OpenMP + MPI + CUDA executable.\n",
            "\n",
            "--- Execution Time Comparison ---\n",
            "Serial Execution Time: 65.417590 seconds\n",
            "OpenMP + MPI Execution Time: 73.421402 seconds\n",
            "Speedup: 0.89x\n",
            "Efficiency: 44.55%\n",
            "\n",
            "--- Recommendations and Observations ---\n",
            "**Observation:** The OpenMP + MPI version is slower than the serial version.\n",
            "**Possible Reasons:**\n",
            "- Overhead from thread creation and synchronization.\n",
            "- Limited number of physical CPU cores in the Colab environment.\n",
            "- Inefficient parallelization or cache contention.\n",
            "- The problem size may not be large enough to benefit from parallelization.\n",
            "- OpenMP directives might not be optimally placed.\n",
            "\n",
            "**Recommendations:**\n",
            "- Increase the problem size (e.g., larger grid or more time steps) to better utilize parallelism.\n",
            "- Experiment with different numbers of threads to find the optimal count.\n",
            "- Optimize OpenMP directives, such as using appropriate scheduling strategies.\n",
            "- Profile the code to identify and address bottlenecks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ### **2D Heat Conduction Simulation: Serial, OpenMP + MPI, and OpenMP + MPI + CUDA Versions**\n",
        "\n",
        "# This script performs the following:\n",
        "# 1. Installs MPI and verifies CUDA installation.\n",
        "# 2. Writes and compiles the **Serial** C program with a grid size of 3000x3000.\n",
        "# 3. Writes and compiles the **OpenMP + MPI** C program with a grid size of 3000x3000.\n",
        "# 4. Writes and compiles the **OpenMP + MPI + CUDA** C program with a grid size of 3000x3000.\n",
        "# 5. Executes the Serial and OpenMP + MPI programs.\n",
        "# 6. Captures and displays their execution times.\n",
        "# 7. Attempts to compile the OpenMP + MPI + CUDA program (execution is skipped due to Colab limitations).\n",
        "\n",
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "import re\n",
        "\n",
        "# Function to write C/CUDA code to a file\n",
        "def write_code(filename, code):\n",
        "    with open(filename, \"w\") as f:\n",
        "        f.write(code)\n",
        "    print(f\"Code written to {filename}\")\n",
        "\n",
        "# Function to compile C code\n",
        "def compile_c(source, output, flags=[]):\n",
        "    compile_command = [\"gcc\"] + flags + [source, \"-o\", output]\n",
        "    result = subprocess.run(compile_command, capture_output=True, text=True)\n",
        "    if result.returncode != 0:\n",
        "        print(f\"Error compiling {source}:\\n{result.stderr}\")\n",
        "        return False\n",
        "    else:\n",
        "        print(f\"Compiled {source} successfully.\")\n",
        "        return True\n",
        "\n",
        "# Function to compile MPI + OpenMP C code\n",
        "def compile_mpi_openmp(source, output, flags=[]):\n",
        "    compile_command = [\"mpicc\"] + flags + [source, \"-o\", output]\n",
        "    result = subprocess.run(compile_command, capture_output=True, text=True)\n",
        "    if result.returncode != 0:\n",
        "        print(f\"Error compiling {source}:\\n{result.stderr}\")\n",
        "        return False\n",
        "    else:\n",
        "        print(f\"Compiled {source} successfully.\")\n",
        "        return True\n",
        "\n",
        "# Function to compile MPI + OpenMP + CUDA C code\n",
        "def compile_mpi_openmp_cuda(source, output, flags=[]):\n",
        "    # Attempt to locate mpi.h\n",
        "    mpi_h_locations = [\n",
        "        \"/usr/include/openmpi/mpi.h\",\n",
        "        \"/usr/include/x86_64-linux-gnu/mpich/mpi.h\",\n",
        "        \"/usr/include/mpi.h\"\n",
        "    ]\n",
        "    mpi_include = None\n",
        "    for path in mpi_h_locations:\n",
        "        if os.path.exists(path):\n",
        "            mpi_include = os.path.dirname(path)\n",
        "            print(f\"Found mpi.h at {path}\")\n",
        "            break\n",
        "    if not mpi_include:\n",
        "        print(\"mpi.h not found. Cannot compile CUDA program with MPI.\")\n",
        "        return False\n",
        "\n",
        "    # Locate MPI libraries\n",
        "    mpi_lib_paths = [\n",
        "        \"/usr/lib/x86_64-linux-gnu\",\n",
        "        \"/usr/lib\",\n",
        "        \"/usr/local/lib\"\n",
        "    ]\n",
        "    mpi_lib = None\n",
        "    for lib_path in mpi_lib_paths:\n",
        "        if os.path.exists(lib_path):\n",
        "            mpi_lib = lib_path\n",
        "            break\n",
        "    if not mpi_lib:\n",
        "        print(\"MPI library not found. Cannot compile CUDA program with MPI.\")\n",
        "        return False\n",
        "\n",
        "    # Compile using nvcc with appropriate include and library paths\n",
        "    compile_command = [\n",
        "        \"nvcc\",\n",
        "        \"-Xcompiler\", \"-fopenmp\",  # Pass OpenMP flag to host compiler\n",
        "        source,\n",
        "        \"-o\",\n",
        "        output,\n",
        "        f\"-I{mpi_include}\",          # Include path for MPI headers\n",
        "        f\"-L{mpi_lib}\",              # Library path for MPI\n",
        "        \"-lmpi\"                       # Link against MPI library\n",
        "    ] + flags\n",
        "    result = subprocess.run(compile_command, capture_output=True, text=True)\n",
        "    if result.returncode != 0:\n",
        "        print(f\"Error compiling {source}:\\n{result.stderr}\")\n",
        "        return False\n",
        "    else:\n",
        "        print(f\"Compiled {source} successfully.\")\n",
        "        return True\n",
        "\n",
        "# Function to run an executable and capture output\n",
        "def run_executable(exec_path):\n",
        "    try:\n",
        "        result = subprocess.run([f\"./{exec_path}\"], capture_output=True, text=True, check=True)\n",
        "        return result.stdout.strip()\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Error running {exec_path}:\\n{e.output}\")\n",
        "        return None\n",
        "\n",
        "# ------------------------------\n",
        "# 1. Install MPI and Verify CUDA\n",
        "# ------------------------------\n",
        "print(\"Installing MPI...\")\n",
        "# Update package lists and install MPI\n",
        "!apt-get update -y\n",
        "!apt-get install -y mpi-default-bin mpi-default-dev\n",
        "\n",
        "print(\"\\nVerifying MPI installation:\")\n",
        "# Check MPI compiler version\n",
        "!mpicc --version\n",
        "\n",
        "print(\"\\nVerifying CUDA installation:\")\n",
        "# Check CUDA compiler version\n",
        "!nvcc --version\n",
        "\n",
        "# ------------------------------\n",
        "# 2. Serial C Implementation\n",
        "# ------------------------------\n",
        "serial_code = \"\"\"\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <time.h>\n",
        "\n",
        "#define NX 3000    // Number of grid points in X-direction\n",
        "#define NY 3000    // Number of grid points in Y-direction\n",
        "#define NSTEPS 500 // Number of time steps\n",
        "\n",
        "double temp[NY][NX];\n",
        "double temp_new[NY][NX];\n",
        "\n",
        "void initialize() {\n",
        "    for (int i = 0; i < NY; i++) {\n",
        "        for (int j = 0; j < NX; j++) {\n",
        "            temp[i][j] = 0.0; // Initial temperature\n",
        "            // Set boundary conditions\n",
        "            if (i == 0 || i == NY - 1 || j == 0 || j == NX - 1) {\n",
        "                temp[i][j] = 100.0; // Hot edges\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "void update() {\n",
        "    double alpha = 0.01; // Thermal diffusivity\n",
        "    double dx = 1.0;\n",
        "    double dy = 1.0;\n",
        "\n",
        "    for (int step = 0; step < NSTEPS; step++) {\n",
        "        for (int i = 1; i < NY - 1; i++) {\n",
        "            for (int j = 1; j < NX - 1; j++) {\n",
        "                temp_new[i][j] = temp[i][j] + alpha * (\n",
        "                    (temp[i+1][j] - 2 * temp[i][j] + temp[i-1][j]) / (dx * dx) +\n",
        "                    (temp[i][j+1] - 2 * temp[i][j] + temp[i][j-1]) / (dy * dy)\n",
        "                );\n",
        "            }\n",
        "        }\n",
        "        // Copy temp_new to temp for the next iteration\n",
        "        for (int i = 1; i < NY - 1; i++) {\n",
        "            for (int j = 1; j < NX - 1; j++) {\n",
        "                temp[i][j] = temp_new[i][j];\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    clock_t start, end;\n",
        "    double cpu_time_used;\n",
        "\n",
        "    initialize();\n",
        "\n",
        "    start = clock();\n",
        "    update();\n",
        "    end = clock();\n",
        "\n",
        "    cpu_time_used = ((double) (end - start)) / CLOCKS_PER_SEC;\n",
        "    printf(\"Serial Execution Time: %f seconds\\\\n\", cpu_time_used);\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Write Serial C Code\n",
        "write_code(\"heat_serial.c\", serial_code)\n",
        "\n",
        "# Compile Serial C Code\n",
        "if compile_c(\"heat_serial.c\", \"heat_serial\"):\n",
        "    # Run Serial Executable\n",
        "    print(\"\\nRunning Serial Executable...\")\n",
        "    serial_output = run_executable(\"heat_serial\")\n",
        "    if serial_output:\n",
        "        print(serial_output)\n",
        "        # Extract Serial Execution Time\n",
        "        serial_time_match = re.search(r\"Serial Execution Time:\\s+([0-9.]+)\\s+seconds\", serial_output)\n",
        "        if serial_time_match:\n",
        "            serial_time = float(serial_time_match.group(1))\n",
        "            print(f\"Serial Execution Time: {serial_time:.6f} seconds\\n\")\n",
        "        else:\n",
        "            print(\"Failed to extract serial execution time.\\n\")\n",
        "    else:\n",
        "        print(\"Serial executable did not run successfully.\\n\")\n",
        "else:\n",
        "    print(\"Serial compilation failed.\\n\")\n",
        "\n",
        "# ------------------------------\n",
        "# 3. OpenMP + MPI C Implementation\n",
        "# ------------------------------\n",
        "openmp_mpi_code = \"\"\"\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <time.h>\n",
        "#include <mpi.h>\n",
        "#include <omp.h>\n",
        "\n",
        "#define NX 3000    // Number of grid points in X-direction\n",
        "#define NY 3000    // Number of grid points in Y-direction\n",
        "#define NSTEPS 500 // Number of time steps\n",
        "\n",
        "double **temp;\n",
        "double **temp_new;\n",
        "\n",
        "// Function to allocate a 2D array dynamically\n",
        "double** allocate_2D(int rows, int cols) {\n",
        "    double **array = (double**) malloc(rows * sizeof(double*));\n",
        "    for(int i = 0; i < rows; i++) {\n",
        "        array[i] = (double*) malloc(cols * sizeof(double));\n",
        "    }\n",
        "    return array;\n",
        "}\n",
        "\n",
        "// Function to free a 2D array\n",
        "void free_2D(double **array, int rows) {\n",
        "    for(int i = 0; i < rows; i++) {\n",
        "        free(array[i]);\n",
        "    }\n",
        "    free(array);\n",
        "}\n",
        "\n",
        "// Initialize the temperature grid\n",
        "void initialize(int rank, int size, int local_NY, int start_row) {\n",
        "    #pragma omp parallel for collapse(2)\n",
        "    for(int i = 1; i <= local_NY; i++) { // 1 to local_NY inclusive\n",
        "        for(int j = 0; j < NX; j++) {\n",
        "            temp[i][j] = 0.0; // Initial temperature\n",
        "            // Set boundary conditions\n",
        "            if ((start_row + i -1 == 0) || (start_row + i -1 == NY -1) || j == 0 || j == NX -1) {\n",
        "                temp[i][j] = 100.0; // Hot edges\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "// Exchange ghost rows with neighboring MPI processes\n",
        "void exchange_ghost_rows(int rank, int size, int local_NY, MPI_Comm comm) {\n",
        "    MPI_Request requests[4];\n",
        "    int req_count = 0;\n",
        "\n",
        "    // Send to upper neighbor, receive from lower neighbor\n",
        "    if(rank != 0){\n",
        "        MPI_Isend(temp[1], NX, MPI_DOUBLE, rank -1, 0, comm, &requests[req_count++]);\n",
        "        MPI_Irecv(temp[0], NX, MPI_DOUBLE, rank -1, 1, comm, &requests[req_count++]);\n",
        "    }\n",
        "\n",
        "    // Send to lower neighbor, receive from upper neighbor\n",
        "    if(rank != size -1){\n",
        "        MPI_Isend(temp[local_NY], NX, MPI_DOUBLE, rank +1, 1, comm, &requests[req_count++]);\n",
        "        MPI_Irecv(temp[local_NY +1], NX, MPI_DOUBLE, rank +1, 0, comm, &requests[req_count++]);\n",
        "    }\n",
        "\n",
        "    MPI_Waitall(req_count, requests, MPI_STATUSES_IGNORE);\n",
        "}\n",
        "\n",
        "// Update the temperature grid\n",
        "void update(int local_NY) {\n",
        "    double alpha = 0.01; // Thermal diffusivity\n",
        "    double dx = 1.0;\n",
        "    double dy = 1.0;\n",
        "\n",
        "    #pragma omp parallel for collapse(2)\n",
        "    for(int i =1; i <= local_NY; i++) {\n",
        "        for(int j =1; j < NX -1; j++) {\n",
        "            temp_new[i][j] = temp[i][j] + alpha * (\n",
        "                (temp[i+1][j] - 2 * temp[i][j] + temp[i-1][j]) / (dx * dx) +\n",
        "                (temp[i][j+1] - 2 * temp[i][j] + temp[i][j-1]) / (dy * dy)\n",
        "            );\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // Swap temp and temp_new\n",
        "    #pragma omp parallel for collapse(2)\n",
        "    for(int i =1; i <= local_NY; i++) {\n",
        "        for(int j =1; j < NX -1; j++) {\n",
        "            temp[i][j] = temp_new[i][j];\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "int main(int argc, char *argv[]) {\n",
        "    int rank, size;\n",
        "    double start_time, end_time;\n",
        "    int local_NY, start_row;\n",
        "    MPI_Init(&argc, &argv);\n",
        "    MPI_Comm comm = MPI_COMM_WORLD;\n",
        "    MPI_Comm_rank(comm, &rank);\n",
        "    MPI_Comm_size(comm, &size);\n",
        "\n",
        "    // Determine the number of rows per process\n",
        "    local_NY = NY / size;\n",
        "    int remainder = NY % size;\n",
        "    start_row = rank * local_NY + (rank < remainder ? rank : remainder);\n",
        "    local_NY += (rank < remainder) ? 1 : 0;\n",
        "\n",
        "    // Allocate local arrays with ghost rows\n",
        "    temp = allocate_2D(local_NY +2, NX);      // +2 for ghost rows\n",
        "    temp_new = allocate_2D(local_NY +2, NX);\n",
        "\n",
        "    // Initialize local grid\n",
        "    initialize(rank, size, local_NY, start_row);\n",
        "\n",
        "    MPI_Barrier(comm); // Synchronize before starting the timer\n",
        "    if(rank ==0){\n",
        "        start_time = MPI_Wtime();\n",
        "    }\n",
        "\n",
        "    // Simulation loop\n",
        "    for(int step =0; step < NSTEPS; step++) {\n",
        "        exchange_ghost_rows(rank, size, local_NY, comm);\n",
        "        update(local_NY);\n",
        "    }\n",
        "\n",
        "    MPI_Barrier(comm); // Synchronize before stopping the timer\n",
        "    if(rank ==0){\n",
        "        end_time = MPI_Wtime();\n",
        "        printf(\"OpenMP + MPI Execution Time: %f seconds\\\\n\", end_time - start_time);\n",
        "    }\n",
        "\n",
        "    // Free allocated memory\n",
        "    free_2D(temp, local_NY +2);\n",
        "    free_2D(temp_new, local_NY +2);\n",
        "\n",
        "    MPI_Finalize();\n",
        "    return 0;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Write OpenMP + MPI C Code\n",
        "write_code(\"heat_openmp_mpi.c\", openmp_mpi_code)\n",
        "\n",
        "# Compile OpenMP + MPI C Code\n",
        "if compile_mpi_openmp(\"heat_openmp_mpi.c\", \"heat_openmp_mpi\", flags=[\"-fopenmp\"]):\n",
        "    # Run OpenMP + MPI Executable with appropriate MPI processes and OpenMP threads\n",
        "    print(\"\\nRunning OpenMP + MPI Executable with adjusted MPI processes and OpenMP threads...\")\n",
        "\n",
        "    # Set OpenMP threads\n",
        "    os.environ[\"OMP_NUM_THREADS\"] = \"2\"\n",
        "\n",
        "    # Determine available CPU cores\n",
        "    cpu_count = subprocess.check_output(\"nproc\", shell=True).decode().strip()\n",
        "    cpu_count = int(cpu_count)\n",
        "    print(f\"Number of available CPU cores: {cpu_count}\")\n",
        "\n",
        "    # Set number of MPI processes and OpenMP threads based on available cores to prevent oversubscription\n",
        "    if cpu_count >= 4:\n",
        "        mpi_processes = 2\n",
        "        threads_per_process = 2\n",
        "    elif cpu_count >= 2:\n",
        "        mpi_processes = 2\n",
        "        threads_per_process = 1\n",
        "    else:\n",
        "        mpi_processes = 1\n",
        "        threads_per_process = 2  # All threads in one process\n",
        "\n",
        "    print(f\"Setting number of MPI processes to {mpi_processes} with {threads_per_process} OpenMP threads each.\")\n",
        "\n",
        "    # Update OMP_NUM_THREADS accordingly\n",
        "    os.environ[\"OMP_NUM_THREADS\"] = str(threads_per_process)\n",
        "\n",
        "    # Execute the OpenMP + MPI program\n",
        "    try:\n",
        "        # Using subprocess to capture the output\n",
        "        # Use --oversubscribe to allow running more MPI processes than available slots if necessary\n",
        "        openmp_mpi_command = [\"mpirun\", \"--oversubscribe\", \"-np\", str(mpi_processes), \"./heat_openmp_mpi\"]\n",
        "        openmp_mpi_output = subprocess.check_output(openmp_mpi_command, stderr=subprocess.STDOUT, text=True)\n",
        "        print(openmp_mpi_output)\n",
        "        # Extract OpenMP + MPI Execution Time\n",
        "        openmp_mpi_time_match = re.search(r\"OpenMP \\+ MPI Execution Time:\\s+([0-9.]+)\\s+seconds\", openmp_mpi_output)\n",
        "        if openmp_mpi_time_match:\n",
        "            openmp_mpi_time = float(openmp_mpi_time_match.group(1))\n",
        "            print(f\"OpenMP + MPI Execution Time: {openmp_mpi_time:.6f} seconds\\n\")\n",
        "        else:\n",
        "            print(\"Failed to extract OpenMP + MPI execution time.\\n\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Error running OpenMP + MPI executable:\\n{e.output}\\n\")\n",
        "else:\n",
        "    print(\"OpenMP + MPI compilation failed.\\n\")\n",
        "\n",
        "# ------------------------------\n",
        "# 4. OpenMP + MPI + CUDA C Implementation\n",
        "# ------------------------------\n",
        "openmp_mpi_cuda_code = \"\"\"\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <time.h>\n",
        "#include <mpi.h>\n",
        "#include <omp.h>\n",
        "#include <cuda.h>\n",
        "\n",
        "// Define the macros\n",
        "#define NX 3000    // Number of grid points in X-direction\n",
        "#define NY 3000    // Number of grid points in Y-direction\n",
        "#define NSTEPS 500 // Number of time steps\n",
        "\n",
        "// CUDA kernel for updating temperature\n",
        "__global__ void update_kernel(double *temp, double *temp_new, int NX, int NY, double alpha, double dx, double dy) {\n",
        "    int j = blockIdx.x * blockDim.x + threadIdx.x + 1; // +1 to skip boundary\n",
        "    int i = blockIdx.y * blockDim.y + threadIdx.y + 1; // +1 to skip boundary\n",
        "\n",
        "    if(i < NY -1 && j < NX -1){\n",
        "        int idx = i * NX + j;\n",
        "        temp_new[idx] = temp[idx] + alpha * (\n",
        "            (temp[idx + NX] - 2 * temp[idx] + temp[idx - NX]) / (dx * dx) +\n",
        "            (temp[idx +1] - 2 * temp[idx] + temp[idx -1]) / (dy * dy)\n",
        "        );\n",
        "    }\n",
        "}\n",
        "\n",
        "// Function to allocate 1D arrays on the host\n",
        "double* allocate_1D(int size){\n",
        "    double *array;\n",
        "    cudaMallocHost(&array, size * sizeof(double));\n",
        "    return array;\n",
        "}\n",
        "\n",
        "// Function to free 1D arrays on the host\n",
        "void free_1D(double *array){\n",
        "    cudaFreeHost(array);\n",
        "}\n",
        "\n",
        "int main(int argc, char *argv[]) {\n",
        "    int rank, size;\n",
        "    double start_time, end_time;\n",
        "    int local_NY, start_row;\n",
        "    MPI_Init(&argc, &argv);\n",
        "    MPI_Comm comm = MPI_COMM_WORLD;\n",
        "    MPI_Comm_rank(comm, &rank);\n",
        "    MPI_Comm_size(comm, &size);\n",
        "\n",
        "    // Determine the number of rows per process\n",
        "    local_NY = NY / size;\n",
        "    int remainder = NY % size;\n",
        "    start_row = rank * local_NY + (rank < remainder ? rank : remainder);\n",
        "    local_NY += (rank < remainder) ? 1 : 0;\n",
        "\n",
        "    // Allocate host memory with ghost rows\n",
        "    int total_rows = local_NY + 2; // +2 for ghost rows\n",
        "    double *h_temp = allocate_1D(total_rows * NX);\n",
        "    double *h_temp_new = allocate_1D(total_rows * NX);\n",
        "\n",
        "    // Initialize the grid\n",
        "    for(int i =1; i <= local_NY; i++) {\n",
        "        for(int j =0; j < NX; j++) {\n",
        "            h_temp[i * NX + j] = 0.0; // Initial temperature\n",
        "            // Set boundary conditions\n",
        "            if((start_row + i -1 == 0) || (start_row + i -1 == NY -1) || j ==0 || j == NX -1){\n",
        "                h_temp[i * NX + j] = 100.0; // Hot edges\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // Allocate device memory\n",
        "    double *d_temp, *d_temp_new;\n",
        "    size_t size_bytes = total_rows * NX * sizeof(double);\n",
        "    cudaMalloc((void**)&d_temp, size_bytes);\n",
        "    cudaMalloc((void**)&d_temp_new, size_bytes);\n",
        "\n",
        "    // Copy initial data to device\n",
        "    cudaMemcpy(d_temp, h_temp, size_bytes, cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Define CUDA grid and block dimensions\n",
        "    dim3 blockDim(16,16);\n",
        "    dim3 gridDim( (NX + blockDim.x -1)/blockDim.x, (local_NY + blockDim.y -1)/blockDim.y );\n",
        "\n",
        "    double alpha =0.01;\n",
        "    double dx =1.0, dy =1.0;\n",
        "\n",
        "    MPI_Barrier(comm); // Synchronize before starting the timer\n",
        "    if(rank ==0){\n",
        "        start_time = MPI_Wtime();\n",
        "    }\n",
        "\n",
        "    // Simulation loop\n",
        "    for(int step =0; step < NSTEPS; step++) {\n",
        "        // Exchange ghost rows with neighboring MPI processes\n",
        "        MPI_Request requests[4];\n",
        "        int req_count =0;\n",
        "\n",
        "        // Send to upper neighbor, receive from lower neighbor\n",
        "        if(rank !=0){\n",
        "            MPI_Isend(&h_temp[1 * NX], NX, MPI_DOUBLE, rank -1, 0, comm, &requests[req_count++]);\n",
        "            MPI_Irecv(&h_temp[0 * NX], NX, MPI_DOUBLE, rank -1, 1, comm, &requests[req_count++]);\n",
        "        }\n",
        "\n",
        "        // Send to lower neighbor, receive from upper neighbor\n",
        "        if(rank != size -1){\n",
        "            MPI_Isend(&h_temp[local_NY * NX], NX, MPI_DOUBLE, rank +1, 1, comm, &requests[req_count++]);\n",
        "            MPI_Irecv(&h_temp[(local_NY +1) * NX], NX, MPI_DOUBLE, rank +1, 0, comm, &requests[req_count++]);\n",
        "        }\n",
        "\n",
        "        // Wait for all non-blocking operations to complete\n",
        "        MPI_Waitall(req_count, requests, MPI_STATUSES_IGNORE);\n",
        "\n",
        "        // Copy updated ghost rows to device\n",
        "        cudaMemcpy(d_temp, h_temp, size_bytes, cudaMemcpyHostToDevice);\n",
        "\n",
        "        // Launch CUDA kernel to update temperature\n",
        "        update_kernel<<<gridDim, blockDim>>>(d_temp, d_temp_new, NX, NY, alpha, dx, dy);\n",
        "        cudaDeviceSynchronize();\n",
        "\n",
        "        // Copy updated data back to host\n",
        "        cudaMemcpy(h_temp_new, d_temp_new, size_bytes, cudaMemcpyDeviceToHost);\n",
        "\n",
        "        // Swap h_temp and h_temp_new pointers\n",
        "        double *temp_ptr = h_temp;\n",
        "        h_temp = h_temp_new;\n",
        "        h_temp_new = temp_ptr;\n",
        "    }\n",
        "\n",
        "    MPI_Barrier(comm); // Synchronize before stopping the timer\n",
        "    if(rank ==0){\n",
        "        end_time = MPI_Wtime();\n",
        "        printf(\"OpenMP + MPI + CUDA Execution Time: %f seconds\\\\n\", end_time - start_time);\n",
        "    }\n",
        "\n",
        "    // Free device and host memory\n",
        "    cudaFree(d_temp);\n",
        "    cudaFree(d_temp_new);\n",
        "    free_1D(h_temp);\n",
        "    free_1D(h_temp_new);\n",
        "\n",
        "    MPI_Finalize();\n",
        "    return 0;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Write OpenMP + MPI + CUDA C Code\n",
        "write_code(\"heat_openmp_mpi_cuda.cu\", openmp_mpi_cuda_code)\n",
        "\n",
        "# Compile OpenMP + MPI + CUDA C Code\n",
        "print(\"\\n**Note:** Compiling the OpenMP + MPI + CUDA version in Google Colab is not recommended due to environment constraints.\\n\")\n",
        "print(\"Attempting to compile OpenMP + MPI + CUDA C code (This may fail in Colab)...\")\n",
        "\n",
        "if compile_mpi_openmp_cuda(\"heat_openmp_mpi_cuda.cu\", \"heat_openmp_mpi_cuda\", flags=[\"-Xcompiler\", \"-fopenmp\"]):\n",
        "    print(\"Compiled heat_openmp_mpi_cuda.cu successfully.\")\n",
        "    # **Execution is skipped due to Colab limitations**\n",
        "    print(\"**Execution of OpenMP + MPI + CUDA executable is skipped in Colab.**\\n\")\n",
        "else:\n",
        "    print(\"Compilation failed for OpenMP + MPI + CUDA executable.\\n\")\n",
        "\n",
        "# ------------------------------\n",
        "# 5. Performance Comparison\n",
        "# ------------------------------\n",
        "print(\"--- Execution Time Comparison ---\")\n",
        "if 'serial_time' in locals():\n",
        "    print(f\"Serial Execution Time: {serial_time:.6f} seconds\")\n",
        "else:\n",
        "    print(\"Serial Execution Time: Not Available\")\n",
        "\n",
        "if 'openmp_mpi_time' in locals():\n",
        "    print(f\"OpenMP + MPI Execution Time: {openmp_mpi_time:.6f} seconds\")\n",
        "else:\n",
        "    print(\"OpenMP + MPI Execution Time: Not Available\")\n",
        "\n",
        "# Calculate Speedup and Efficiency\n",
        "if 'serial_time' in locals() and 'openmp_mpi_time' in locals() and openmp_mpi_time > 0:\n",
        "    speedup = serial_time / openmp_mpi_time\n",
        "    total_threads = mpi_processes * threads_per_process\n",
        "    efficiency = (speedup / total_threads) * 100\n",
        "    print(f\"Speedup: {speedup:.2f}x\")\n",
        "    print(f\"Efficiency: {efficiency:.2f}%\")\n",
        "else:\n",
        "    print(\"Insufficient data to calculate Speedup and Efficiency.\")\n",
        "\n",
        "# ------------------------------\n",
        "# 6. Recommendations and Observations\n",
        "# ------------------------------\n",
        "print(\"\\n--- Recommendations and Observations ---\")\n",
        "if 'serial_time' in locals() and 'openmp_mpi_time' in locals():\n",
        "    if openmp_mpi_time < serial_time:\n",
        "        print(\"**Observation:** The OpenMP + MPI version is faster than the serial version.\")\n",
        "        print(\"**Performance Benefits Achieved Through OpenMP + MPI Parallelization.**\")\n",
        "    else:\n",
        "        print(\"**Observation:** The OpenMP + MPI version is slower than the serial version.\")\n",
        "        print(\"**Possible Reasons:**\")\n",
        "        print(\"- Overhead from thread creation and synchronization.\")\n",
        "        print(\"- Limited number of physical CPU cores in the Colab environment.\")\n",
        "        print(\"- Inefficient parallelization or cache contention.\")\n",
        "        print(\"- The problem size may not be large enough to benefit from parallelization.\")\n",
        "        print(\"- OpenMP directives might not be optimally placed.\\n\")\n",
        "        print(\"**Recommendations:**\")\n",
        "        print(\"- Increase the problem size (e.g., larger grid or more time steps) to better utilize parallelism.\")\n",
        "        print(\"- Experiment with different numbers of threads to find the optimal count.\")\n",
        "        print(\"- Optimize OpenMP directives, such as using appropriate scheduling strategies.\")\n",
        "        print(\"- Profile the code to identify and address bottlenecks.\")\n",
        "else:\n",
        "    print(\"Insufficient data to provide observations and recommendations.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxvITKN2vJH3",
        "outputId": "b292d87d-c3a7-4b05-c4c5-1f5213fe538e"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing MPI...\n",
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Ign:4 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:5 https://r2u.stat.illinois.edu/ubuntu jammy Release\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "mpi-default-bin is already the newest version (1.14).\n",
            "mpi-default-dev is already the newest version (1.14).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 52 not upgraded.\n",
            "\n",
            "Verifying MPI installation:\n",
            "gcc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "Copyright (C) 2021 Free Software Foundation, Inc.\n",
            "This is free software; see the source for copying conditions.  There is NO\n",
            "warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n",
            "\n",
            "\n",
            "Verifying CUDA installation:\n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n",
            "Code written to heat_serial.c\n",
            "Compiled heat_serial.c successfully.\n",
            "\n",
            "Running Serial Executable...\n",
            "Serial Execution Time: 64.078429 seconds\n",
            "Serial Execution Time: 64.078429 seconds\n",
            "\n",
            "Code written to heat_openmp_mpi.c\n",
            "Compiled heat_openmp_mpi.c successfully.\n",
            "\n",
            "Running OpenMP + MPI Executable with adjusted MPI processes and OpenMP threads...\n",
            "Number of available CPU cores: 2\n",
            "Setting number of MPI processes to 2 with 1 OpenMP threads each.\n",
            "OpenMP + MPI Execution Time: 73.298637 seconds\n",
            "\n",
            "OpenMP + MPI Execution Time: 73.298637 seconds\n",
            "\n",
            "Code written to heat_openmp_mpi_cuda.cu\n",
            "\n",
            "**Note:** Compiling the OpenMP + MPI + CUDA version in Google Colab is not recommended due to environment constraints.\n",
            "\n",
            "Attempting to compile OpenMP + MPI + CUDA C code (This may fail in Colab)...\n",
            "Found mpi.h at /usr/include/x86_64-linux-gnu/mpich/mpi.h\n",
            "Error compiling heat_openmp_mpi_cuda.cu:\n",
            "heat_openmp_mpi_cuda.cu(15): error: expected a \")\"\n",
            "  __attribute__((global)) void update_kernel(double *temp, double *temp_new, int 3000, int 3000, double alpha, double dx, double dy) {\n",
            "                                                                                 ^\n",
            "\n",
            "heat_openmp_mpi_cuda.cu(21): error: identifier \"alpha\" is undefined\n",
            "          temp_new[idx] = temp[idx] + alpha * (\n",
            "                                      ^\n",
            "\n",
            "heat_openmp_mpi_cuda.cu(22): error: identifier \"dx\" is undefined\n",
            "              (temp[idx + 3000] - 2 * temp[idx] + temp[idx - 3000]) / (dx * dx) +\n",
            "                                                                       ^\n",
            "\n",
            "heat_openmp_mpi_cuda.cu(23): error: identifier \"dy\" is undefined\n",
            "              (temp[idx +1] - 2 * temp[idx] + temp[idx -1]) / (dy * dy)\n",
            "                                                               ^\n",
            "\n",
            "heat_openmp_mpi_cuda.cu(117): error: too many arguments in function call\n",
            "          update_kernel<<<gridDim, blockDim>>>(d_temp, d_temp_new, 3000, 3000, alpha, dx, dy);\n",
            "                                                                         ^\n",
            "\n",
            "5 errors detected in the compilation of \"heat_openmp_mpi_cuda.cu\".\n",
            "\n",
            "Compilation failed for OpenMP + MPI + CUDA executable.\n",
            "\n",
            "--- Execution Time Comparison ---\n",
            "Serial Execution Time: 64.078429 seconds\n",
            "OpenMP + MPI Execution Time: 73.298637 seconds\n",
            "Speedup: 0.87x\n",
            "Efficiency: 43.71%\n",
            "\n",
            "--- Recommendations and Observations ---\n",
            "**Observation:** The OpenMP + MPI version is slower than the serial version.\n",
            "**Possible Reasons:**\n",
            "- Overhead from thread creation and synchronization.\n",
            "- Limited number of physical CPU cores in the Colab environment.\n",
            "- Inefficient parallelization or cache contention.\n",
            "- The problem size may not be large enough to benefit from parallelization.\n",
            "- OpenMP directives might not be optimally placed.\n",
            "\n",
            "**Recommendations:**\n",
            "- Increase the problem size (e.g., larger grid or more time steps) to better utilize parallelism.\n",
            "- Experiment with different numbers of threads to find the optimal count.\n",
            "- Optimize OpenMP directives, such as using appropriate scheduling strategies.\n",
            "- Profile the code to identify and address bottlenecks.\n"
          ]
        }
      ]
    }
  ]
}